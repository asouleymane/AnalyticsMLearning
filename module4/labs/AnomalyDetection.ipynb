{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Anomaly Detection\n",
    "\n",
    "## Lab 1: Intro to Outlier/Novelty Detection \n",
    "\n",
    "In this lab, we'll be covering some very basic algorithms for anomaly detection.\n",
    "\n",
    "## Getting started\n",
    "First, we'll need to import a few modules to construct a regression problem and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data and first attempts at a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make some data for regression\n",
    "X, y = make_regression(n_samples = 20, n_features = 1, noise=3.0, bias=100.0)\n",
    "\n",
    "# Generate some significant outliers\n",
    "X_outliers = np.random.normal(0, 0.5, size=(4, 1))\n",
    "y_outliers = np.random.normal(0, 2.0, size=4)\n",
    "X_outliers[:2, :] += X.max() + X.mean() / 4.\n",
    "X_outliers[2:, :] += X.min() - X.mean() / 4.\n",
    "y_outliers[:2] += y.min() - y.mean() / 4.\n",
    "y_outliers[2:] += y.max() + y.mean() / 4.\n",
    "\n",
    "# Add outliers to existing data\n",
    "X = np.vstack((X, X_outliers))\n",
    "y = np.concatenate((y, y_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to visually confirm any suspicions about a particular dataset.  \n",
    "If we plot every data point, we should see four obvious outliers given the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the data above contains four strong outliers.\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The outliers are the values in the corner.**\n",
    "Looking good so far.  \n",
    "The outliers are the pairs of points in the upper left and lower right corners of the plot.\n",
    "\n",
    "#### What would you expect to be the result of fitting a linear regression model to this data?\n",
    "\n",
    "We should attempt to fit a model to the data, first without outlier reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and predict without running outlier reduction\n",
    "raw_model = Ridge().fit(X, y)\n",
    "y_pred = raw_model.predict(X)\n",
    "\n",
    "# plot results\n",
    "plt.scatter(X, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is obviously wrong - the high-variance outliers are causing strong *overfitting* in the model,  \n",
    "producing a negative-slope model for a positive-slope trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reduce overfitting: EllipticEnvelope\n",
    "One method of outlier reduction is to fit an `EllipticEnvelope` to the data  \n",
    "and use its `predict()` method to detect and retain only those data points which are *inliers*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elliptic_envelope_session():\n",
    "    # Fit envelope\n",
    "    envelope = EllipticEnvelope(support_fraction=1, contamination=0.2).fit(X, y)\n",
    "\n",
    "    # Create an boolean indexing array to pick up outliers\n",
    "    outliers = envelope.predict(X)==-1\n",
    "\n",
    "    # Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "    X_clean = X[~outliers]\n",
    "    y_clean = y[~outliers]\n",
    "    return X_clean, y_clean\n",
    "\n",
    "def fit_plot(dataset_clean):\n",
    "    # Unpack cleaned dataset\n",
    "    X_clean, y_clean = dataset_clean\n",
    "    \n",
    "    # Assuming we have our set of inliers in X_clean we can re-fit\n",
    "    model = Ridge().fit(X_clean, y_clean)\n",
    "    y_pred = model.predict(X_clean)\n",
    "    \n",
    "    # Plot the regression\n",
    "    plt.scatter(X_clean, y_pred)\n",
    "    plt.show()\n",
    "    \n",
    "fit_plot(elliptic_envelope_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can successfully create a regression model on the cleaned dataset. \n",
    "It did a job of sequestering the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to the EllipticEnvelope: KMeans, LocalOutlierFactor\n",
    "\n",
    "Ultimately, most of the statistical learning algorithms available to you in  \n",
    "`scikit` for outlier reduction are similar in usage and function.  \n",
    "\n",
    "Given that the goal of this exercise is to group 'good' inputs while avoiding 'bad' inputs,  \n",
    "and the 'bad' inputs are those with relatively high variance, \n",
    "we can attempt to preprocess our inputs with a round of `KMeans` clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_session():\n",
    "    # run k-means clustering\n",
    "    km_clusters = KMeans(n_clusters=3, algorithm=\"full\").fit_predict(X, y)\n",
    "    \n",
    "    # create cluster distribution, this time they are in tuples so we can sort easily\n",
    "    dist_clusters = ((np.sum(km_clusters==z), z) for z in np.unique(km_clusters))\n",
    "    \n",
    "    # sort clusters descendingly by number of data entries in cluster\n",
    "    dist_clusters = sorted(dist_clusters, reverse = True)\n",
    "    \n",
    "    # find out the cluster with max number of data entries\n",
    "    max_cluster = dist_clusters[0][1]\n",
    "\n",
    "    # select data in max_cluster as inliers\n",
    "    inliers = km_clusters == max_cluster\n",
    "    \n",
    "    return X[inliers], y[inliers]\n",
    "\n",
    "fit_plot(kmeans_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! It seems we have managed to compensate for the outliers.\n",
    "\n",
    "### Further practice: tweaking parameters\n",
    "As an exercise, you could try changing to selector clusters other than `max_cluster`,  \n",
    "and see how the predictions change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other options: LocalOutlierFactor\n",
    "\n",
    "Though, KMeans is by no means the only way to achieve outlier reduction.  \n",
    "There are many other valid measures of covariance which we can apply to  \n",
    "reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_outlier_factor_session():\n",
    "    lof_labels = LocalOutlierFactor(n_neighbors=10).fit_predict(X, y)\n",
    "    inliers = lof_labels == 1 # select inliers\n",
    "    return X[inliers], y[inliers]\n",
    "\n",
    "fit_plot(local_outlier_factor_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning with LocalOutlierFactor\n",
    "As an exercise, try changing n_neighbors in the LocalOutlierFactor constructor call.  \n",
    "Different values will change the fit and behavior of the model - this is especially true given such a small dataset,  \n",
    "and given that the default value for n_neighbors is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
