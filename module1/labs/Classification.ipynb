{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Classification\n",
    "\n",
    "In this lab you will create a classification model on the same red wine quality dataset and then apply and practice the same training and validation methodology. \n",
    "The classification model will be based on Naive Bayes provided by sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We will load the dataset from file into a Panda data frame and investigate its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "# View some metadata of the dataset and see if that makes sense\n",
    "print('dataset.shape', dataset.shape)\n",
    "\n",
    "X = np.array(dataset.iloc[:,:-1])[:, [1,2,6,9,10]]\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "print('X', X.shape, 'y', y.shape)\n",
    "print('Label distribution:', {i: np.sum(y==i) for i in np.unique(dataset.quality)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the train/validation split and then train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have seen this before!\n",
    "# If you are so inclined, you may want to tweak the test_size and see how the model performs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally you can print out a sample and see for yourself how the classification performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[20:50])\n",
    "print(model.predict(X[20:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Confusion\n",
    "Again, we will judge the classification performance with a confusion matrix.\n",
    "\n",
    "Please read about it here: https://en.wikipedia.org/wiki/Confusion_matrix  \n",
    "You will note that from a confusion matrix, a large number of additional model performance metrics can be computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Compute confusion matrix with expected value, predicted values... similar to RMSE \n",
    "confusion_matrix(y_test, np.round(model.predict(X_test)).astype('i4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond Confusion Matrix: Precision, Recall, and F1\n",
    "\n",
    "Here we are going to look at a couple additional measures.\n",
    "\n",
    "First: \n",
    "  * _condition positive_ (P) is the number of real positive cases in the data\n",
    "  * _condition negatives_ (N) is the number of real negative cases in the data \n",
    "\n",
    "Then: \n",
    "  * _true positive_ (TP) is a correct prediction of a class, eqv. with hit in a Yes / No model\n",
    "  * _true negative_ (TN) is a correct prediction of not a class, eqv. with correct rejection in a Yes / No model\n",
    "  * _false positive_ (FP) is misclassification, eqv. with false alarm in a Yes / No model, **Type I error**\n",
    "  * _false negative_ (FN) is misclassification, eqv. with miss in a Yes / No model, **Type II error** \n",
    "\n",
    "Metrics:\n",
    "  * Recall or True Positive Rate:$$ Recall = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$ \n",
    "  * Precision or Positive Predictive Value:$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "  * [F1 is the harmonic mean of precision and sensitivity](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)$$ F_{1} = 2 * \\frac{Precision * Recall}{Precision + Recall}$$\n",
    "  \n",
    "#### More details on scikit-learn model scoring:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Please read documentation on this parameter\n",
    "f1_score(y_test, np.round(model.predict(X_test)).astype('i4'), average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
