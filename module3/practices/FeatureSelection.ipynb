{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Feature selection - Practice\n",
    "\n",
    "In this session you will practice **feature selection**, which reduces the dimensionality of data for the following reasons:\n",
    "\n",
    "1. Reduces overfitting by removing noise introduced by some of the features.\n",
    "2. Reduces training time, which allows you to experiment more with different models and hyperparameters.\n",
    "3. Reduces data acquisition requirements.\n",
    "4. Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. That will enable you to focus on the main sources of predictability, make the model more justifiable to another person.\n",
    "\n",
    "We are going to use **titanic** dataset for this practice.\n",
    "\n",
    "sklearn API reference:\n",
    "\n",
    "+ [sklearn.feature_selection.SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "+ [sklearn.feature_selection.chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "+ [sklearn.preprocessing.LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html)\n",
    "+ [sklearn.feature_selection.sklearn.feature_selection.f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "+ [sklearn.preprocessing.scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html)\n",
    "+ [sklearn.feature_selection.mutual_info_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)\n",
    "+ [sklearn.feature_selection.RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.base import clone\n",
    "\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.312360</td>\n",
       "      <td>0.642697</td>\n",
       "      <td>29.548697</td>\n",
       "      <td>0.503371</td>\n",
       "      <td>0.351685</td>\n",
       "      <td>32.865772</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>0.389888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.837241</td>\n",
       "      <td>0.479475</td>\n",
       "      <td>13.379025</td>\n",
       "      <td>1.095286</td>\n",
       "      <td>0.790069</td>\n",
       "      <td>52.639685</td>\n",
       "      <td>0.529535</td>\n",
       "      <td>0.487999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.775000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.925000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pclass         sex         age       sibsp       parch        fare  \\\n",
       "count  890.000000  890.000000  890.000000  890.000000  890.000000  890.000000   \n",
       "mean     2.312360    0.642697   29.548697    0.503371    0.351685   32.865772   \n",
       "std      0.837241    0.479475   13.379025    1.095286    0.790069   52.639685   \n",
       "min      1.000000    0.000000    0.170000    0.000000    0.000000    0.000000   \n",
       "25%      2.000000    0.000000   21.000000    0.000000    0.000000    7.895800   \n",
       "50%      3.000000    1.000000   28.000000    0.000000    0.000000   13.775000   \n",
       "75%      3.000000    1.000000   37.000000    1.000000    0.000000   29.925000   \n",
       "max      3.000000    1.000000   80.000000    8.000000    9.000000  512.329200   \n",
       "\n",
       "         embarked    survived  \n",
       "count  890.000000  890.000000  \n",
       "mean     0.895506    0.389888  \n",
       "std      0.529535    0.487999  \n",
       "min      0.000000    0.000000  \n",
       "25%      1.000000    0.000000  \n",
       "50%      1.000000    0.000000  \n",
       "75%      1.000000    1.000000  \n",
       "max      2.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/titanic_ML/titanic.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create variable **X** and **y** and pull features and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "X = np.array(dataset.iloc[:,:-1])\n",
    "y = np.array(dataset.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/validate split with **20%** of data held out for validation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## χ² feature selection\n",
    "\n",
    "Create a k-best feature selector with [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "that uses [χ² feature selection](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "to select **top 5** features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "selector = SelectKBest(chi2,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit** the selector to the **training dataset**.\n",
    "\n",
    "Then **print** χ² statistic (the score for this selector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_squared statistic [   20.10600106    74.40316364    14.7231048      4.28721714     9.80957579\n",
      "  4084.61256534     5.20247536]\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment \n",
    "# ----------------------------------\n",
    "selector.fit(X_train,y_train)\n",
    "print('X_squared statistic',selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns indices that just got selected by the feature selector?\n",
    "\n",
    "**Print these indices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected indices [0 1 2 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment \n",
    "# ----------------------------------\n",
    "print('Selected indices',selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the names of these columns selected?\n",
    "Does it logically make sense that these features got selected?\n",
    "\n",
    "**Print name of columns selected.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pclass', 'sex', 'age', 'parch', 'fare']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment \n",
    "# ----------------------------------\n",
    "[dataset.columns[i] for i in selector.get_support(True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call **selector.transform()** method to select those feature columns from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **fit** a Gaussian Naive Bayes model with these selected features.\n",
    "\n",
    "**Compute validation accuracy** using model.score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7247191011235955"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_selected, y_train)\n",
    "model.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to a model trained without feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7191011235955056"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are those feature selected statistically significant? <span style=\"background: yellow;\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".7247 for the model with feature selection compared to .71910.I wouldsay very small difference.\n",
    "The main question is if that difference will scale somehow if used on more data or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.32664275e-06   6.36868624e-18   1.24511081e-04   3.83999690e-02\n",
      "   1.73605546e-03   0.00000000e+00   2.25547470e-02]\n"
     ]
    }
   ],
   "source": [
    "chi2_sklearn, pvalue_sklearn = chi2(X_train, y_train)\n",
    "print(pvalue_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  33.,   11.,   22.,   94.,  100.,    0.,    0.,  100.,  100.,\n",
       "         92.,    0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2 as chi2_distribution\n",
    "u = np.array([11.3,15.6,13.0,4.12,0.752,162,2.76e+03,2.30e-04,0.155,4.56,46.4])\n",
    "np.round(chi2_distribution.sf(u, 10) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information\n",
    "\n",
    "Mutual information is generally considered a robust measure of dependence, coming from information theory.\n",
    "It could apply to both regression and classification problems.\n",
    "sklearn provided [mutual_info_classif()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "and [mutual_info_regression()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) respectively.\n",
    "\n",
    "We review feature selection again and also try this method below.\n",
    "\n",
    "**Tip**: _Putting Python variables in a function allows you to limit its scope and hide global variables.\n",
    "Only keywords that create new scope in Python are **class**, **def** and **lambda** ._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7247191011235955"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "def mutual_info_session():\n",
    "    selector = SelectKBest(mutual_info_classif, k=3)\n",
    "    selector.fit(X_train, y_train)\n",
    "    print(selector.get_support(True))\n",
    "    model = GaussianNB()\n",
    "    model.fit(selector.transform(X_train), y_train)\n",
    "    return model.score(selector.transform(X_test), y_test)\n",
    "    \n",
    "mutual_info_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward selection\n",
    "\n",
    "Complete following code based on your understanding of forward selection.\n",
    "Please refer to the feature selection lab, same section for hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ForwardSelector(object):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y, k): \n",
    "        selected = np.zeros(X.shape[1]).astype(bool)\n",
    "        score = lambda X_features: clone(self.estimator).fit(X_features, y).score(X_features, y)\n",
    "        selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "\n",
    "        # What is the exit condition for forward selection?\n",
    "        # ----------------------------------\n",
    "        while np.sum(selected) < k:\n",
    "            rest_indices = list(np.flatnonzero(~selected))\n",
    "            \n",
    "            scores = list()\n",
    "            for i in rest_indices:\n",
    "                # Which columns are we currently using to score the model?\n",
    "                # ----------------------------------\n",
    "                feature_subset = selected_indices()+[i]\n",
    "                s = score(X[:, feature_subset])\n",
    "                scores.append(s)\n",
    "           \n",
    "            idx_to_add = rest_indices[np.argmax(scores)]\n",
    "            selected[idx_to_add] = True\n",
    "\n",
    "        self.selected = selected.copy()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X[:, self.selected]\n",
    "    \n",
    "    def get_support(self, indices=False):\n",
    "        return np.flatnonzero(self.selected) if indices else self.selected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **write a test case** for your completed forward selection algorithm and see if it works well for you.\n",
    "** Select 3 features. **\n",
    "\n",
    "**Tip**: _You could copy-paste-edit what we just did in mutual information section._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7415730337078652"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_selection_session():\n",
    "    model = GaussianNB()\n",
    "    selector = ForwardSelector(model)\n",
    "    # Add code below this comment \n",
    "    # selector = SelectKBest(mutual_info_classif, k=3)\n",
    "    selector.fit(X_train,y_train,3)\n",
    "    print(selector.get_support(True))\n",
    "    #model = GaussianNB()\n",
    "    model.fit(selector.transform(X_train), y_train)\n",
    "    return model.score(selector.transform(X_test),y_test)\n",
    "    \n",
    "forward_selection_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination\n",
    "\n",
    "Finally, also give it a shot at RFE provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7415730337078652"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete code below this comment \n",
    "# ----------------------------------\n",
    "def rfe_session():\n",
    "    # using SVC instead because RFE doesn't support GaussianNB\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel=\"linear\")\n",
    "    # Add code below this comment \n",
    "    # ----------------------------------\n",
    "    selector = RFE(model,3)\n",
    "    selector.fit(X_train,y_train)\n",
    "    print(selector.get_support(True))\n",
    "    model.fit(selector.transform(X_train),y_train)\n",
    "    return model.score(selector.transform(X_test), y_test)\n",
    "\n",
    "rfe_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
