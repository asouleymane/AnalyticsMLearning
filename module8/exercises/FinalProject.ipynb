{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "\n",
    "For your final project, you will build a classifer for\n",
    "the **Backorder Prediction** dataset by following our\n",
    "operationalized machine learning pipeline.\n",
    "\n",
    "![AppliedML_Workflow IMAGE MISSING](../images/AppliedML_Workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Data\n",
    "\n",
    "Details of the dataset are located here:\n",
    "\n",
    "Dataset: https://www.kaggle.com/tiredgeek/predict-bo-trial\n",
    "\n",
    "The files are accessible in the JupyterHub environment:\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Training_Dataset_v2.csv`\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration, Training, and Validation\n",
    "\n",
    "You will examine the _training_ dataset and perform \n",
    " * **data preparation and exploratory data analysis**, \n",
    " * **anomaly detection / removal**,\n",
    " * **dimensionality reduction** and then\n",
    " * **train and validate 3 different models**.\n",
    "\n",
    "Of the 3 different models, you are free to pick any estimator from scikit-learn \n",
    "or models we have so far covered using TensorFlow.\n",
    "\n",
    "### Validation Assessment\n",
    "\n",
    "Your first, intermediate, result will be an **assessment** of the models' performance.\n",
    "This assessement should be grounded within a 10-fold cross-validation methodology.\n",
    "\n",
    "This should include the confusion matrix and F-score for each classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing\n",
    "\n",
    "Once you have chosen your final model, you will need to re-train it using all the training data.\n",
    "\n",
    "\n",
    "--- \n",
    "##  Overview / Roadmap\n",
    "\n",
    "**General steps**:\n",
    "\n",
    "* Dataset carpentry & Exploratory Data Analysis\n",
    "  * Develop functions to perform the necessary steps, you will have to carpentry the Training and the Testing data.\n",
    "* Create 3 pipelines, each does:\n",
    "    * Anomaly detection\n",
    "    * Dimensionality reduction\n",
    "    * Model training/validation\n",
    "* Train chosen model full training data\n",
    "* Evaluate model against testing\n",
    "* Write a summary of your processing and an analysis of the model performance\n",
    "\n",
    "\n",
    "#### <span style=\"background:yellow\">Note:</span> The use of sklearn Pipelines and FeatureUnion is optional.   \n",
    "However, your three models should follow a readable path from data to cross-validation statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "**Description**\n",
    "~~~\n",
    "sku - Random ID for the product\n",
    "national_inv - Current inventory level for the part\n",
    "lead_time - Transit time for product (if available)\n",
    "in_transit_qty - Amount of product in transit from source\n",
    "forecast_3_month - Forecast sales for the next 3 months\n",
    "forecast_6_month - Forecast sales for the next 6 months\n",
    "forecast_9_month - Forecast sales for the next 9 months\n",
    "sales_1_month - Sales quantity for the prior 1 month time period\n",
    "sales_3_month - Sales quantity for the prior 3 month time period\n",
    "sales_6_month - Sales quantity for the prior 6 month time period\n",
    "sales_9_month - Sales quantity for the prior 9 month time period\n",
    "min_bank - Minimum recommend amount to stock\n",
    "potential_issue - Source issue for part identified\n",
    "pieces_past_due - Parts overdue from source\n",
    "perf_6_month_avg - Source performance for prior 6 month period\n",
    "perf_12_month_avg - Source performance for prior 12 month period\n",
    "local_bo_qty - Amount of stock orders overdue\n",
    "deck_risk - Part risk flag\n",
    "oe_constraint - Part risk flag\n",
    "ppap_risk - Part risk flag\n",
    "stop_auto_buy - Part risk flag\n",
    "rev_stop - Part risk flag\n",
    "went_on_backorder - Product actually went on backorder. **This is the target value.**\n",
    "~~~\n",
    "\n",
    "**Note**: This is a real-world dataset without any processing.  \n",
    "There will also be warnings due to fact that the 1st column is mixing integer and string values.  \n",
    "The last column is what we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Train_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "In this section, goal is to figure out:\n",
    "\n",
    "* which columns we can use directly,  \n",
    "* which columns are usable after some processing,  \n",
    "* and which columns are not processable or obviously irrelevant (like product id) that we will discard.\n",
    "\n",
    "Then process and prepare this dataset for creating a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take samples and examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,12:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:3,18:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns that are obviously irrelevant or not processable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8001)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique values of string columns\n",
    "\n",
    "Now try to make sure that these Yes/No columns really only contains Yes or No.  \n",
    "If that's true, proceed to convert them into binaries (0s and 1s).\n",
    "\n",
    "**Tip**: use [unique()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) function of pandas Series.\n",
    "\n",
    "Example\n",
    "\n",
    "~~~python\n",
    "print('went_on_backorder', dataset['went_on_backorder'].unique())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E8002)\n",
    "# ----------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see **nan** also as possible values representing missing values in the dataset.\n",
    "\n",
    "We fill them using most popular values, the [Mode](https://en.wikipedia.org/wiki/Mode_%28statistics%29) in Stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert yes/no columns into binary (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8003)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all columns should be either int64 or float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this section, design an operationalized machine learning pipeline, which includes:\n",
    "\n",
    "* Anomaly detection\n",
    "* Dimensionality Reduction\n",
    "* Train a model\n",
    "\n",
    "You can add more notebook cells or import any Python modules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 1st pipeline \n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8004)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 2nd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8005)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 3rd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8006)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document the cross-validation analysis for the three models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your analysis in this cell\n",
    "# (Question #E8007)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"background:yellow\">Don't forget to share your chosen models and their cross-validation performance with the class on the dicussion board for module 8.</span>** \n",
    "\n",
    "---\n",
    "\n",
    "# Retrain a model using the full training data set\n",
    "\n",
    "## Train\n",
    "Use the full training data set to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8008)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8009)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the trained model from the pickle file\n",
    "### Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8010)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test your new model using the testing data set.\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Add code below this comment  (Question #E8011)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E8012)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E8013)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
