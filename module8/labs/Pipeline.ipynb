{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Pipelines\n",
    "\n",
    "This notebook uses the built-in Digits dataset you saw previously in the semester, \n",
    "where the task is the recognize / classify written digits.\n",
    "\n",
    " * Derived from Scikit Learn Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the segements of the pipe\n",
    "\n",
    "Here we define a pipeline as an ordered list of classes that will take data.\n",
    "In the example below:\n",
    "  1. Data --> PCA --> Data_Features\n",
    "  1. Data_Features --> LinearSVC --> Classifications\n",
    "\n",
    "Therefore, \n",
    "  1. Data --> Pipeline --> Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    # Named step \"reduce_dim\" ... uses code module PCA\n",
    "    ('reduce_dim', PCA()),\n",
    "    # Named step \"classify\" ... uses code module LinerSVC\n",
    "    ('classify', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning \n",
    "### is part of the machine learning process\n",
    "\n",
    "This is the process of evaluating a collection of model hyperparameters when seeking the optimal performance.\n",
    "We saw a similar technique used on many of the clustering libraries that performed this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the possible number of features to reduce to\n",
    "N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "# Classify C parameter to explore\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "param_grid = [\n",
    "    {\n",
    "        # Options to instantiate the PCA model\n",
    "        'reduce_dim': [PCA(iterated_power=7), NMF()],\n",
    "        \n",
    "        #####################################\n",
    "        # Parameters of the estimators in the \n",
    "        # pipeline can be accessed using the \n",
    "        # <estimator>__<parameter> syntax:\n",
    "        # So: reduce_dim = <estimator> and it, the PCA, takes a parameter n_components\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        # So: classify = <estimator> and it, the LinearSVC, takes a parameter C\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        # A second set of tests cases for hyperparameters\n",
    "        'reduce_dim': [SelectKBest(chi2)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a search grid (collection of parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data then fit the Grid\n",
    "\n",
    "This fitting does an exhaustive search across the hyperparameter space,\n",
    "each time re-using the pipeline for the movement from raw data to classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "grid.fit(digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Note the grid has cross-validation results stored in .cv_results_['mean_test_score']\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "\n",
    "bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "               (len(reducer_labels) + 1) + .5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "COLORS = 'bgrcmyk'\n",
    "for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n",
    "\n",
    "plt.title(\"Comparing feature reduction techniques\")\n",
    "plt.xlabel('Reduced number of features')\n",
    "plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\n",
    "plt.ylabel('Digit classification accuracy')\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that reducing the digits data to 8 features through \n",
    "PCA gets us the best classification performance.\n",
    "\n",
    "We can now go back and rebuild our model using this knowledge of the \n",
    "performance and hyperparameter relationship.\n",
    "\n",
    "#### Additional Pipelining Examples:\n",
    " * [Text Feature Extraction](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)\n",
    " * [Feature Map Approximation for RBF Kernels](http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py)\n",
    " * [PCA to Logistic Regression](http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py)\n",
    " \n",
    "##### Feature Unions\n",
    "Feature Union is closely related to the pipelining.\n",
    "  * [Scikit Learn Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    "  * Read more here: http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
