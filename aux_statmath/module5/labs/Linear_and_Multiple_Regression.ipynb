{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Multiple Regression\n",
    "\n",
    "In this module, our aim is to better understand what makes linear regression tick. \n",
    "We'll begin with simple linear regression and make our way to multiple regression and, \n",
    "in the next notebook, we will get our feet wet with logistic regression.\n",
    "\n",
    "### Simple Linear Regression \n",
    "\n",
    "In simple linear regression, we want to predict the values of one variable based on the values of another variable. These are often denoted as `x` and `y`, where `x` is the variable that predicts `y`. \n",
    "These are also referred to as the independent (`x`) and dependent (`y`) variables. \n",
    "For example, it is probably reasonable to assume that the amount of time that an individual spends studying correlates to the grade that the individual obtains on an exam. \n",
    "In this case, the variable, \"exam grade\" is **dependent** on the variable \"time spent studying\".\n",
    "\n",
    "  - `x = study time` \n",
    "  - `y = score on a test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we have some data that evaluates these two variables. \n",
    "Using the given information about how x and y are related, \n",
    "we can then create a linear model to predict future values of y for any particular values of x. \n",
    "In other words, we want to estimate y based on x. \n",
    "\n",
    "\n",
    "We can represent a simple linear regression line with the following equation, $y = \\beta_0 + \\beta_{1}x$, \n",
    "where $y$ is a point we are predicting given the estimated intercept ($\\beta_0$) and slope ($\\beta_1$) at a given point $x$. \n",
    "Following our example, given the amount of time someone studies ($x$), what would their expected grade on their exam ($y$) be? \n",
    "But how do we find the linear regression line that best approximates the linear relationship? \n",
    "**Minimization of the squared errors.**\n",
    "In other words, we create a line that minimizes the squared distance from each point to the residual line.\n",
    "\n",
    "We can make three simple assumptions for simple linear regression model: \n",
    "\n",
    "Given our linear equation, $y^i = \\beta_0 + \\beta_1.{x_1}^i + \\epsilon^i$,&nbsp;&nbsp;&nbsp; where $ i = 1, 2, . . . ,n $\n",
    "    \n",
    "i) We assume that the relationship between x and y is linear\n",
    "\n",
    "ii) There is an error associated with each $y_i$\n",
    "\n",
    "iii) We lastly assume that the errors($\\epsilon$) are independent and identically normally distributed with mean 0 and variance $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data for `x` and `y` for observation `i` in the dataset. \n",
    "Using the equation above, we make the prediction $\\beta_0 + \\beta_1.x_1$. \n",
    "Since the coefficients $\\beta_0$ and $\\beta_1$ have to be same for all observations we often make some error which we call $\\epsilon_i$(epsilon). \n",
    "These error terms are also called residuals. \n",
    "The error will be zero if all the points lie perfectly on a straight line, which rarely happens. \n",
    "So, every model has some error. \n",
    "The best model is the one with coefficients that has small error terms. \n",
    "\n",
    "\n",
    "student | study time | test score\n",
    "--------|------------|-----------\n",
    "$1$     | $x_1$      | $y_1$\n",
    "$...$   | $...$      | $...$\n",
    "$i$     | $x_i$      | $y_i$\n",
    "$i + 1$ | $x_{i+1}$    | $y_{i+1}$\n",
    "$...$   | $...$      | $...$\n",
    "$n$ | $x_{n}$    | $y_{n}$\n",
    "\n",
    "The above table is just an illustration of the mathematical notation presented above in tabular format using the example of student study time and student test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to understand residuals of data using cars data in datasets package...  \n",
    "\n",
    "**Speed and stopping distance of cars:** \n",
    "The cars dataset in the datasets package has two variables: speed and dist. \n",
    "Take a quick look at some of the values in the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(datasets)\n",
    "head(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed represents how fast the car was going (x) in miles per hour and dist (y) measures how far it took the car to stop, in feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple linear regression model\n",
    "reg <- lm(cars$dist ~ cars$speed)\n",
    "\n",
    "library(ggplot2)\n",
    "\n",
    "# The predicted values for dependent variable 'dist' can be accessed by referring to fitted.values of the model as shown below.\n",
    "pred_values<-reg$fitted.values\n",
    "\n",
    "# Calculate the error terms, i.e. the difference between each point and the regression line. \n",
    "diff <- cars$dist-pred_values     \n",
    "\n",
    "# Display the summary of the model\n",
    "summary(reg)\n",
    "\n",
    "#plot the regression line and residual errors\n",
    "qplot(x=cars$speed, y=cars$dist)+\n",
    "        geom_line(y=pred_values)+ # Draw the regression line with predicted values\n",
    "        geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=pred_values, color=\"error\"))+ # Draw residuals, lines \n",
    "                                                                    # from each point to the regression line.\n",
    "        geom_smooth(method=lm,level = 0.95)+ # Add linear regression line, by default includes 95% confidence region\n",
    "        geom_hline(aes(yintercept=mean(cars$dist)))+ # baseline model. line drawn at mean(dist).\n",
    "       labs(title=\"residual errors\", color=\"series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know how good our model is? \n",
    "Using `summary()` on the model we fitted will show the metrics measuring the quality of the model.\n",
    "\n",
    "### Observations from the model `reg`\n",
    "\n",
    "* **Residuals:** You saw what residuals are from the linear model we fit on car speed and dist variables, the difference between the actual values of the dependent variable (dist) and the predicted values from our regression `pred_values`. In our model, the lowest difference is `-29.07` and highest is 43.2. \n",
    "\n",
    "\n",
    "* **Significance stars: ** The stars indicate significance level. The number of asterisks displayed correspond to the p-value calculated for that variable. A variable is significant if its p value is less than 0.05, as you learned in module 2. $^{***}$(3 stars) for high significance  and $^*$(1 star) for low significance. In our model, speed has three stars indicating a strong relationship between speed and dist variables.\n",
    "\n",
    "\n",
    "* **Estimated Coeffecient:** The estimated coefficient is the value of slope calculated by the regression. It calculates the slope of the line.\n",
    "\n",
    "\n",
    "* **Standard Error of the Coefficient Estimate: ** It's the measure of the variability in the estimate for the coefficient. The lower this value the better it is and its value is relative to the value of the coefficient. As a rule of thumb, it is suggested to have this value at least an order of magnitude less than the coefficient estimate. In our model, the std error of the parent variable is 0.4155 against 3.9324 which is the estimate of the coefficient. So, it's significant.\n",
    "\n",
    "\n",
    "* **t-value of the Coefficient Estimate: ** Score that measures whether or not the coefficient for this variable is meaningful for the model. We normally use p-value or stars(*) to test the variable significance.\n",
    "\n",
    "\n",
    "* **Variable p-value: ** As explained above, this value should be as small as possible. \n",
    "\n",
    "\n",
    "* **Residual Std Error / Degrees of Freedom: ** The Residual Std Error is just the standard deviation of your residuals. For a normal distribution, the 1st and 3rd quantiles should be 1.5 +/- the std error. In our model, the first quantile Q1(-9.525) is a little over 1.5 standard deviations away from the standard error 15.38.\n",
    "\n",
    "\n",
    "* **R-squared: ** It's the metric to measure the goodness of fit of the model. The higher the value the better the model is, with 1 being the highest. For our model, it is 0.6511 which is not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/residuals.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple scatterplot of the data is shown above. \n",
    "We have drawn two regression lines. \n",
    "The black line corresponds to regression line drawn using the average distance a car took to stop. \n",
    "It's the baseline model which gives largest residuals, i.e. differences between each point, with the black line. \n",
    "The baseline model has an intercept of 43.\n",
    "\n",
    "The blue line corresponds to a regression line drawn using the model \"dist ~ speed\". \n",
    "If you take point A on the plot, its actual `y` value is somewhere around 82 but using our model we predict its value to be somewhere around 36. \n",
    "So, the residual for point A is 82-36=46, which is calculated as '$actual$ $value - predicted$ $value$'. \n",
    "The linear regression model we have fit has an intercept value of -17.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of the Squared Errors\n",
    "\n",
    "One of the measures of the quality of a regression line is the Sum of the Squared Errors (**SSE**), \n",
    "which is the sum of all the squared residuals or error terms.\n",
    "\n",
    "Let's assume we have $n$ data points in our data. Then **SSE** is calculated as... \n",
    "\n",
    "$$SSE = {(\\epsilon_1)^2 + (\\epsilon_2)^2+......+ (\\epsilon_n)^2}$$\n",
    "\n",
    "In other words, $\\epsilon_1$ is the residual of the first observation, $\\epsilon_2$ is the residual of the second observation, \n",
    "and $\\epsilon_n$ is the residual of the $n^{th}$ observation where $n$ is the number of observation in the data set.\n",
    "\n",
    "The line which gives the minimal sum of all of these residuals is the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"R-squared\"></a>\n",
    "\n",
    "\n",
    "### Other error measures:\n",
    "\n",
    "SSE can be difficult to interpret for two reasons:\n",
    "\n",
    "    a) SSE depends on the number of data points. If you repeat an experiment with twice as much data the new SSE might be twice as big. The increase doesn't mean it's a bad model. \n",
    "    \n",
    "    b) Sum of Squared Errors is in squared units of the dependent variable. \n",
    "    \n",
    "To avoid the above issues, Root Mean Squared Error(**RMSE**) is often used. \n",
    "\n",
    "#### Root Mean Squared Error\n",
    "\n",
    "RMSE divides SSE by N and takes the square root of the result. So, it is normalized by `N` and is in the same units as dependent variable. \n",
    "\n",
    "$$RMSE = {\\sqrt{\\dfrac{SSE}{N}}}$$\n",
    "\n",
    "#### R-Squared \n",
    "\n",
    "\n",
    "Another common error measure for Linear Regression is $R^2$. The $R^2$ is a better choice because it compares the best model with the baseline model. The baseline model is the one which doesn't use any variables. The black line above is the baseline model. It is the simple average of `y` values. The baseline model predicts the average value of the dependent variable regardless of the value of independent variable `x`.  \n",
    "\n",
    "The Sum of Squared Errors for the baseline is called the Total Sum of Squares(**SST**). Let's calculate the SSE of our model and SST for our baseline model. \n",
    "\n",
    "Using the above values for SSE and SST we can calculate the $R^2$ value for the model. It is given as $R^2 = 1- {\\dfrac{SSE}{SST}}$\n",
    "\n",
    "                11353\n",
    "        =  1 - -------\n",
    "                32538\n",
    "                \n",
    "        =  0.65108\n",
    "        \n",
    "Compare this result with the summary of the model above. The $R^2$ values should match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST <- sum((cars$dist - mean(cars$dist))^2)\n",
    "SST\n",
    "\n",
    "# You can find the residuals of the regression model by referring to the residuals term of the model as shown below.\n",
    "SSE = sum(reg$residuals^2)\n",
    "\n",
    "# SSE is the sum of squares of all residuals which is calculated below\n",
    "SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning the sum of squares\n",
    "\n",
    "When a regression is calculated for a variable, it divides the variation in the dependent variable Y into two parts: \n",
    "the variation of the predicted scores (Y') and the variation of the errors of prediction. \n",
    "The variation of Y (SSY) is the sum of the squared deviations of Y from the mean of Y. \n",
    "In the population, the formula is\n",
    "\n",
    "SSY = $\\sum (Y-\\mu_{y})^2 $\n",
    "\n",
    "where $\\mu_y$ is the mean of Y. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model above, the mean of Y (dist) is 42.98 and SSY is 32538.98. \n",
    "SSY is calculated in the cells below.\n",
    "\n",
    "SSY can be partitioned into two parts: the sum of squares predicted (SSY_pred) and the sum of squares error (SSE). \n",
    "The sum of squares predicted (SSY_pred) is the sum of the squared deviations of the predicted scores \n",
    "subtracted from the mean predicted score. \n",
    "In the above model, it is 21185.4589489051, as calculated below (SSY_pred). \n",
    "The sum of squared error (SSE) is 11353.5210510949, as calculated below. This can be summed up as:\n",
    "\n",
    "$$SSY = SSY\\_pred + SSE$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of squares of errors SST which is the total variation in Y(dist). SST is the same as SSY. \n",
    "SSY <- sum((cars$dist - mean(cars$dist))^2) \n",
    "SSY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the squares of difference between predicted values of the dist variable and the mean value of predictions.  \n",
    "# Sum of squares of difference between dist and its mean is taken\n",
    "\n",
    "SSY_pred = sum((mean(fitted(reg))-fitted(reg))^2) \n",
    "\n",
    "# SSY = SSY_pred + SSE should hold true as shown below.\n",
    "paste(\"SSY: \", SSY)\n",
    "paste(\"SSY_pred + SSE : \",SSY_pred + SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SSY is the total variation, the SSY_pred is the variation explained, and the SSE is the variation unexplained. \n",
    "Therefore, the proportion of variation explained can be computed as:\n",
    "\n",
    "$$Proportion\\ explained = \\frac{SSY\\_pred}{SSY}$$\n",
    "\n",
    "Similarly, the proportion not explained is:\n",
    "\n",
    "$$Proportion\\ not\\ explained = \\frac{SSE}{SSY}$$\n",
    "\n",
    "There is an important relationship between the proportion of variation explained and Pearson's correlation: \n",
    "$R^2$ is the proportion of variation explained. \n",
    "Therefore, if r = 1, then, naturally, the proportion of variation explained is 1; \n",
    "if r = 0, then the proportion explained is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$R^2$ is a better choice of error measure because it captures the value added from using a linear regression model over just predicting the average outcome for every data point. \n",
    "\n",
    "In $R^2 = 1- {\\dfrac{SSE}{SSY}}$,  &nbsp;&nbsp;&nbsp;&nbsp;$0 \\leq SSE \\leq SSY$ and $0 \\leq SSY$ \n",
    "\n",
    "Because **SSE** and **SSY** are sums they cannot be less than 0. \n",
    "A linear regression model will never be worse than a baseline model because if we replace the independent variable coefficient with 0 we will get a baseline model. \n",
    "So, in the worst case **SSE** will be equal to **SSY** and the $R^2$ will be 0. \n",
    "This means no improvement over the baseline model. \n",
    "In the best case, our model makes no errors and the **SSE** is equal to 0. So, the $R^2$ is equal to 1. \n",
    "When $R^2$ is 1, it means you have a perfect or almost perfect predictive model.\n",
    "\n",
    "$R^2$ is better because it is unitless and universally interpretable. \n",
    "It is still sometimes hard to interpret $R^2$ because...\n",
    "\n",
    "i) Good models for easy problems will have $R^2$ equal to 1\n",
    "\n",
    "ii) Models for hard problems may have $R^2$ closer to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error of the Estimate\n",
    "\n",
    "The standard error of the estimate is a measure of the accuracy of predictions. \n",
    "We know the regression line is the line that minimizes the sum of squared deviations of a prediction (also called the sum of squares error). \n",
    "The standard error of the estimate is closely related to this quantity and is defined as below:\n",
    "\n",
    "$\\sigma_{est}$ = $\\sqrt{\\frac{\\sum(Y-Y')^2}{N}}$\n",
    "\n",
    "where $σ_{est}$ is the standard error of the estimate, Y is an actual score, \n",
    "Y' is a predicted score, and N is the number of pairs of scores. \n",
    "The numerator is the sum of squared differences between the actual scores and the predicted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values of dist using 'reg' regression model.\n",
    "pred_values=as.vector(fitted(reg))\n",
    "\n",
    "# Mean of the predicted values.\n",
    "pred_values_mean = mean(fitted(reg))\n",
    "\n",
    "# Calculate the differences between predicted dist values and the mean of predicted values.\n",
    "pred_values=pred_values_mean-pred_values\n",
    "\n",
    "# # Sum of squares of difference between dist and its mean is taken\n",
    "pred_values_squares_sum = sum(pred_values^2)\n",
    "\n",
    "# Calculate Standard error of the estimate based on above formula.\n",
    "standard_error_estimate = sqrt(pred_values_squares_sum/(nrow(cars))) \n",
    "paste(\"standard error of the estimate\", standard_error_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence intervals for slope**\n",
    "\n",
    "When plotting the regression line to predict wine price, you can shade a region on both sides of regression line. \n",
    "The geom_smooth(method=lm) command adds a linear regression line and includes a 95% confidence region by default. \n",
    "If you want to play with confidence level add the parameter `level`. \n",
    "Below is an example worked out to show the same regression line with different confidence intervals. \n",
    "You can observe that as the confidence region is increased, \n",
    "the more observations are included in the shaded region... but the accuracy of the model also decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gridExtra)\n",
    "\n",
    "grid.arrange(\n",
    "    ggplot(cars, aes(x=speed, y=dist)) +\n",
    "    geom_point() +   \n",
    "    geom_smooth(method=lm,level = 0.90),   # Add linear regression line, includes 90% confidence region\n",
    "    \n",
    "    ggplot(cars, aes(x=speed, y=dist)) +\n",
    "    geom_point() +   \n",
    "    geom_smooth(method=lm,level = 0.99),   # Add linear regression line, includes 99% confidence region\n",
    "nrow=1,ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confint() computes confidence intervals for one or more parameters in the fitted model. \n",
    "#For \n",
    "confint(reg, level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))\n",
    "plot(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Normality Assumption\n",
    "\n",
    "**Normal Q-Q plot:** \n",
    "The Normal Q-Q plot helps to check the normality of the residuals graphically. \n",
    "From the normal Q-Q plot above, \n",
    "we see that the distribution of the residuals are not aligned with the straight dotted line. \n",
    "If the assumption of normality were true, \n",
    "then the points will be randomly scattered about the dotted straight line. \n",
    "But here we see a slight departure from normality in that the dots show systematic clustering on one side of the line.\n",
    "We would say there is some evidence that the residuals are not perfectly normal. \n",
    "Look out for any curvature or substantial departures from the straight line.\n",
    "\n",
    "\n",
    "There are many statistical tests of normality.\n",
    "Here we will use the Shapiro-Wilk test.\n",
    "\n",
    "We perform the Shapiro-Wilk test, using the shapiro.test function from the stats package. \n",
    "The hypotheses are\n",
    "    \n",
    "$H_0$ : the residuals are normally distributed\n",
    "                \n",
    "                versus\n",
    "    \n",
    "$H_1$ : the residuals are not normally distributed.\n",
    "\n",
    "The results are below using R..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro.test(residuals(reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these data the assumption of normality of the residuals would be rejected at the $\\alpha$ = 0.05 significance level, \n",
    "but the regression model is reasonably robust to deviations from the normality assumption. \n",
    "As long as the residual distribution is not highly skewed, the regression estimators will perform reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##### Constant Variance Assumption\n",
    "**Scale-Location: ** \n",
    "The Scale-Location plot helps to determine if the spread of the residuals changes over time (or index). \n",
    "Residuals do not have constant variance so it's difficult to determine. \n",
    "In order to check the constant variance assumption we must standardize the residuals before plotting. \n",
    "\n",
    "We estimate the standard error of $E_i$ with $s_{E_i} = s\\sqrt{(1-h_{ii})}$ and define the standardized residuals $R_i, i = 1,2,..,n$\n",
    "\n",
    "$$R_i = \\frac{E_i}{s\\sqrt{(1-h_{ii})}}$$\n",
    "\n",
    "As we look at a scatterplot of $\\sqrt{|R_i|}$ versus $\\hat{Y}_i$ we would expect under the regression assumptions to \n",
    "see a constant band of observations, indicating no change in the magnitude of the observed distance from the line. \n",
    "We want to watch out for a fanning-out of the residuals, or a less common funneling-in of the residuals. \n",
    "Both patterns indicate a change in the residual variance and a consequent departure \n",
    "from the regression assumptions, the first an increase and the second a decrease. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For these data the residuals are slightly fanning-out.\n",
    "\n",
    "#### Testing the Constant Variance Assumption\n",
    "\n",
    "We will use the Breusch-Pagan (BP) test to decide whether the variance of the residuals is non-constant. \n",
    "The null hypothesis is that the variance is the same for all observations and the \n",
    "alternative hypothesis is that the variance is not the same for all observations. \n",
    "\n",
    "We reject the null hypothesis if BP is too large, \n",
    "which happens when the explained variation in the new model is large relative to the unexplained variation in the original model. \n",
    "We do it in R with the bptest function from the `lmtest` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "bptest(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these data we would not reject the null hypothesis at the $\\alpha$ = 0.05 level. \n",
    "There is relatively weak evidence against the assumption of constant variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independence Assumption\n",
    "\n",
    "**Residuals vs Fitted:** \n",
    "\n",
    "One of the strongest of the regression assumptions is the one regarding independence. \n",
    "Departures from the independence assumption are often exhibited by correlation \n",
    "(or autocorrelation, literally, self-correlation) present in the residuals. \n",
    "There can be positive or negative correlation. \n",
    "\n",
    "Positive correlation is displayed by positive residuals followed by positive residuals, \n",
    "and negative residuals followed by negative residuals. \n",
    "Looking from left to right, this is exhibited by a cyclical feature in the residual plots, \n",
    "with long sequences of positive residuals being followed by long sequences of negative ones.\n",
    "\n",
    "On the other hand, negative correlation implies positive residuals followed by negative residuals, \n",
    "which are then followed by positive residuals, etc. \n",
    "Consequently, negatively correlated residuals are often associated with an alternating pattern in the residual plots. \n",
    "There is no obvious cyclical wave pattern or structure to the residual plot we have above. \n",
    "\n",
    "**Testing the Independence Assumption**: \n",
    "Statistically to test whether there is evidence of autocorrelation in the residuals or not can be done with the Durbin-Watson test. \n",
    "It is performed with the `dwtest` function from the `lmtest` package. \n",
    "We will conduct a two sided test that the correlation is not zero, \n",
    "which is not the default (the default is to test that the autocorrelation is positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "dwtest(reg, alternative = \"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we do not reject the null hypothesis at the $\\alpha$ = 0.05 significance level, \n",
    "since there is very little evidence of nonzero autocorrelation in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing observations\n",
    "\n",
    "There are two types of observations with which we must be especially careful:\n",
    "1. Influential observations are those that have a substantial effect on our estimates, predictions, or inferences. A small change in an influential observation is followed by a large change in the parameter estimates or inferences.\n",
    "\n",
    "2. Outlying observations are those that fall fall far from the rest of the data. They may be indicating a lack of fit for our regression model, or they may just be a mistake or typographical error that should be corrected. Regardless, special attention should be given to these observations. An outlying observation may or may not be influential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "There are three ways that an observation (xi; yi) may be an outlier: \n",
    "it can have an xi value which falls far from the other x values, \n",
    "it can have a yi value which falls far from the other y values, \n",
    "or it can have both its xi and yi values falling far from the other x and y values.\n",
    "\n",
    "** Leverage:**\n",
    "Leverage statistics are designed to identify observations which have x values that are far away from the rest of the data. \n",
    "In the simple linear regression model the leverage of xi is denoted by hii and defined by\n",
    "\n",
    "$$h_{ii} = \\frac{1}{n}+\\frac{(x_i - \\bar{x})^2}{\\sum_{k=1}^{n} (x_k - \\bar{x})^2}, \\quad i=1,2,3,...,n $$\n",
    "\n",
    "if the distance from $x_i$ to x is large relative to the other x’s then $h_{ii}$ will be close to 1. \n",
    "Leverages satisfies $0 \\le h_{ii} \\le 1$\n",
    "\n",
    "A rule of thumb is to consider leverage values to be large if they are more than double their average size (which is 2=n). \n",
    "So, leverages larger than 4=n are suspect. \n",
    "Another rule of thumb is to say that values bigger than 0.5 indicate high leverage, \n",
    "while values between 0.3 and 0.5 indicate moderate leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardized and Studentized Deleted Residuals: **\n",
    "We have already seen standardized residuals $r_i$ under the Constant Variance Assumption above. \n",
    "They are merely residuals that have been divided by their respective standard deviations.\n",
    "\n",
    "$$R_i = \\frac{E_i}{s\\sqrt{(1-h_{ii})}}, \\quad i=1,2,3,...,n $$\n",
    "\n",
    "Values of $|R_i| > 2$ are extreme and suggest that the observation has an outlying y-value.\n",
    "Now delete the $i_{th}$ case and fit the regression function to the remaining n - 1 cases, \n",
    "producing a fitted value $\\hat{Y_{(i)}}$ with deleted residual \n",
    "$$D_i = Y_i - \\hat{Y_{(i)}}$$\n",
    "\n",
    "so that the studentized deleted residuals $t_i$ is defined by\n",
    "\n",
    "$$t_i = \\frac{D_i}{S_{(i)}/(1-h_{ii})}, \\quad i=1,2,...,n$$\n",
    "\n",
    "We can calculate the standardized residuals with the rstandard function. \n",
    "The input is the `lm` object, which is reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres <- rstandard(reg)\n",
    "sres[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out which observations have standardized residuals larger than two with the command below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres[which(abs(sres) > 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that observations 23, 35, and 49 are potential outliers with respect to their y-value. \n",
    "We can compute the studentized deleted residuals with rstudent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdelres <- rstudent(reg)\n",
    "sdelres[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should compare these values with critical values from a `t` (df = n - 3) distribution, \n",
    "which in this case is `t` (df = 50 - 3 = 47). \n",
    "We can calculate a 0.005 quantile and check with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0.005 <- qt(0.005, df = 47, lower.tail = FALSE)\n",
    "sdelres[which(abs(sdelres) > t0.005)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that observations 23 and 49 have a large studentized deleted residuals. \n",
    "The leverages can be found with the hatvalues function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leverage <- hatvalues(reg)\n",
    "leverage[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leverage[which(leverage > 4/50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that observations 1, 2, and 50 have leverages bigger than double their mean value. \n",
    "These observations would be considered outlying with respect to their x value (although they may or may not be influential)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influential Observations\n",
    "\n",
    "It is always a concern when a single observation plays too large a role in the regression model. \n",
    "Hence, it becomes desirable to check to see how much our estimates and predictions would change if one of the observations were not included in the analysis.\n",
    "If an observation changes the estimates/predictions a large amount, \n",
    "then the observation is influential and should be subjected to a higher level of scrutiny.\n",
    "\n",
    "We measure the change in the parameter estimates as a result of deleting an observation with DFBETAS.\n",
    "\n",
    "The signs of the DFBETAS indicate whether the coeffcients would increase or decrease as a result of including the observation. \n",
    "If the DFBETAS are large, then the observation has a large impact on those regression coeffcients. \n",
    "We label observations as suspicious if their DFBETAS have magnitude greater 1 for small data or $2/ \\sqrt{n}$ for large data sets.\n",
    "\n",
    "We can calculate the DFBETAS with the dfbetas function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb <- dfbetas(reg)\n",
    "head(dfb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the inclusion of the first observation slightly increases the intercept and slightly decreases the coeffcient on speed. \n",
    "We can measure the influence that an observation has on its fitted value with DFFITS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff <- dffits(reg)\n",
    "dff[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule of thumb is to flag observations whose DFFITS exceeds one in absolute value, but there are none of those in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cook’s Distance: **\n",
    "\n",
    "The DFFITS are good for measuring the influence of a single fitted value but we may want to measure the influence an observation has on all of the fitted values simultaneously. \n",
    "Cook's distance is used to measure this statistic. \n",
    "To assess the significance of Cook's D, we compare to quantiles of an f (df1 = 2, df2 = n - 2) distribution. \n",
    "A rule of thumb is to classify observations falling higher than the 50th percentile as being extreme.\n",
    "\n",
    "As a common rule of thumb an observation with a value of Cook's D over 1.0 has too much influence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the observations that have relatively large values of Cook's D. \n",
    "A conventional cut-off point is 4/n, where n is the number of observations in the data set. \n",
    "We will use this criterion to select the values to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooksD <- cooks.distance(reg)\n",
    "cooksD[1:5]\n",
    "plot(reg, which =4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations with the largest Cook’s D values are labeled, \n",
    "hence we see that observations 23, 39, and 49 are suspicious. \n",
    "However, we need to compare this to the quantiles of an f (df1 = 2; df2 = 48) distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F0.50 <- qf(0.1, df1 = 2, df2 = 48)\n",
    "cooksD[which(cooksD > F0.50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one observation with an extreme Cook’s distance. \n",
    "After scrutiny we can either include or discard it from dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Let's work with wine price dataset for multiple regression... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data <- read.csv('/dsa/data/all_datasets/wine price/wine.csv')\n",
    "head(wine_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Regression\n",
    "\n",
    "The multiple regression model is similar to one variable regression model but has a coefficient term for each independent variable `x`.  \n",
    "Mathematically, it can be represented as \n",
    "\n",
    "$$\n",
    "y^i = \\beta_0 + \\beta_1.{x_1}^i + \\beta_2.{x_2}^i + ... + \\beta_k.{x_k}^i +\\epsilon^i, \\quad i = 1, 2, . . . , n.\n",
    "$$ \n",
    "\n",
    "We predict the dependent variable `y` using independent variables ${x_1}$, ${x_2}$, \n",
    "through ${x_k}$ and so on. `K` here represents the number of independent variables in the model. \n",
    "\n",
    "$\\beta_0$ is the coefficient for intercept term and $\\beta_1$ through $\\beta_k$ are coefficients for independent variables. \n",
    "`i` is used to denote a particular observation in the dataset. \n",
    "The best model is selected, the same as it was above. We will minimize the Squared errors (**SSE**), \n",
    "using error terms $\\epsilon^i$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Let's continue working with the wine price dataset to fit a multiple regression model for predicting wine price. \n",
    "Let's try to fit single variable regression models using each independent variable to predict wine price. \n",
    "Fitting simple linear regression models using all the independent variables gave $R^2$ values as listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "variable | $R^2$\n",
    "---------|-------\n",
    "Year     | 0.20 \n",
    "WinterRain|0.02\n",
    "AGST|0.44\n",
    "HarvestRain|0.32\n",
    "Age|0.20\n",
    "FrancePop|0.22\n",
    "\n",
    "Based on the above table, we see that Average Growing Season Temparature (`AGST`) has an $R^2$ value of 0.44. \n",
    "We will build our multiple regression model using `AGST` and keep adding variables one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_reg1 <- lm(Price ~ AGST, data=wine_data)\n",
    "summary(multi_reg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the table above, `HarvestRain` is the one with next highest $R^2$ value. So, we will include HarvestRain in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_reg2 <- lm(Price ~ AGST + HarvestRain, data=wine_data)\n",
    "summary(multi_reg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant increase in $R^2$ value after adding the `HarvestRain` model. \n",
    "The `HarvestRain` model is able to explain 70% of the variation in the data. \n",
    "The adjusted R-squared value has increased from 0.4105 to 0.6808. \n",
    "Keep an eye on the adjusted R-squared value, as we will examine it closely soon.\n",
    "Finally, we will add FrancePop as our third predictor of our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_reg3 <- lm(Price ~ AGST + HarvestRain + FrancePop, data=wine_data)\n",
    "summary(multi_reg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there is a good improvement in $R^2$ value from 0.70 to 0.79. \n",
    "``FrancePop` is able to explain variation in the data that `AGST` and `HarvestRain` couldn't. \n",
    "Adjusted R-squared value has increased from 0.6808 to 0.7638. \n",
    "Add `Year` as our fourth predictor of our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_reg4 <- lm(Price ~ AGST + HarvestRain + FrancePop + Year, data=wine_data)\n",
    "summary(multi_reg4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is barely any increase in the $R^2$ value from 0.7933 to 0.7947. \n",
    "Year did not contribute anything to the model. \n",
    "The adjusted R-squared value has decreased from 0.7638 to 0.7537.\n",
    "So, when you keep adding predictors to the model that don't contribute much, \n",
    "there is a penalty on the model in the form of Adjusted R-squared. \n",
    "If you keep adding more variables there won't be a significant improvement in $R^2$ value but \n",
    "Adjusted R-squared value keeps decreasing which suggests there is no use in adding the variables to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
