{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the data set width\n",
    "\n",
    "\n",
    "Reducing the data set width comes in two flavors:\n",
    "  1. Feature Selection - Selecting from existing features\n",
    "  1. Dimensionality Reduction - Using numerical methods to alter the feature space from known variables to computed variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "----\n",
    "If you are dealing with multivariate data, most of the times the data has many variables in it. \n",
    "Not all features are equally significant. \n",
    "You should be able to make better predictions using the minimum possible number of features from the dataset.\n",
    "When the data is huge, computation time matters a lot. \n",
    "Building models with a minimum features will help in reducing the computational effort. \n",
    "\n",
    "Feature selection acts like a filter, eliminating features that aren’t useful. \n",
    "It helps in building predictive models free from correlated variables, biases, and unwanted noise. \n",
    "You might be interested in knowing which features of your data provide the most information about the target variable of interest. \n",
    "For example, suppose we’d like to predict the species of Iris based on sepal length and width as well as petal length and width (using the iris dataset in R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the above four features provides the “purest” segmentation with respect to the target? Or put differently, if you were to place a bet on the correct species and could only ask for the value of one feature, which feature would give you the greatest likelihood of winning your bet?\n",
    "\n",
    "### Filter Methods: \n",
    "\n",
    "These methods apply a statistical measure and assign a score to each feature. The features are selected to be kept or removed from the dataset. The methods are often univariate or with regard to the dependent variable. Some of the  methods that fall into this category include the Chi squared test, information gain, and correlation coefficient scores. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Chi squared test:** As discussed in module 1, chi square test/goodness of fit test will check whether significant differences occur within a single category in a categorical variable. We can know the distribution of a variable; if values are equally distributed among different categories then the variable is not providing any new information.\n",
    "    \n",
    "Let's see how it works on iris data. \n",
    "\n",
    "syntax: chisq.test(x, p)\n",
    "\n",
    "- x: a numeric vector\n",
    "- p: a vector of probabilities of the same length of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = c(50, 50, 50)        # observed frequencies\n",
    "expected = c(0.333333333333333,0.333333333333333,0.333333333333333)      # expected proportions\n",
    "\n",
    "chisq.test(x = observed, p = expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of the test is 1, which is greater than the significance level alpha = 0.05. We can conclude that the observed proportions are not significantly different from the expected proportions.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Entropy: ** \n",
    "\n",
    "$$H(p_1 \\dots p_n) = -\\sum_{i=1}^{n} p_i\\log_2 p_i$$\n",
    "\n",
    "Where $p_i$  is the probability of value i and n  is the number of possible values. For example in the iris dataset, we have 3 possible values for Species (Setosa, Versicolor, Virginica), each representing $\\frac{1}{3}$ of the data. Therefore\n",
    "\n",
    "$$\\sum_{i=1}^{3} \\frac{1}{3}_i \\log_2 \\frac{1}{3}_i = 1.59$$\n",
    "\n",
    "Note: following equation $(1/3)*log2(1/3)*3$ will give the result above. \n",
    "\n",
    "----\n",
    "\n",
    "<div style=\"float:left;width:600px\" id=\"container\">\n",
    "    <div id=\"leftContainer\" style=\"float:left;width:500px;\">\n",
    "        <p><b>Example:</b> What is the entropy of a group in which all examples belong to the same class?</p>\n",
    "    </div>\n",
    "    <div id=\"rightContainer\" style=\"float:right;width:100px;\">\n",
    "        <img src=\"../images/minimum_entropy.PNG\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    entropy = - 1 $log_2$1 = 0\n",
    "\n",
    "Not a good training set for learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:600px\" id=\"container\">\n",
    "    <div id=\"leftContainer\" style=\"float:left;width:500px;\">\n",
    "        <p><b> Example:</b> What is the entropy of a group with 50% in either class?</p>\n",
    "    </div>\n",
    "    <div id=\"rightContainer\" style=\"float:right;width:100px;\">\n",
    "        <img src=\"../images/maximum_entropy.PNG\" align=\"center\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    entropy = -0.5 $log_2$ 0.5 – 0.5 $log_2$ 0.5 = 1\n",
    "    \n",
    "A good training set for learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "* **Information gain: **\n",
    "\n",
    "Information gain helps in making two important decisions when building decision trees on data. What is the best split(s) and which is the best variable to split a node.\n",
    "\n",
    "Along a similar line, we want to determine which attribute in a given set of training feature vectors is most useful for discriminating between the classes to be learned.\n",
    "\n",
    "    - Information gain tells us how important a given attribute of the feature vectors is.\n",
    "    - We will use it to decide the ordering of attributes.\n",
    "$$IG = H_p - \\sum_{i=1}^{n} p_{ci}H_{ci}$$\n",
    "\n",
    "Where H_p  is the entropy of the parent (the complete, unsegmented dataset), n  is the number of values of our target variable (and the number of child segments), $p_{ci}$  is the probability that an observation is in child i (the weighting), and $H_{ci}$  is the entropy of child (segment) i.\n",
    "\n",
    "Consider following data with 30 elements of which 16 elements are green circles and remaining 14 are pink crosses. \n",
    "\n",
    "<img src=\"../images/circles_and_crosses.PNG\">\n",
    "\n",
    "We know,\n",
    "**$$Information Gain = entropy(parent) – [average entropy(children)]$$**\n",
    "\n",
    "Subset 1 Child entropy =  $-\\Bigg(\\frac{13}{17} \\log_2 \\frac{13}{17}\\Bigg) - \\Bigg(\\frac{4}{17} \\log_2 \\frac{4}{17}\\Bigg) = 0.787$\n",
    "\n",
    "Subset 2 Child entropy =  $-\\Bigg(\\frac{1}{13} \\log_2 \\frac{1}{13}\\Bigg) - \\Bigg(\\frac{12}{13} \\log_2 \\frac{12}{13}\\Bigg) = 0.391$\n",
    "\n",
    "Parent entropy =  $-\\Bigg(\\frac{16}{30} \\log_2 \\frac{16}{30}\\Bigg) - \\Bigg(\\frac{14}{30} \\log_2 \\frac{14}{30}\\Bigg) = 0.996$\n",
    "\n",
    "(Weighted) Average Entropy of Children = $\\Bigg(\\frac{17}{30} * 0.787 \\Bigg) + \\Bigg(\\frac{13}{30} * 0.391 \\Bigg) = 0.615$\n",
    "\n",
    "    \n",
    "    Information Gain = 0.996 - 0.615 = 0.38 for this split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods: \n",
    "\n",
    "Wrapper methods use a subset of features and train a model using them. Based on the results drawn from the previous model, features are either added or removed from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "**Forward Selection:** Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model until the addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "**Backward Elimination:** In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "**Recursive Feature elimination:** It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the features left until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "Read more about Recursive Feature Elimination implementation in the caret package. \n",
    "\n",
    "[Feature selection using Caret package](https://www.r-bloggers.com/feature-selection-with-carets-genetic-algorithm-option/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "-----\n",
    "\n",
    "Let's continue the discussion with the communities and crime dataset. \n",
    "The data is socio-economic data with a total of 1994 instances and 128 features. \n",
    "Out of the 128 variables, 122 are predictive, 5 are non-predictive and one variable is a target variable. \n",
    "The first five variables are non predictive so we don't have to consider them when building the model.\n",
    "\n",
    "The dataset has missing values. \n",
    "The per-capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States-- namely murder, rape, robbery, and assault. \n",
    "There was apparently some controversy in some states concerning the counting of rapes. \n",
    "These resulted in missing values for rape, which resulted in incorrect values for per capita violent crime. \n",
    "These cities are not included in the dataset. \n",
    "\n",
    "Missing values should be treated before building any models. \n",
    "All numeric data is normalized into the decimal range 0.00-1.00 using an unsupervised, equal-interval binning method. \n",
    "Read the description about the dataset by opening a terminal and running this command:\n",
    "\n",
    "```Bash\n",
    "less /dsa/data/all_datasets/crime/readme.txt\n",
    "```\n",
    "\n",
    "The actual data doesn't have any column headers. \n",
    "You need to grab the headers information from the readme file. \n",
    "We have to do a little bit of data carpentry before we can start using the data to apply linear regression on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headers information is present in readme file. \n",
    "Keep this information in a separate file called names.txt so we can access only the part of data we are interetsed in. The headers data in names.txt has so much unwanted information.\n",
    "A sample record is shown below\n",
    "\n",
    "    '-- state: US state (by number) - not counted as predictive above, but if considered, should be consided nominal (nominal)'\n",
    "\n",
    "The only thing we are interested in is the first word in every line, which is the actual column name. \n",
    "So read the data separating every word using the parameter sep=\"\". \n",
    "Header will be FALSE, because we don't have the header in the actual data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names= read.csv('/dsa/data/all_datasets/crime/names.txt',header = FALSE,sep=\"\")\n",
    "head(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute names are extracted but they still need some cleaning. Every attribute name has a ':' appended at the end. Get rid of the ':' from every word using gsub() function. It will replace characters in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute names are in 2nd column. Extract them. \n",
    "column_names=column_names[,2]\n",
    "\n",
    "# The first argument to gsub() ':' is replaced with second argument ''(nothing here) from every string in names.\n",
    "column_names=gsub(':','',column_names)\n",
    "head(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set to assign these names to crime dataset.\n",
    "\n",
    "**Note** Error expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines of code below and run it.\n",
    "crime_data <- read.csv('/dsa/data/all_datasets/crime/communities_and_crime.txt',header=FALSE)\n",
    "names(crime_data)=column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error\n",
    "-----\n",
    "The error\n",
    "\n",
    "    Notebook Validation failed: \"Error in names(crime_data) = column_names: 'names' attribute [132] must be the same length as the vector [128]\\nTraceback:\\n\" is not of type 'array':\n",
    "    \"Error in names(crime_data) = column_names: 'names' attribute [132] must be the same length as the vector [128]\\nTraceback:\\n\"\n",
    "\n",
    "is saying something about the lengths of vector column_names and names() attribute. Check the lengths of column_names vector and number of columns in crime_data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol(crime_data)\n",
    "length(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you understand what is happening here. There are 132 names in column_names vector which we are trying to assign to 128 columns/variables in crime_data dataframe. Some how we ended up extracting 132 names instead of 128. If we observe the names vector closely we can see what are those extra names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"-\", \"and\", \"(numeric\", \"Part\" are the four names that are created. Once we eliminate these we should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below command, we are selecting strings in names vector which are not in the specified list using the negation \n",
    "#operator'!'\n",
    "column_names = column_names [! column_names %in% c('-', 'and', '(numeric', 'Part')]\n",
    "length(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have names for our columns in actual crime_data, let's assign them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(crime_data)=column_names\n",
    "head(crime_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets\n",
    "\n",
    "#### How will you check the accuracy or how good is the fit of your model?\n",
    "\n",
    "You cannot build and test the model on the same data. It's meaningless. You have to test the accuracy of the model on unknown test data. R has libraries to split the data into train and test datasets. \n",
    "\n",
    "Split the dataset into training and testing datasets. We can do this using the catools package, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set.seed(x) function is used to reproduce the data when using the same input. It helps to split the data in same equal partitions \n",
    "# no matter how many times you split it.\n",
    "set.seed(144)\n",
    "\n",
    "# install.packages(\"caTools\",repo=\"http://cran.uk.r-project.org\")\n",
    "library(caTools)\n",
    "\n",
    "split = sample.split(crime_data$ViolentCrimesPerPop, SplitRatio = 0.7)\n",
    "\n",
    "crime_train_data = subset(crime_data, split == TRUE)\n",
    "\n",
    "crime_test_data = subset(crime_data, split == FALSE)\n",
    "\n",
    "nrow(crime_train_data)\n",
    "nrow(crime_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(crime_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is not Feature selection. Even though both try to reduce the number of attributes in the dataset, dimensionality reduction method creates new combinations of attributes whereas the feature selection method includes and excludes attributes present in the data without altering them. Principal Component Analysis, Singular Value Decomposition, Factor Analysis and Sammon’s Mapping, etc. are all examples of dimensionality reduction.\n",
    "\n",
    "Here are some of the simplest of techniques for dimensionality reduction/variable exclusion...\n",
    "\n",
    "**Missing Values Ratio:** Columns with many missing values carry less useful information. Thus, if the number of missing values in a column is greater than a threshold value it can be removed.\n",
    "\n",
    "**Low Variance Filter:** Columns with little variance in data carry little information. Thus, if the number of values in a column is less than a threshold value it can be removed. Variance is range dependent. Therefore, data should be normalized before applying this technique.\n",
    "\n",
    "**High Correlation Filter:** Columns with high correletion provide almost the same information. Only one of them is enough to feed data to the model. Correlation is scale sensitive. So, column normalization should be done for a meaningful correlation comparison.\n",
    "\n",
    "**Random Forests / Ensemble Trees:**. Decision Tree Ensembles or random forests are useful for feature selection in addition to the classfication of data that they do. Trees are constructed with attributes as nodes. If an attribute is selected as best split, it is likely to be the most informative feature of dataset.\n",
    "\n",
    "**Principal Component Analysis (PCA):**. Principal Component Analysis (PCA) is a statistical technique takes n features of the dataset to transform into a new set of n coordinates called principal components. The transformation helps the first principal component to explain the largest possible variance. The components following have the next highest possible variance without any correletion with other components.\n",
    "[Additional Reading](https://www.r-bloggers.com/principal-component-analysis-using-r/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(crime_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(crime_train_data$LemasSwFTFieldPerPop=='?')\n",
    "table(is.na(crime_train_data$LemasSwFTFieldPerPop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variables who have missing values filled with `?`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(crime_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "##### Centering and Standardizing Variables\n",
    "\n",
    "Standardizing the variables is very important if we have to perform principal component analysis on the variables. \n",
    "If the variables are not standardized, then variables with large variances dominate other variables.\n",
    "\n",
    "When the variables are standardized, they will all have variance 1 and mean 0. This would allow us to find the principal components that provide the best low-dimensional representation of the variation in the original data, without being overly biased by those variables that show the most variance in the original data.\n",
    "\n",
    "We will use `scale()` function In R to standardize the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_vars <- as.data.frame(scale(crime_train_data[!sapply(crime_train_data,class) %in% c('factor')]))\n",
    "dim(standard_vars)\n",
    "head(standard_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the means and standard deviations of the variables. The means will be nearly equal to zero and all standard deviations will equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(standard_vars,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(standard_vars,sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(prcomp)\n",
    "crime_train_data_pca <- prcomp(standard_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(crime_train_data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Principal Components to Retain\n",
    "\n",
    "A scree plot helps us to decide on number of principal components to be retained. The plot will summarize the PCA analysis results. `screeplot()` function in R will help us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot(crime_train_data_pca, type=\"lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious change in slope in the scree plot occurs at component 7, therefore first six components should be retained.\n",
    "\n",
    "Another approach to decide on number of PCA components to choose is by using Kaiser’s criterion. It suggests that we should only retain principal components for which the variance is above 1 (on standardized variables). We can check this by finding the variance of each of the principal components. The standard deviations of PCA components are saved in a standard variable called sdev. You can access it in crime_train_data_pca dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(crime_train_data_pca$sdev)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components 1 through 14 have variance above 1. Using Kaiser’s criterion, we can retain the first fourteen principal components.\n",
    "\n",
    "One more method to decide on number of PCA components to retain is to keep as few components as required to explain at least some minimum amount of the total variance. For example, if you want to explain at least 70% of the variance, we will retain the first eight principal components, as we can see from the output of `summary(crime_train_data_pca)` that the first eight principal components explain 70% of the variance (while the first four components explain 56%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of Principal components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the principal components are stored in a named element `x` of the variable returned by `prcomp()`. `x` contains a matrix where the first column contains the first principal component, the second column the second component, and so on.\n",
    "\n",
    "Thus, `housing_prices_pca$x[,1]` contains the first principal component, and `housing_prices_pca$x[,2]` contains the second principal component.\n",
    "\n",
    "We will make a scatterplot of the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "pca_comp1_comp2 <- ggplot(crime_train_data, aes(x=crime_train_data_pca$x[,1],y=crime_train_data_pca$x[,2]))\n",
    "\n",
    "pca_comp1_comp2+geom_point(alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total number of elements in the dataset\n",
    "len = length(as.matrix(crime_train_data))/length(crime_train_data)\n",
    "\n",
    "biplot(crime_train_data_pca, xlabs = rep( '.', len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Analysis\n",
    "\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. Much like the cluster analysis grouping similar cases, help investigate variable relationships. It groups similar variables into dimensions. The key concept of factor analysis is that multiple observed variables have similar patterns of responses because they are all associated with a latent (i.e. not directly measured) variable. For example, people may respond similarly to questions about income, education, and occupation, which are all associated with the latent variable \"socioeconomic status\". It is possible that variations in n observed variables reflect the variations in just two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables and aims to find independent latent variables.\n",
    "\n",
    "Factor analysis is related to principal component analysis (PCA), but the two are not identical. PCA is a more basic version of exploratory factor analysis (EFA). Factor Analysis reduces the information in a model by reducing the dimensions of the observations.  This procedure has multiple purposes.  It can be used to simplify the data, for example reducing the number of variables in predictive regression models.  If factor analysis is used for these purposes, most often factors are rotated after extraction.  Factor analysis has several different rotation methods—some of them ensure that the factors are orthogonal.  Then the correlation coefficient between two factors is zero, which eliminates problems of multicollinearity in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor Analysis is a method for analyzing the covariation among the observed variables to address following questions:\n",
    "\n",
    "* How many latent factors are needed to account for most of the variation among the observed variables?\n",
    "* Which variables appear to define each factor; hence what labels should we give to these factors?\n",
    "\n",
    "Factors are listed according to factor loadings, or how much variation in the data they can explain. There are two types of factor analysis: exploratory and confirmatory.\n",
    "\n",
    "##### Exploratory factor analysis\n",
    "It is done if a researcher doesn’t have any idea about the structure of data or how many dimensions are in a set of variables. It helps identify complex interrelationships among items and group items that are part of unified concepts.\n",
    "\n",
    "##### Confirmatory Factor Analysis\n",
    "It is used for verification where the researcher has specific idea about the structure of data or how many dimensions are in a set of variables. It helps test the hypothesis that the items are associated with specific factors. Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Analysis vs. PCA\n",
    "Both methods have the aim of reducing the dimensionality of a vector of random variables. Also both methods assume that the modelling subspace is linear (Kernel PCA is a more recent techniques that attempts dimensionality reduction in non-linear spaces).\n",
    "\n",
    "But while Factor Analysis assumes a model (that may fit the data or not), PCA is just a data transformation and for this reason it always exists. Furthermore while Factor Analysis aims at explaining (covariances) or correlations, PCA concentrates on variances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load in the dataset...\n",
    "\n",
    "Let's perform a factor analysis on student subject preferences data. The dataset contains a hypothetical sample of 300 responses on 6 items from a survey of college students’ favorite subject matter. The items range in value from 1 to 5, which represent a scale from Strongly Dislike to Strongly Like. Our 6 items asked students to rate their liking of different college subject matter areas, including biology (BIO), geology (GEO), chemistry (CHEM), algebra (ALG), calculus (CALC), and statistics (STAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data = read.csv(\"/dsa/data/all_datasets/student_prefs/student_subject_preferences.csv\")\n",
    "head(subjects_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(subjects_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `stats` has a function factanal() that can be used to perform factor analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.factors <- 2   \n",
    "\n",
    "fit <- factanal(subjects_data, n.factors,  scores=c(\"regression\"), rotation=\"none\") # number of factors to extract\n",
    "print(fit, digits=2, cutoff=.3, sort=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fit$scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit$loadings[,1:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot factor 1 by factor 2 \n",
    "load <- fit$loadings[,1:2] \n",
    "plot(load,type=\"n\") # set up plot. type='n' tells R not to plot the points. \n",
    "text(load,labels=names(subjects_data),cex=.7) # text() will add variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output maximizes variance for the 1st and subsequent factors, while all are orthogonal to each other.\n",
    "\n",
    "Rotation serves to make the output more understandable, by seeking so-called “Simple Structure”. \n",
    "Simple structure is a pattern of loadings where items load most strongly on one factor, \n",
    "and much more weakly onto the other factors. \n",
    "In other words, varimax rotation is an orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix, which has the effect of differentiating the original variables by the extracted factor. \n",
    "Each factor will tend to have either large or small loadings of any particular variable. \n",
    "A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit <- factanal(subjects_data, n.factors, rotation=\"varimax\")     # 'varimax' is an ortho rotation\n",
    "\n",
    "load <- fit$loadings[,1:2] \n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(load,type=\"n\") # set up plot \n",
    "text(load,labels=names(subjects_data),cex=.7) # add variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at both plots we see that the courses Geology, Biology, and Chemistry all have high factor loadings around 0.8 on the first factor (PA1) while Calculus, Algebra, and Statistics load highly on the second factor (PA2). \n",
    "\n",
    "Note that STAT has a much lower loading on PA2 than ALG or CALC and that it has a slight loading on factor PA1. \n",
    "This suggests that statistics is less related to the concept of Math than Algebra and Calculus. \n",
    "Just below the loadings table, we can see that each factor accounted for around 30% of the variance in responses, \n",
    "leading to a factor solution that accounted for 66% of the total variance in students’ subject matter preference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
