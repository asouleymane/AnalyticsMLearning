{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised Learning\n",
    "\n",
    "\n",
    "Most of the machine learning algorithms fall into one of two categories: supervised or unsupervised. \n",
    "The algorithms we have discussed so far in this course fall under supervised learning domain. \n",
    "For each observation measurement $x_i$, where $i = 1, . . . , n$ there is an associated response measurement $y_i$. \n",
    "The model is fit such that it relates the response to the independent variables, \n",
    "so that it can accurately predict the response for future observations. \n",
    "Classical statistical learning methods such as linear regression and logistic regression \n",
    "as well as modern approaches such as boosting, and support vector machines, \n",
    "operate in the supervised learning domain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in unsupervised learning there are n observations $x_i$ \n",
    "where $i=1, . . . , n$ but no associated response $y_i$. \n",
    "It is not possible to fit a linear regression model or any other models we learned already, \n",
    "since there is no response variable to predict. \n",
    "The situation is referred to as unsupervised because we lack a response variable (knowledge) \n",
    "that can supervise the model's to learning.\n",
    "\n",
    "So, what kind of analysis can be done here? \n",
    "One can try to understand the relationships between the variables, or between the observations. \n",
    "Concept of cluster analysis, or clustering can be applied. \n",
    "The goal of cluster analysis is to ascertain, on the basis of x1, . . . , xn, \n",
    "whether the observations can form relatively distinct groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, \n",
    "in a market segmentation study we can try to cluster the customers on the basis of the variables measured, \n",
    "in order to identify distinct groups of potential customers. \n",
    "Identifying such groups can be of interest because it might be that the groups differ with \n",
    "respect to some property of interest, such as spending habits.\n",
    "\n",
    "The image below shows a clustering data set involving three groups. \n",
    "Each group is shown using a different colored symbol. \n",
    "The left image shows three groups that are well-separated. \n",
    "In this kind of setting, a clustering approach should successfully identify the three groups. \n",
    "In the right image, there is some overlap among the groups, wherein the clustering task is more challenging.\n",
    "\n",
    "<img src=\"../images/cluster1.JPG\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cannot expect a clustering method to assign all of the overlapping points to their correct group. \n",
    "In the example shown above there are only two variables. \n",
    "So one can simply visually inspect the scatterplots of the observations in order to identify clusters. \n",
    "But in reality, data is high-dimensional, having many more than two variables. \n",
    "In those cases, it's not easy to plot the observations.\n",
    "\n",
    "**Note:**\n",
    "When performing clustering (unsupervised machine learning) on data such as shown on the right, there is no _correct answer_.\n",
    "There are only some answers that are better or worse than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "Here the observations are partitioned into a pre-specified number of **K** distinct, non-overlapping clusters. \n",
    "Once the desired number of clusters K is specified, \n",
    "K-means algorithm will assign each observation to exactly one of the K clusters. \n",
    "The observations satisfy the following two conditions:\n",
    "\n",
    "    i) Each observation belongs to at least one of the K clusters.\n",
    "    ii) The clusters are non-overlapping. No observation belongs to more than one cluster\n",
    "\n",
    "For instance, if the $i$th observation is in the $k$th cluster, then $i \\in C_k$. \n",
    "The idea behind K-means clustering is that a good clustering is one for which \n",
    "the within-cluster variation is as small as possible. \n",
    "The within-cluster variation for cluster $C_k$ is a measure W($C_k$) of the amount \n",
    "by which the observations within a cluster differ from each other. \n",
    "Hence, we want to solve the problem in the below equation:\n",
    "\n",
    "$$\\underset{C1,...,CK}{\\mathrm{minimize}} \\bigg\\{\\sum_{k=1}^{K} W(C_k) \\bigg\\} \\quad \\quad \\quad \\quad - (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply, it says that we want to partition the observations into _K_ clusters \n",
    "such that the total within-cluster variation, summed over all _K_ clusters, is as small as possible. \n",
    "In order to make it actionable we need to define the within-cluster variation. \n",
    "There are many possible ways to define this concept, \n",
    "but by far the most common choice involves squared Euclidean distance.\n",
    "That is, we define\n",
    "\n",
    "\n",
    "$$W(C_k) = \\frac{1}{|{C_k}|} \\sum_{i,\\ {i'} \\in C_k} \\sum_{j=1}^P({x_{ij}}-{x_{i'j}})^2  \\quad \\quad \\quad \\quad - (2)$$\n",
    "\n",
    "\n",
    "where |$C_k$| denotes the number of observations in the $k$th cluster. \n",
    "In other words, the within-cluster variation for the $k$th cluster is the sum of all of the \n",
    "pairwise squared Euclidean distances between the observations in the kth cluster, \n",
    "divided by the total number of observations in the kth cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining (1) and (2) gives the optimization problem that defines K-means clustering,\n",
    "\n",
    "$$\\underset{C1,...,CK}{\\mathrm{minimize}} \\bigg\\{\\sum_{k=1}^{K} \\frac{1}{|{C_k}|} \\sum_{i,\\ {i'} \\in C_k} \\sum_{j=1}^P({x_{ij}}-{x_{i'j}})^2\\bigg\\}  \\quad \\quad \\quad \\quad - (3)$$\n",
    "\n",
    "We need an algorithm to solve the above equation (3) that is, \n",
    "a method to partition the observations into K clusters such that the objective of (3) is minimized.\n",
    "It could be a very difficult problem to solve, \n",
    "since there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. \n",
    "This is a huge number unless $K$ and $n$ are tiny. \n",
    "Fortunately, a very simple algorithm exists to provide a local optimum to \n",
    "the K-means optimization problem, which is laid out below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Algorithm : K-Means Clustering\n",
    "\n",
    "-------\n",
    "1. Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations.\n",
    "\n",
    "2. Iterate until the cluster assignments stop changing:\n",
    "    \n",
    "    a. For each of the K clusters, compute the cluster centroid. The $k$th cluster centroid is the vector of the p feature means for the observations in the kth cluster.  \n",
    "    b. Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n",
    "----\n",
    "\n",
    "In Step 2(a) the cluster means for each feature are the constants that minimize the sum-of-squared deviations. \n",
    "In Step 2(b) observations are reallocated. \n",
    "This means that as the algorithm is run, \n",
    "the clustering obtained will continually improve until the result no longer changes. \n",
    "The objective of (3) will never increase. \n",
    "When the result no longer changes, a local optimum has been reached. \n",
    "K-means clustering derives its name from the fact that in Step 2 (a), \n",
    "the cluster centroids are computed as the mean of the observations assigned to each cluster. \n",
    "Because the K-means algorithm finds a local rather than a global optimum,\n",
    "the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm 1. \n",
    "For this reason, it is important to run the algorithm multiple times from different random initial configurations. Then one selects the best solution, for which the objective (3) is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the progression of the algorithm on the toy example\n",
    "\n",
    "<img src=\"../images/kmeans.JPG\">\n",
    "\n",
    "The top left image shows initial observations. \n",
    "Top center image shows step 1 of the algorithm where each observation is randomly assigned to a cluster. \n",
    "The top right image shows step 2(a) of an iteration where cluster centroids are computed. \n",
    "Centroids are shown as large colored disks. \n",
    "In beginning centroids are almost overlapping because the initial cluster assignments were chosen at random. \n",
    "The bottom left image shows is step 2(b) of the iteration, each observation is assigned to the nearest centroid. \n",
    "The bottom center image shows step 2(a) of next iteration, leading to new cluster centroids. \n",
    "The bottom right image shows the results obtained after ten iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The `kmeans()` function performs K-means clustering in R. \n",
    "We begin with a simple simulated set of observations where there are two clusters in the data. \n",
    "The first 25 observations have a mean shift relative to the next 25 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the seed to reproduce the results\n",
    "set.seed(4)\n",
    "\n",
    "# Generate a random normal distribution of 100 values. Generate a matrix out of this normal distribution.\n",
    "x=matrix(rnorm(50*2), ncol=2)\n",
    "\n",
    "# Add 3 to rows 1 to 25 in first column of the matrix\n",
    "x[1:25, 1]=x[1:25, 1] + 3\n",
    "\n",
    "# Subtract 4 from rows 1 to 25 in second column of the matrix\n",
    "x[1:25, 2]=x[1:25, 2] - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform K-means clustering with $K = 2$. \n",
    "nstart=20 implies multiple initial cluster assignments that is K-means clustering \n",
    "will be performed using multiple random assignments in Step 1 of Algorithm 1. \n",
    "The `kmeans()` function will report only the best results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km.fit =kmeans(x,2, nstart =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignments of the 50 observations are contained in km.fit$cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit$cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering perfectly separated the observations into two clusters \n",
    "even though we did not supply any group information to `kmeans()`. \n",
    "We can plot the data, with each observation colored according to its cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, col =(km.fit$cluster +1) , main=\"K-Means Clustering Results with K=2\", xlab =\"\", ylab=\"\", pch =20, cex =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply K-means clustering algorithm on $x$ and divide it into 3 clusters... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed (4)\n",
    "km.fit = kmeans(x,3,nstart = 20)\n",
    "\n",
    "# Clustering results\n",
    "km.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data, with each observation colored according to its cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, col=(km.fit$cluster+1), main=\"K-Means Clustering Results with K=3\", xlab =\"\", ylab=\"\", pch =20, cex =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `kmeans()` with multiple initial cluster assignments explained...\n",
    "\n",
    "Here we compare using nstart=1 to nstart=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "km.fit = kmeans(x,3,nstart = 1)\n",
    "km.fit$tot.withinss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "km.fit = kmeans(x,3,nstart = 20)\n",
    "km.fit$tot.withinss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, km.fit\\$tot.withinss is the total within-cluster sum of squares, \n",
    "which we seek to minimize by performing K-means clustering. \n",
    "The individual within-cluster sum-of-squares are contained in the vector km.fit$withinss. \n",
    "It is strongly recommended to always run K-means clustering with a large value of nstart, \n",
    "such as 20 or 50, since otherwise an undesirable local optimum may be obtained. \n",
    "In addition to using multiple initial cluster assignments, \n",
    "it is also **important to set a random seed using the `set.seed()` function**. \n",
    "This way, the **initial cluster assignments in Step 1 can be replicated**, and the K-means output will be fully reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
