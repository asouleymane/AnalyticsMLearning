{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging, Random Forests, and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, \n",
    "we will talk about bagging and random forests and apply them to the Boston data, \n",
    "using the _randomForest_ package in R. \n",
    "We will see that Bagging is a special case of a random forest. \n",
    "Therefore, the `randomForest()` function can be used to perform both random forests and bagging. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "The decision trees discussed previously suffer from high variance. \n",
    "This means that if we split the training data into two parts at random, \n",
    "and fit a decision tree to both halves, \n",
    "the results that we get could be quite different. \n",
    "In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets. \n",
    "**Bootstrap aggregation**, or _bagging_, \n",
    "is a general-purpose procedure for reducing the variance of a statistical learning method.\n",
    "We introduce it here because it is particularly useful and frequently used in the context of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of $n$ independent observations $Z_1, . . . , Z_n$, \n",
    "each with variance $\\sigma^2$, the variance of the mean $\\hat{Z}$ of the observations is given by $\\sigma^2/n$. \n",
    "In other words, _averaging a set of observations reduces variance_.\n",
    "Hence, a natural way to reduce the variance and increase the prediction accuracy \n",
    "of a statistical learning method is to take many training sets from the population, \n",
    "build a separate prediction model using each training set, \n",
    "and average the resulting predictions. \n",
    "\n",
    "In other words, we could calculate $\\hat{f}^1(x), \\hat{f}^2(x), . . . , \\hat{f}^B(x)$ \n",
    "using B separate training sets, and average them in order to obtain a single low-variance \n",
    "statistical learning model, given by\n",
    "\n",
    "$$\\hat{f}_{avg}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not practical because we generally do not have access to multiple training sets. \n",
    "Instead, we can bootstrap, by taking repeated samples from the (single) training data set. \n",
    "In this approach we generate B different bootstrapped training data sets. \n",
    "We then train our method on the `b`th bootstrapped training set in order to get $\\hat{f}^{∗b}(x)$, \n",
    "and finally average all the predictions, to obtain\n",
    "\n",
    "$$\\hat{f}_{bag}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{*b}(x) $$\n",
    "\n",
    "** This is called bagging. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "####  How bagging is useful in trees \n",
    "\n",
    "While bagging can improve predictions for many regression methods, \n",
    "it is particularly useful for decision trees. \n",
    "To apply bagging to regression trees, \n",
    "we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. \n",
    "Hence, each individual tree has high variance, but low bias. \n",
    "Averaging these B trees reduces the variance. \n",
    "Bagging has been demonstrated to give impressive improvements in accuracy by combining \n",
    "together hundreds or even thousands of trees into a single procedure. \n",
    "Thus far, we have described the bagging procedure in the regression context, \n",
    "to predict a quantitative outcome Y.\n",
    "\n",
    "How can bagging be extended to a classification problem where Y is qualitative? \n",
    "In that situation, there are a few possible approaches, but the simplest is as follows. \n",
    "For a given test observation, we can record the class predicted by each of the B trees, \n",
    "and take a majority vote: the overall prediction is the most commonly occurring class among the B predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Out-of-Bag Error Estimation \n",
    "\n",
    "Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. \n",
    "On average, each bagged tree makes use of around two-thirds of the observations. \n",
    "The remaining one-third of the observations not used to fit a given bagged tree are \n",
    "referred to as the out-of-bag (OOB) observations. \n",
    "We can predict the response for the $ith$ observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the $ith$ observation. \n",
    "\n",
    "In order to obtain a single prediction for the $ith$ observation, \n",
    "we can average these predicted responses (if regression is the goal) \n",
    "or can take a majority vote (if classification is the goal). \n",
    "This leads to a single OOB prediction for the $ith$ observation. \n",
    "An OOB prediction can be obtained in this way for each of the n observations, \n",
    "from which the overall OOB MSE (for a regression problem) or \n",
    "classification error (for a classification problem) can be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable Importance Measures\n",
    "\n",
    "Bagging typically results in improved accuracy over prediction using a single tree.\n",
    "Unfortunately, it can be difficult to interpret the resulting model. \n",
    "One of the advantages of decision trees is the easily interpreted diagram that results. \n",
    "However, when we bag a large number of trees it is no longer possible to represent the \n",
    "resulting statistical learning procedure using a single tree, \n",
    "and it is no longer clear which variables are most important to the procedure. \n",
    "Thus, **bagging improves prediction accuracy at the expense of interpretability**.\n",
    "\n",
    "Although the collection of bagged trees is much more difficult to interpret than a single tree, \n",
    "one can obtain an overall summary of the importance of each predictor using the \n",
    "RSS (for bagging regression trees) or the Gini index (for bagging classification trees).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "In the case of bagging regression trees, we can record the total amount that the RSS is decreased\n",
    "due to splits over a given predictor, averaged over all B trees. \n",
    "A large value indicates an important predictor. \n",
    "Similarly, in the context of bagging classification trees, \n",
    "we can add up the total amount that the Gini index is decreased by splits over a given predictor, \n",
    "averaged over all the trees.\n",
    "\n",
    "A graphical representation of the variable importances in the `Heart` data is shown in the below figure. \n",
    "We see the mean decrease in Gini index for each variable, relative to the largest. \n",
    "The variables with the largest mean decrease in Gini index are `Thal`, `Ca`, and `ChestPain`.\n",
    "\n",
    "<img src=\"../images/var_imp.PNG\" height=600 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forests provide an improvement over bagged trees by way of a small tweak that _decorrelates the trees_.\n",
    "As in bagging, we build a number of decision trees on bootstrapped training samples. \n",
    "But when building these decision trees, each time a split in a tree is considered, \n",
    "a random sample of `m` predictors is chosen as split candidates from the full set of p predictors.\n",
    "The split is allowed to use only one of those m predictors. \n",
    "A fresh sample of m predictors is taken at each split, \n",
    "and typically we choose $m \\approx \\sqrt{p}$ that is, \n",
    "the number of predictors considered at each split is approximately equal to \n",
    "the square root of the total number of predictors (4 out of the 13 for the `Heart` data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "In building a random forest, at each split in the tree, \n",
    "the algorithm is not even allowed to consider a majority of the available predictors. \n",
    "There is a clever rationale behind this idea. \n",
    "Suppose that there is one very strong predictor in the data set, \n",
    "along with a number of other moderately strong predictors. \n",
    "Then in the collection of bagged trees,\n",
    "most or all of the trees will use this strong predictor in the top split. \n",
    "\n",
    "Consequently, all of the bagged trees will look quite similar to each other.\n",
    "Hence, the predictions from the bagged trees will be highly correlated. \n",
    "Unfortunately, averaging many highly correlated quantities does not lead to as large of a \n",
    "reduction in variance as averaging many uncorrelated quantities. \n",
    "In particular, this means that bagging will not lead to a substantial reduction \n",
    "in variance over a single tree in this setting.\n",
    "\n",
    "Random forests overcome this problem by forcing each split to consider only a subset of the predictors. \n",
    "Therefore, on average $(p − m)/p$ of the splits will not even consider the strong predictor, \n",
    "and so other predictors will have more of a chance. \n",
    "We can think of this process as **decorrelating the trees**.\n",
    "\n",
    "The main difference between bagging and random forests is the choice of predictor subset size m. \n",
    "For instance, if a random forest is built using m = p, then this amounts simply to bagging. \n",
    "On the `Heart` data, random forests using $m \\approx \\sqrt{p}$ leads to a reduction in both test error and OOB error over bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Boosting\n",
    "\n",
    "Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, \n",
    "fitting a separate decision tree to each copy, \n",
    "and then combining all of the trees in order to create a single predictive model. \n",
    "Notably, each tree is built on a bootstrap data set, independent of the other trees. \n",
    "Boosting works in a similar way, except that the trees are grown sequentially: \n",
    "each tree is grown using information from previously grown trees. \n",
    "Boosting does not involve bootstrap sampling; \n",
    "instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "Like bagging, boosting involves combining a large number of decision trees, $\\hat{f}^1, . . . , \\hat{f}^B$. \n",
    "However, the boosting approach instead learns slowly. \n",
    "Given the current model, we fit a decision tree to the residuals from the model. \n",
    "That is, we fit a tree using the current residuals, rather than the outcome $Y$, as the response. \n",
    "We then add this new decision tree into the fitted function in order to update the residuals. \n",
    "Each of these trees can be rather small, with just a few terminal nodes, \n",
    "determined by the parameter `d` in the algorithm. \n",
    "By fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it does not perform well. \n",
    "The shrinkage parameter $\\lambda$ slows the process down even further, \n",
    "allowing more and different shaped trees to attack the residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bagging and Random Forests in R\n",
    "\n",
    "We will apply bagging and random forests to the Boston data, using the randomForest package. \n",
    "Bagging is simply a special case of a random forest with $m = p$ \n",
    "(where m is the number of predictors selected to build the model \n",
    "and p is the total number of predictors available).\n",
    "Therefore, the `randomForest()` function can be used to perform both random forests and bagging. \n",
    "\n",
    "**We perform bagging as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "library(MASS)\n",
    "set.seed(1)\n",
    "train = sample (1: nrow(Boston ), nrow(Boston )/2)\n",
    "bag.boston =randomForest(medv~.,data=Boston, subset=train, mtry=13, importance=TRUE)\n",
    "bag.boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The argument `$mtry=13$` indicates that all 13 predictors should be considered for each split of the tree; \n",
    "in other words, that bagging should be done. \n",
    "How well does this bagged model perform on the test set?\n",
    "\n",
    "We will compare to a boosting approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.bag = predict (bag.boston, newdata=Boston[-train,])\n",
    "boston.test=Boston[-train,\"medv\"]\n",
    "plot(yhat.bag, boston.test)\n",
    "abline(0,1)\n",
    "mean((yhat.bag - boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test setMSE associated with the bagged regression tree is approximately 13.33, \n",
    "almost half that obtained using an optimally-pruned single tree. \n",
    "We could change the number of trees grown by `randomForest()` using the `ntree` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.boston=randomForest(medv~., data=Boston, subset=train, mtry=13, ntree=25)\n",
    "yhat.bag = predict(bag.boston, newdata=Boston[-train,])\n",
    "mean((yhat.bag-boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growing a random forest proceeds in exactly the same way, \n",
    "except that we use a smaller value of the `mtry` argument. \n",
    "By default, `randomForest()` uses $p/3$ variables when building a random forest of regression trees, \n",
    "and $\\sqrt{p}$ variables when building a random forest of classification trees. \n",
    "Here we use `mtry = 6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed (1)\n",
    "rf.boston=randomForest(medv~.,data=Boston, subset=train, mtry=6, importance=TRUE)\n",
    "yhat.rf = predict(rf.boston, newdata=Boston[-train,])\n",
    "mean((yhat.rf-boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is 11.48; this indicates that random forests yielded an improvement over bagging in this case. Using the `importance()` function, we can view the importance of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance(rf.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two measures of variable importance are reported. \n",
    "The first is based upon the mean decrease of accuracy in predictions on the \n",
    "out of bag samples when a given variable is excluded from the model. \n",
    "The second is a measure of the total decrease in node impurity that results from splits over that variable, \n",
    "averaged over all trees. \n",
    "In the case of regression trees, the node impurity is measured by the training RSS, \n",
    "and for classification trees by the deviance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varImpPlot(rf.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `gbm` package, and within it the `gbm()` function, \n",
    "to fit boosted regression trees to the Boston data set. \n",
    "We run `gbm()` with the option `distribution=\"gaussian\"` if it's a regression problem. \n",
    "If it's a binary classification problem, we would use `distribution=\"bernoulli\"`. \n",
    "The argument `n.trees=5000` indicates that we want 5000 trees and the option `interaction.depth=4` limits the depth of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "set.seed(1)\n",
    "boost.boston=gbm(medv~.,data=Boston[train,], distribution=\"gaussian\",n.trees=5000, interaction.depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() function produces a relative influence plot and also outputs the relative influence statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(boost.boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
