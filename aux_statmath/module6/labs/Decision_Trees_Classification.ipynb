{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification tree we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\n",
    "\n",
    "The task of growing (inducing) a classification tree is typically recursive; that is we load data into a root node and successively split that node into two children, then those children each into two children, and so on.\n",
    "This is referred to as _recursive binary splitting_ to grow a classification tree. \n",
    "The critical technique of learning the model parameters is the analysis of classification error rate per node. \n",
    "Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class. \n",
    "\n",
    "[Dig deeper](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the classification error as such:\n",
    "\n",
    "$$E = 1 - \\max_k(\\hat{p}_{mk})$$\n",
    "\n",
    "Here $\\hat{p}_{mk}$ represents the proportion of training observations in the $mth$ region that are from the $kth$ class. \n",
    "However, it turns out that classification error is not sufficiently sensitive for tree-growing, \n",
    "and in practice two other measures are preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini index\n",
    "\n",
    "$$ G = \\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\ ,$$\n",
    "\n",
    "The Gini index is a measure of total variance across the $K$ classes. \n",
    "It is not hard to see that the Gini index takes on a small value if all of the $\\hat{p}_{mk}$'s are close to zero or one. \n",
    "For this reason the Gini index is referred to as a measure of **node purity** - \n",
    "a small value indicates that a node contains predominantly observations from a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy\n",
    "\n",
    "Cross-entropy is similar to Gini index, substituting $log\\  \\hat{p}_{mk}$ for $(1-\\hat{p}_{mk})$.\n",
    "\n",
    "$$D = -\\sum_{k=1}^K \\hat{p}_{mk}\\ log\\  \\hat{p}_{mk}$$\n",
    "\n",
    "Since 0 ≤ $\\hat{p}_{mk}$ ≤ 1, it follows that 0 ≤ − $\\hat{p}_{mk}$ log $\\hat{p}_{mk}$. \n",
    "One can show that the cross-entropy will take on a value near zero if the $\\hat{p}mk$’s are all near zero or near one. \n",
    "Therefore, like the Gini index, the cross-entropy will take on a small value if the $m^{th}$ node is pure.\n",
    "\n",
    "When building a classification tree, either the Gini index or the cross-entropy are typically used to evaluate the quality of a particular split, \n",
    "since these two approaches are more sensitive to node purity than is the classification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Classification Tree\n",
    "\n",
    "Decision trees can be constructed even in the presence of qualitative predictor variables. \n",
    "For instance, in the Carseat data below, some of the predictors, such as `ShelveLoc` and `Urban`\n",
    "are qualitative. \n",
    "Therefore, a split on one of these variables amounts to assigning some of the qualitative values \n",
    "to one branch and assigning the remaining to the other branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use classification trees to analyze the <span style=\"color:#a5541a\">Carseats</span> data set. \n",
    "In this data, <span style=\"color:#a5541a\">Sales</span> is a continuous variable, and so we begin by recoding it as a binary variable. \n",
    "We use the `ifelse()` function to create a variable, called **High**, \n",
    "which takes on a value of **Yes** if the `Sales` variable exceeds 8, \n",
    "and takes on a value of **No** otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "attach(Carseats)\n",
    "High=ifelse(Sales <= 8, \"No\", \"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the <span style=\"color:#a5541a\">data.frame()</span> function to merge <span style=\"color:#a5541a\">High</span> with the rest of the <span style=\"color:#a5541a\">Carseats</span> data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Carseats = data.frame(Carseats, High)\n",
    "str(Carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the `rpart()` (recursive partition) function to fit a classification tree in order to predict **High** using all variables except `Sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "rpart_tree <- rpart(High~.-Sales, method=\"anova\", data=Carseats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"color:#a5541a\">summary()</span> function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(rpart_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the plot() function to display the tree structure, \n",
    "and the text() function to display the node labels. \n",
    "\n",
    "The argument `pretty=0` instructs R to include the category names for any qualitative predictors, \n",
    "rather than simply displaying a letter for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rpart_tree)\n",
    "text(rpart_tree, pretty=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important indicator of `Sales` appears to be shelving location, \n",
    "since the first branch differentiates `Good` locations from `Bad` and `Medium` locations. \n",
    "\n",
    "If we just type the name of the tree object, R prints output corresponding to each branch of the tree. \n",
    "R displays the split criterion (e.g. `Price<92.5`), \n",
    "the number of observations in that branch, \n",
    "the deviance, \n",
    "the overall prediction for the branch (Yes or No), \n",
    "and the fraction of observations in that branch that take on values of Yes and No. \n",
    "Branches that lead to terminal nodes are indicated using asterisks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a decision treee\n",
    "\n",
    "When we need to estimate the test error.\n",
    "We split the observations into a training set and a test set, \n",
    "build the tree using the training set, and evaluate its performance on the test data.\n",
    "The `predict()` function can be used for this purpose. \n",
    "In the case of a classification tree, the argument `type=\"class\"` instructs R to return the actual class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed (2)\n",
    "train = sample(1:nrow(Carseats), 200)\n",
    "Carseats.test = Carseats[-train,]\n",
    "High.test = High[-train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart_tree <- rpart(High~.-Sales, data=Carseats, subset=train)\n",
    "\n",
    "yhat = predict(rpart_tree, Carseats.test,type =\"class\")\n",
    "\n",
    "table(yhat, High.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The table command here produces are confusion matrix, a contingency table between expected and predicted values.\n",
    "\n",
    "Next, we consider whether pruning the tree might lead to improved results. \n",
    "The function `prune()` performs cross-validation in order to determine the optimal level of tree complexity. \n",
    "Cost complexity pruning is used in order to select a sequence of trees for consideration. \n",
    "We use the argument `FUN=prune.misclass` in order to indicate that we want the classification error rate to guide the cross-validation and pruning process. \n",
    "\n",
    "The `cv.tree()` function reports the number of terminal nodes of each tree considered (size),\n",
    "as well as the corresponding error rate and the value of the cost-complexity parameter used \n",
    "(k, which corresponds to α in (8.4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell takes a little bit of time to execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(e1071)\n",
    "\n",
    "set.seed (3)\n",
    "cpGrid = expand.grid(.cp = seq(0.01,0.5,0.01))\n",
    "\n",
    "train(High~.-Sales, data = Carseats, method = \"rpart\", tuneGrid = cpGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart_tree <- rpart(High~.-Sales, data=Carseats, method=\"class\", cp = 0.5, subset=train)\n",
    "\n",
    "yhat = predict(rpart_tree, newdata = Carseats.test, type = \"class\")\n",
    "\n",
    "table(yhat, High.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model didn't change.\n",
    "Pruning did not alter the model accuracy. \n",
    "\n",
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
