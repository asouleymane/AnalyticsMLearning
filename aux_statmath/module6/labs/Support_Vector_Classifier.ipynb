{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "SVM in short is a generalization of a simple and intuitive classifier called the _maximal margin classifier_.\n",
    "The maximal margin classifier unfortunately cannot be applied to most data sets,\n",
    "since it requires that the classes be separable by a linear boundary.\n",
    "An extension to maximal margin classifier called _support vector classifier_ can be applied in a \n",
    "broader range of cases but cannot handle non-linear cases. \n",
    "**Support Vector Machine** (SVM) is an extension of the support vector classifier that was developed \n",
    "in order to accommodate non-linear class boundaries.\n",
    "Support vector machines are intended for the binary classification setting in which there \n",
    "are two classes but can be applied to the case of more than two classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperplane?\n",
    "\n",
    "In a _p-dimensional space_, a hyperplane is a subspace of dimension p − 1. \n",
    "In two dimensions, a hyperplane is a flat one-dimensional subspace, i.e.  a line.\n",
    "In three dimensions, hyperplane is a flat 2-d subspace a plane. \n",
    "In p > 3 dimensions, it can be hard to visualize a hyperplane. \n",
    "In two dimensions, a hyperplane is defined by the equation $$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0$$\n",
    "\n",
    "The above equation defines the hyperplane. \n",
    "We say any point $X = (X_1,X_2)^T$ that holds true to the above equation is a point on the hyperplane. \n",
    "The above equation is simply the equation of a line, \n",
    "since in two dimensions a hyperplane is a line. \n",
    "This equation, when extended to p-dimensions, gives us \n",
    "\n",
    "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + . . . + \\beta_pX_p = 0 $$\n",
    "\n",
    "any point $X = (X_1,X_2,....,\\beta_pX_p)^T$ that holds true to the equation above is a point on the hyperplane. \n",
    "If the point doesn't satisfy the equation and is greater than the above equation \n",
    "$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + . . . + \\beta_pX_p > 0$ then X lies to one side of the \n",
    "hyperplane or the other side of plane if it’s less than the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../images/svm.JPG\">\n",
    "\n",
    "On the left side of the image above there are two classes of observations, \n",
    "shown in blue and purple, each of which has measurements on two variables. \n",
    "Three separating hyperplanes, out of many possible, are shown in black. \n",
    "\n",
    "In the right image a separating hyperplane is shown in black. \n",
    "The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane. \n",
    "The blue region is the set of points for which line equation is evaluated as > 0, \n",
    "and the purple region is the set of points for which line equation is evaluated as < 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/hyperplane.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum margin classifier\n",
    "\n",
    "The _maximal margin hyperplane_ represents the mid-line of the widest **slab** that we can insert between the two classes. \n",
    "In the above picture the three training observations are equidistant from the maximal margin hyperplane \n",
    "and lie along the dashed lines indicating the width of the margin.\n",
    "These three observations are known as **_support vectors_**, \n",
    "since they are vectors in p-dimensional space and they “support” the maximal margin hyperplane \n",
    "in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "<img src=\"../images/classifier.JPG\">\n",
    "\n",
    "The left most figure above shows that the observations that belong to two classes are \n",
    "not necessarily separable by a hyperplane. \n",
    "A classifier based on a separating hyperplane, \n",
    "which tries to perfectly classify all of the training observations, \n",
    "leads to sensitivity to individual observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As shown in the image towards right, the addition of a single observation compared to \n",
    "middle image leads to a dramatic change in the maximal margin hyperplane. \n",
    "It has only a tiny margin. \n",
    "This is problematic because the distance of an observation from the hyperplane can be \n",
    "seen as a measure of our confidence that the observation was correctly classified. \n",
    "Also, the fact that the maximal margin hyperplane is extremely sensitive to a change in \n",
    "a single observation suggests that it may have over fit the training data.\n",
    "\n",
    "Thus, it is worthwhile to misclassify a few training observations in order to do a better \n",
    "job in classifying the remaining observations. \n",
    "The **support vector classifier**, sometimes called a _soft margin classifier_ does exactly this.\n",
    "Rather than seeking the largest possible margin so that every observation is not only on the \n",
    "correct side of the hyperplane but also on the correct side of the margin,\n",
    "we instead allow some observations to be on the incorrect side of the margin,\n",
    "or even the incorrect side of the hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum margin classifier and the support vector classifier can take care of linear decision boundaries. \n",
    "To handle nonlinear decision boundaries we need **support vector machines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "<img src=\"../images/non_linear_boundary.JPG\">\n",
    "\n",
    "If you look at the picture above, the observations fall into two classes with a non-linear boundary between them. \n",
    "The picture on the right shows a support vector classifier that seeks a linear boundary and \n",
    "consequently performs very poorly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is a nonlinear relationship between the predictors and the outcome we consider \n",
    "enlarging the feature space using functions of the predictors such as quadratic and cubic terms, \n",
    "in order to address this non-linearity. \n",
    "In the enlarged feature space, the decision boundary is in fact linear. \n",
    "But in the original feature space, the decision boundary is of the form q (x) = 0, \n",
    "where q is a quadratic polynomial, and its solutions are generally non-linear.\n",
    "\n",
    "\n",
    "When the support vector classifier is combined with a non-linear kernel, \n",
    "the resulting classifier is known as a support vector machine. \n",
    "A kernel is a function that quantifies the similarity of two observations. \n",
    "Using a kernal amounts to fitting a support vector classifier in a higher-dimensional space \n",
    "involving polynomials of degree **d**, rather than in the original feature space (dimension p). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The image on the left below shows an example of an SVM with a polynomial kernel of degree 3 \n",
    "is applied to the non-linear data shown in the above image. \n",
    "The fit is a substantial improvement over the linear support vector classifier and \n",
    "is a far more appropriate decision rule.\n",
    "The image shown on the right below is an SVM with a radial kernel applied to the same data.\n",
    "\n",
    "<img src=\"../images/non_linear_svm.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's get our hands dirty with some data analysis using support vector machines. \n",
    "\n",
    "The **`svm()`** function can be used to fit a support vector classifier when the \n",
    "argument kernel=\"linear\" is used.  \n",
    "A `cost` argument allows us to specify the cost of a violation to the margin. \n",
    "When the cost argument is small, then the margins will be wide and many support \n",
    "vectors will be on the margin or will violate the margin. \n",
    "When the cost argument is large, then the margins will be narrow and there will be few support \n",
    "vectors on the margin or violating the margin. \n",
    "We begin by generating the observations, which belong to two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the seed to reproduce the experiment results\n",
    "set.seed(1)\n",
    "\n",
    "# Generate a normal distribution of 40 values and form a matrix 'x' out of those values.\n",
    "x=matrix(rnorm(20*2), ncol=2)\n",
    "\n",
    "# Create a vector such that first 10 cells have value -1 and rest 10 cells have value 1.\n",
    "y=c(rep(-1,10), rep(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will increase the value of all columns in x by 1 where y is equal to 1. \n",
    "The condition x[y==1,] selects rows where y=1. \n",
    "The condition y==1 will create a vector of TRUE FALSE values. \n",
    "The indexes in the vector corresponding to rows where y=1 will have TRUE \n",
    "and the indexes where y = -1 will have FALSE. \n",
    "Here we are increasing the values of last 10 rows in x by 1 so that they are linearly separable with first 10 rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x[y==1,] = x[y==1,] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the classes are linearly separable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x values. col=(3-y) will result in the vector of values {4,4...4,2,2,...} where first 10 are 4 and rest 10 are 2. This \n",
    "# will help in plotting the first values in blue and last 10 values in red color. \n",
    "plot(x, col =(3-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are not linearly separable. \n",
    "A red observation is grouped with blue observations on the left side. \n",
    "We will now fit a support vector classifier. \n",
    "In order for the `svm()` function to perform classification instead of SVM-based regression, \n",
    "encode the response as a factor variable.\n",
    "Create a data frame with the response coded as a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat=data.frame(x=x, y=as.factor(y))  # Recode as factor\n",
    "library(e1071)\n",
    "svmfit=svm(y ~., data=dat, kernel =\"linear\", cost =10,scale=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `scale=FALSE` tells the `svm()` function not to scale each feature to \n",
    "have zero mean and standard deviation of one. \n",
    "Depending on the application, one might prefer to use `scale=TRUE`. \n",
    "\n",
    "Now, we can plot the support vector classifier obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply two arguements to plot() function, output of the call to svm() as well as the data used in the call to svm().\n",
    "plot(svmfit, dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region of feature space that is assigned to the −1 class is shown in light blue, \n",
    "and the region that is assigned to the +1 class is shown in purple. \n",
    "The decision boundary between the two classes is linear because of the argument kernel=\"linear\". \n",
    "Note that the second feature (X2) is plotted on the x-axis and the first feature (X1) \n",
    "is plotted on the y-axis, in contrast to the behavior of the usual `plot()` function in R. \n",
    "Here only one observation is misclassified. The support vectors are plotted as crosses \n",
    "and the remaining observations are plotted as circles. \n",
    "Also there are seven support vectors. \n",
    "We can determine their identities as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmfit$index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the summary() command to obtain some basic information about the support vector classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svmfit )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`summary()` tells us that a linear kernel was used with cost=10, and there were seven support vectors. \n",
    "Four support vectors are in one class and three in the other. \n",
    "\n",
    "Now, let's fit the model with smaller cost value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmfit=svm(y~., data=dat, kernel=\"linear\", cost=0.1, scale=FALSE)\n",
    "plot(svmfit, dat)\n",
    "svmfit$index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a smaller value of the cost parameter we got a larger number of support vectors, \n",
    "because the margin is now wider. \n",
    "The `svm()` function does not explicitly output the coefficients of the linear decision boundary \n",
    "obtained when the support vector classifier is fit, nor does it output the width of the margin.\n",
    "\n",
    "\n",
    "The `e1071` library includes a built-in function, `tune()`, to perform cross validation. \n",
    "By default, `tune()` performs ten-fold cross-validation on a set of models of interest. \n",
    "In order to use this function, pass in relevant information about the set of models that are under consideration. \n",
    "The following command compares SVMs with a linear kernel, using a range of values of the cost parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed (1)\n",
    "tune.out=tune(svm,y~.,data=dat, kernel =\"linear\", ranges =list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run summary() on tune.out to access the cross-validation errors for each of the models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(tune.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost=0.1 results in the lowest cross-validation error rate. The best model obtained can be accessed as below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmod = tune.out$best.model\n",
    "summary(bestmod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict() function can be used to predict the class label on a set of test observations,\n",
    "at any given value of the cost parameter. \n",
    "Generate a test data set as train data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtest=matrix(rnorm(20*2), ncol =2)\n",
    "ytest=sample(c(-1,1), 20, rep=TRUE)\n",
    "xtest[ytest==1,] = xtest[ytest==1,] + 1\n",
    "testdat =data.frame(x=xtest, y=as.factor(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class labels of above test observations. \n",
    "The best model with cost=0.1 is used here in order to make predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=predict(bestmod, testdat)\n",
    "table(predict = ypred, truth=testdat$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 of the test observations are correctly classified with cost=0.1. \n",
    "\n",
    "Now consider a situation in which the two classes are linearly separable. \n",
    "Then we can find a separating hyperplane using the `svm()` function. \n",
    "We first further separate the two classes in our simulated data so that they are linearly separable... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[y==1 ,]= x[y==1 ,]+0.5\n",
    "plot(x, col =(y+5) /2, pch =19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the observations are just barely linearly separable. \n",
    "We fit the support vector classifier and plot the resulting hyperplane, \n",
    "using a very large value of cost so that no observations are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=data.frame(x=x,y=as.factor(y))\n",
    "svmfit =svm(y~.,data=dat ,kernel =\"linear\",cost =1e5)\n",
    "summary(svmfit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(svmfit,dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No training errors were made and only three support vectors were used. \n",
    "However, we can see from the figure that the margin is very narrow \n",
    "(because the observations that are not support vectors, indicated as circles, \n",
    "are very close to the decision boundary). \n",
    "It seems likely that this model will perform poorly on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=predict(svmfit, testdat)\n",
    "table(predict = ypred, truth=testdat$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 of the test observations are correctly classified with cost=10000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
