{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Linear Discriminant Analysis or LDA is a technique to classify an object into one of two or more groups \n",
    "depending on a set of features that describes that object. \n",
    "It does so by assigning the object to the group with the highest conditional probability. \n",
    "Remember **[Bayes Rule](../../../Day3/module4/labs/Bayes.ipynb)**?\n",
    "\n",
    "LDA is an extension to logistic regression. \n",
    "Here the distribution of the predictors X is modeled separately in each of the response classes (i.e. given Y), \n",
    "and then Bayes’ theorem is used to flip these observations around into estimates for $P(Y = k\\ |\\ X = x)$.\n",
    "\n",
    "** Why do we need another method, when we have logistic regression?**\n",
    "There are several reasons:\n",
    "* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\n",
    "* If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n",
    "* Linear discriminant analysis is popular when we have more than two response classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes’ Theorem for Classification\n",
    "\n",
    "Suppose there are k (k>=2) classes such that we want an observation to be classified into. \n",
    "The response variable Y can take on K possible distinct and unordered values. \n",
    "Let $π_k$ represent the prior probability that a randomly chosen observation comes from the kth class. \n",
    "This is the probability that a given observation is associated with the $k_{th}$ category of the response variable Y. \n",
    "Let $f_k (X) ≡ P (X = x|Y = k)$ denote the density function of $X$ for an observation that comes from the $k_{th}$ class. \n",
    "In other words, $f_k (x)$ is relatively large if there is a high probability that an observation in the $k_{th}$ class has X ≈ x, \n",
    "and $f_k (x)$ is small if it is very unlikely that an observation in the kth class has $X ≈ x$. \n",
    "Then Bayes’ theorem states that...\n",
    "\n",
    "$$ P(Y = k\\ |\\ X = x) = \\frac{π_k f_k(x)}{\\sum_{l=1}^{K} π_l f_l(x)} \\quad\\quad\\quad\\quad - \\quad (1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above equation is represented using the notation used for logistic regression we will get something like $p_k (X) = P (Y = k\\ |\\ X)$.\n",
    "This means instead of directly computing $p_k (X)$, we can simply plug in estimates of $π_k$ and $f_k (X)$ into the complicated looking equation above. \n",
    "Estimating $π_k$ is easy if we have a random sample of $Ys$ from the population. \n",
    "We compute the fraction of the training observations that belong to the kth class. \n",
    "\n",
    "Estimating $f_k (X)$ is a bit more challenging, \n",
    "$p_k (x)$ is the posterior probability that an observation $X\\ =\\ x$ belongs to the $k_{th}$ class. \n",
    "That is, it is the probability that the observation belongs to the $k_{th}$ class, \n",
    "given the predictor value for that observation. \n",
    "Bayes classifier, classifies an observation to the class for which $p_k (X)$ is largest, \n",
    "has the lowest possible error rate out of all classifiers. \n",
    "Therefore, if we can find a way to estimate $f_k (X)$, then we can develop a classifier that approximates the Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis for p=1 (one predictor)\n",
    "\n",
    "Assume that p = 1, that is, we have only one predictor. \n",
    "We would like to obtain an estimate for $f_k (x)$ that we can plug into (1) above in order to estimate $p_k (x)$. \n",
    "We will then classify an observation to the class for which $p_k (x)$ is greatest. \n",
    "In order to estimate $f_k (x)$, we will first make some assumptions about its form.\n",
    "\n",
    "Suppose we assume that $f_k (x)$ is normal or Gaussian. \n",
    "In the one-dimensional setting, the normal density takes the form…\n",
    "\n",
    "$$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} exp\\bigg(-\\frac{1}{2{\\sigma_k}^2}(x-\\mu_k)^2\\bigg) \\quad\\quad\\quad\\quad - \\quad (2)$$\n",
    "\n",
    "where $\\mu_k$ and ${\\sigma_k}^2$ are the mean and variance parameters for the kth class. \n",
    "For now, let us further assume that ${\\sigma_1}^2 = {\\sigma_2}^2 ... = {\\sigma_k}^2$:  \n",
    "that is, there is a shared variance term across all K classes, which for simplicity we can denote by $\\sigma^2$.\n",
    "\n",
    "Plugging (2) in (1), we find that\n",
    "\n",
    "$$p_k(x) = \\frac{π_k \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\\bigg(-\\frac{1}{2{\\sigma}^2}(x-\\mu_k)^2\\bigg)}\n",
    "{\\sum_{l=1}^{K} π_l \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\\bigg(-\\frac{1}{2{\\sigma}^2}(x-\\mu_l)^2\\bigg)} \\quad\\quad\\quad\\quad - \\quad (3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes classifier involves assigning an observation X = x to the class for which (3) is largest. \n",
    "Taking the log of (3) and rearranging the terms, \n",
    "it is not hard to show that this is equivalent to assigning the observation to the class for which\n",
    "\n",
    "$$\\delta_k(x) = x. \\frac{\\mu_k}{\\sigma^2} - \\frac{{\\mu_k}^2}{2\\sigma^2}+log(\\pi_k)$$\n",
    "\n",
    "is largest. For instance, if K = 2 and $\\pi_1$ = $\\pi_2$, \n",
    "then the Bayes classifier assigns an observation to class 1 if $2x(\\mu_1 − \\mu_2) > {\\mu_1}^2 − {\\mu_2}^2$, \n",
    "and to class 2 otherwise.\n",
    "\n",
    "In this case, the **Bayes decision boundary** corresponds to the point where\n",
    "\n",
    "$$x = \\frac{{\\mu_1}^2 − {\\mu_2}^2}{2(\\mu_1-\\mu_2)} = \\frac{\\mu_1+\\mu_2}{2} \\quad\\quad\\quad\\quad - \\quad (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following image. <img src=\"../images/bayes.PNG\">\n",
    "\n",
    "The dashed vertical line represents the Bayes decision boundary. \n",
    "The two normal density functions that are displayed, $f_1(x)$ and $f_2(x)$, represent two distinct classes. \n",
    "The mean and variance parameters for the two density functions are $\\mu_1$ = −1.25, $\\mu_2$ = 1.25, \n",
    "and $\\sigma_1$ = $\\sigma_2$ = 1. \n",
    "The two densities overlap, and so given that $X = x$, there is some uncertainty about the class to which the observation belongs. \n",
    "If we assume that an observation is equally likely to come from either class that is, $\\pi_1 = \\pi_2$ = 0.5: \n",
    "then by inspection of (4), we see that the Bayes classifier assigns the observation to class 1 if x < 0 and class 2 otherwise. \n",
    "Note that in this case, we can compute the Bayes classifier because we know that X is drawn from a Gaussian distribution within each class,\n",
    "and we know all of the parameters involved. \n",
    "In a real-life situation, we are not able to calculate the Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis for p >1 (more than one predictor)\n",
    "\n",
    "We now extend the LDA classifier to the case of multiple predictors.\n",
    "To do this, we will _assume_ that X = (X1, X2, . . .Xp) is drawn from a multivariate Gaussian \n",
    "(or multivariate normal) distribution, \n",
    "with a class-specific mean vector and a common covariance matrix. \n",
    "\n",
    "To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, \n",
    "we write $X \\sim N (\\mu,\\sum)$. \n",
    "Here $E (X) = \\mu$ is the mean of X (a vector with p components), \n",
    "and $Cov (X) = \\sum$ is the $p × p$ covariance matrix of X. \n",
    "Formally, the multivariate Gaussian density is defined as below...\n",
    "\n",
    "$$f_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} exp\\bigg(-\\frac{1}{2}(x-\\mu)^T\\sum^{-1}(x-\\mu)\\bigg) \\quad\\quad\\quad\\quad - \\quad (5)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of p > 1 predictors, \n",
    "the LDA classifier assumes that the observations in the $k_th$ class are drawn from a multivariate Gaussian distribution $N (μ_k, Σ)$, \n",
    "where $μ_k$ is a class-specific mean vector, \n",
    "and Σ is a covariance matrix that is common to all K classes. \n",
    "Plugging the density function for the $k_th$ class, $f_k (X = x)$, \n",
    "into (1) and performing a little bit of algebra reveals that the Bayes classifier\n",
    "assigns an observation X = x to the class for which \n",
    "\n",
    "$$\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} {\\mu_k}^T \\Sigma^{-1} \\mu_k + log \\pi_k$$\n",
    "\n",
    "is largest. Consider the image below.\n",
    "\n",
    "<img src=\"../images/lda_multi_predictors.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three equally sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. \n",
    "The three ellipses represent regions that contain 95% of the probability for each of the three classes. \n",
    "The dashed lines are the Bayes decision boundaries. \n",
    "Note that there are three lines representing the Bayes decision boundaries \n",
    "because there are three pairs of classes among the three classes.\n",
    "That is, one Bayes decision boundary separates class 1 from class 2, \n",
    "one separates class 1 from class 3, and one separates class 2 from class 3. \n",
    "These three Bayes decision boundaries divide the predictor space into three regions.\n",
    "The Bayes classifier will classify an observation according to the region in which it is located.\n",
    "\n",
    "Once again, we need to estimate the unknown parameters $\\mu_1, . . . , \\mu_K$, $\\pi_1, . . . , \\pi_K$, \n",
    "and $\\Sigma$, the formulas are similar to those used in the one-dimensional case. \n",
    "To assign a new observation X = x,\n",
    "LDA plugs these estimates into (5) and classifies to the class for which $\\hat\\delta_k (x)$ is largest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "We will evaluate the performance of LDA over logistic regression. \n",
    "We are going to use the `Smarket` dataset to predict `Direction`. \n",
    "Fit an LDA model using the `lda()` function, \n",
    "which is part of the MASS library. \n",
    "Notice that the `lda()` syntax for the `lda()` function is identical to that of `lm()` and to that of `glm()`, \n",
    "except for the absence of the family option. \n",
    "\n",
    "We are going to train the model on those observations that occurred before the year 2005, \n",
    "and predict direction for observations in 2005.\n",
    "\n",
    "\n",
    "**Note:** If you decide you want to rerun the cell below, you should first Save the notebook.  \n",
    "Then **restart the kernel** from the notebook menu: `Kernel > Restart and Clear Output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "attach(Smarket)\n",
    "\n",
    "# The object train is a vector of 1,250 elements, corresponding  \n",
    "# to the observations in our data set. The elements of the \n",
    "# vector that correspond to observations that occurred before 2005 \n",
    "# are set to TRUE as they satisfy the condition. \n",
    "# Whereas those that correspond to observations in 2005 are set to FALSE. \n",
    "train = Year<2005\n",
    "\n",
    "# train is a Boolean vector, since its elements are TRUE and FALSE. \n",
    "# So, the TRUE and FALSE values corresponding to each row\n",
    "# will let you subset rows or columns of a matrix. For instance, \n",
    "# the command Smarket[!train,] would pick out a submatrix of the\n",
    "# stock market dataset, corresponding to dates in 2005, since those \n",
    "# are the ones for which the elements of train are FALSE and \n",
    "# `!` operator will reverse the elements of train vector.\n",
    "Smarket.2005= Smarket[!train,]\n",
    "\n",
    "# Check the dimensions of Smarket.2005\n",
    "dim(Smarket.2005)\n",
    "\n",
    "# Save the Direction values corresponding to 2005 dates.\n",
    "Direction.2005 = Direction[!train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "lda.fit=lda(Direction~Lag1+Lag2 ,data=Smarket ,subset=train)\n",
    "lda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below and read about the `lda()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model output indicates that $\\hat{π}_1$ = 0.492 and $\\hat{π}_2$ = 0.508. \n",
    "In other words,  49.2% of the training observations correspond to days during which the market went down. \n",
    "The group means are the average of each predictor within each class, \n",
    "and are used by LDA as estimates of μk. \n",
    "These suggest that there is a tendency for previous 2 days’ returns to be negative on days when the market \n",
    "increases and a tendency for the previous 2 days’ returns to be positive on days when the market declines. \n",
    "\n",
    "The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. \n",
    "In essence, these coefficients map the data onto a line to form the decision space.\n",
    "If (`−0.642×Lag1−0.514×Lag2`) is large, then the LDA classifier will predict a market increase, \n",
    "and if it is small, then the LDA classifier will predict a market decline. \n",
    "\n",
    "Below, the first plot is for observations where market went down and second plot observations show market went up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(lda.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot()` function produces plots of the linear discriminants, \n",
    "obtained by computing (`−0.642 × Lag1 − 0.514 × Lag2`) for each of the training observations. \n",
    "The horizontal axis is the linear combination of Lag1 and Lag2. \n",
    "Both the groups are centered on 0, and have similar spread, which means there is nothing that distinguishes these two groups. \n",
    "The prediction error will be close to 50%, \n",
    "which indicates that you cannot distinguish between the groups. \n",
    "The range of x-axis (-4 to 4) represents the range of Lag1 and Lag2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict()` function returns a list with three elements. \n",
    "The first element, class, contains LDA’s predictions about the movement of the market. \n",
    "The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class. \n",
    "Finally, x contains the linear discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.pred=predict(lda.fit, Smarket.2005)\n",
    "names(lda.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.class =lda.pred$class\n",
    "table(lda.class ,Direction.2005)\n",
    "\n",
    "mean(lda.class == Direction.2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a 50% threshold to the posterior probabilities allows us to recreate the predictions contained in `lda.pred$class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lda.pred$posterior[ ,1]>=0.5)\n",
    "\n",
    "sum(lda.pred$posterior[,1]<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(lda.pred)[1:20,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A posterior prob played important in predicting market direction. \n",
    "If the posterior prob is > 0.5 for up, \n",
    "it resulted in predicting up and if the posterior probability for down is > 0.5, the model predicted down. \n",
    "The 0.5 is the threshold. \n",
    "It also appears that the LD1 is used to calculate the posterior probs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use a posterior probability threshold other than 50% in order to make predictions, \n",
    "then we could easily do so. \n",
    "For instance, suppose that we wish to predict a market decrease only if we are very certain that the market \n",
    "will indeed decrease on that day — i.e., if the posterior probability is at least 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lda.pred$posterior[,1]>.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%.\n",
    "\n",
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
