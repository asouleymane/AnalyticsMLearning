{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look into tree-based methods for regression and classification.\n",
    "These involve stratifying or segmenting the predictor space into a number of simple regions. \n",
    "In order to make a prediction for a given observation, \n",
    "we typically use the mean or the mode of the training observations in the region to which it belongs. \n",
    "Since the set of splitting rules used to segment the predictor space can be summarized in a tree, \n",
    "these types of approaches are known as decision tree methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "*** Predicting Baseball Players’ Salaries Using Regression Trees* ** \n",
    "\n",
    "Let's consider the _Hitters_ data set to predict a baseball player’s `Salary` based on `Years`\n",
    "(the number of years that he has played in the major leagues) and `Hits` \n",
    "(the number of hits that he made in the previous year). \n",
    "We first remove observations that are missing `Salary` values, \n",
    "and log-transform `Salary` so that its distribution has more of a typical bell-shape. \n",
    "Note: Salary is measured in thousands of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows a regression tree fit to this data. \n",
    "It consists of a series of splitting rules, \n",
    "starting at the top of the tree. \n",
    "The top split assigns observations having <span style=\"color:#a5541a\">$Years<4.5$</span> to the left branch. \n",
    "The predicted salary for these players is given by the mean response value for the players \n",
    "in the data set with <span style=\"color:#a5541a\">$Years<4.5$</span>. \n",
    "\n",
    "For such players, the mean log salary is 5.107, and so we make a prediction of $\\epsilon^{5.107}$ \n",
    "thousands of dollars, i.e. \\$165,174, for these players. \n",
    "Players with $Years>=4.5$ are assigned to the right branch, \n",
    "and then that group is further subdivided by Hits. \n",
    "Overall, the tree segments the players into three regions of predictor space: \n",
    " * players who have played for four or fewer years, \n",
    " * players who have played for five or more years and who made fewer than 118 hits last year, and \n",
    " * players who have played for five or more years and who made at least 118 hits last year.\n",
    "\n",
    "<img src=\"../images/simple_decision_tree.PNG\"/>$$Figure\\ 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three regions can be written as \n",
    "\n",
    "$$R_1 ={X | Years<4.5}$$\n",
    "$$R_2 ={X | Years>=4.5,\\quad Hits<117.5}$$\n",
    "$$R_3 ={X | Years>=4.5, \\quad Hits>=117.5}$$\n",
    "\n",
    "The figure below illustrates the regions as a function of Years and Hits. The predicted salaries for these three groups are \\$1,000×$\\epsilon^{5.107}$ = \\$165,174,  \\$1,000×$\\epsilon^{5.999}$ = \\$402,834, and  \\$1,000×$\\epsilon^{6.740}$ = \\$845,346 respectively.\n",
    "\n",
    "<img src=\"../images/decision_regions.PNG\"/>$$Figure\\ 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "In keeping with the tree analogy, the regions R1, R2, and R3 are known as terminal nodes or leaves of the tree. \n",
    "As is the case for Figure 1, decision trees are typically drawn upside down, \n",
    "in the sense that the leaves are at the bottom of the tree. \n",
    "The points along the tree where the predictor space is split are referred to as internal nodes. \n",
    "In Figure 1, the two internal nodes are indicated by the text `$Years<4.5$` and `$Hits<117.5$`. \n",
    "We refer to the segments of the trees that connect the nodes as branches.\n",
    "\n",
    "\n",
    "The regression tree displayed in Figure 1 can be interpreted as follows:\n",
    "<span style=\"color:#a5541a\">Years</span> is the most important factor in determining Salary, \n",
    "and players with less experience earn lower salaries than more experienced players. \n",
    "Given that a player is less experienced,\n",
    "the number of hits that he made in the previous year seems to play little role in his salary.\n",
    "But among players who have been in the major leagues for five or more years, \n",
    "the number of hits made in the previous year does affect salary, \n",
    "and players who made more hits last year tend to have higher salaries. \n",
    "The regression tree shown in Figure 1 is likely an over-simplification of the true relationship between <span style=\"color:#a5541a\">Hits, Years, and Salary</span>. \n",
    "However, it has advantages over other types of regression models where it is easier to interpret, and has a nice graphical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction via Stratification of the Feature Space\n",
    "\n",
    "The two steps below constitute building a regression tree.\n",
    "\n",
    "1. Divide the predictor space — that is, the set of possible values for $X_1,X_2, . . .,X_p$ — into J distinct and non-overlapping regions, $R_1,R_2, . . . , R_J$.\n",
    "2. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "For instance, suppose that in Step 1 we obtain two regions, R1 and R2, \n",
    "and that the response mean of the training observations in the first region is 10, \n",
    "while the response mean of the training observations in the second region is 20. \n",
    "Then for a given observation $X = x$, if $x \\in R_1$ we will predict a value of 10, \n",
    "and if $x \\in R_2$ we will predict a value of 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how regions $R_1, . . .,R_J$ are constructed. \n",
    "The regions could have any shape. \n",
    "However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, \n",
    "for simplicity and for ease of interpretation of the resulting predictive model.\n",
    "The goal is to find boxes $R_1, . . . , R_J$ that minimize the RSS, given by\n",
    "\n",
    "$$\\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - {\\hat{y}}_{R_j})\\ ,$$\n",
    "where ${\\hat{y}}_{R_j}$ is the mean response for the training observations within the $j_{th}$ box. \n",
    "We take a top-down, greedy approach that is known as recursive binary splitting. \n",
    "The approach is top-down because it begins at the top of the tree. \n",
    "It is greedy because at each step of the tree-building process, \n",
    "the best split is made at that particular step, \n",
    "rather than looking ahead and picking a split that will lead to a better tree in some future step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "In order to perform recursive binary splitting, \n",
    "we first select the predictor $X_j$ and the cutpoint `s` such that splitting the predictor space \n",
    "into the regions ${X|X_j < s}$ and ${X|Xj ≥ s}$ leads to the greatest possible reduction in RSS. \n",
    "(The notation ${X|X_j < s}$ means the region of predictor space in which Xj takes on a value less than `s`.) \n",
    "That is, we consider all predictors $X_1, . . .,X_p$, \n",
    "and all possible values of the cutpoint `s` for each of the predictors,\n",
    "and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tree Pruning\n",
    "\n",
    "The process described above may produce good predictions on the training set, \n",
    "but is likely to overfit the data. \n",
    "This is because the resulting tree might be too complex. \n",
    "A smaller tree with fewer splits might lead to lower variance and better interpretation \n",
    "at the cost of a little bias. \n",
    "One possible solution is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. The strategy is to grow a very large tree $T_0$, \n",
    "and then prune it back in order to obtain a subtree. \n",
    "\n",
    "The goal is to select a subtree that leads to the lowest test error rate. \n",
    "Given a subtree, we can estimate its test error using cross-validation or the validation set approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Inducing a Regression Tree\n",
    "\n",
    "\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "\n",
    "2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$.\n",
    "\n",
    "3. Use K-fold cross-validation to choose $\\alpha$. That is, divide the training observations into $K$ folds. For each $k = 1, . . .,K$:\n",
    "\n",
    "    (a) Repeat Steps 1 and 2 on all but the $k{th}$ fold of the training data.\n",
    "    \n",
    "    (b) Evaluate the mean squared prediction error on the data in the left-out $k{th}$ fold, as a function of $\\alpha$.\n",
    "    \n",
    "    Average the results for each value of $\\alpha$, and pick $\\alpha$ to minimize the average error.\n",
    "    \n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fitting Regression Trees with R\n",
    "\n",
    "Here we fit a regression tree to the `Boston` data set. \n",
    "First, we create a training set, and fit the tree to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "set.seed (1)\n",
    "head(Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(Boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample()` function below takes a sample of the specified size (second arguement which is 253) \n",
    "from the elements of x (first arguement which is 506) using either with or without replacement. \n",
    "By default it is without replacement. \n",
    "So, we are randomly selecting indexes to create a train and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = sample(1:nrow(Boston), nrow(Boston)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "\n",
    "fit <- rpart(medv~., method=\"anova\", data=Boston, subset=train)\n",
    "\n",
    "summary(fit) # detailed summary of splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of `printcp()` indicates that only three of the variables have been \n",
    "used in constructing the tree. \n",
    "We now plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotcp(fit) # visualize cross-validation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree \n",
    "plot(fit, uniform=TRUE, \n",
    "     main=\"Regression Tree for Mileage \")\n",
    "text(fit, use.n=TRUE, all=TRUE, cex=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `lstat` measures the percentage of individuals with lower socioeconomic status. \n",
    "The tree indicates that lower values of `lstat` correspond to more expensive houses. \n",
    "The tree predicts a median house price of $46,400 for larger homes in suburbs in which \n",
    "residents have a high socioeconomic status (`rm>=7.437` and `lstat<9.715`).\n",
    "\n",
    "#### Pruning the tree\n",
    "\n",
    "Now we use the `prune()` function to see whether pruning the tree will improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prune the tree \n",
    "pfit<- prune(fit, cp=0.01160389) # from cptable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pruned tree \n",
    "plot(pfit, uniform=TRUE,   main=\"Pruned Regression Tree for House prices\")\n",
    "text(pfit, use.n=TRUE, all=TRUE, cex=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both trees almost look similar.\n",
    "\n",
    "Now we will look at the `rpart` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(\"rpart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree <- rpart(medv~., data=Boston, subset=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?rpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rpart.plot)\n",
    "library(RColorBrewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rpart.plot()` and `RcolorBrewer()` functions help us to create a beautiful plot. \n",
    "`rpart.plot()` plots rpart models.\n",
    "It extends `plot.rpart` and `text.rpart` in the rpart package. \n",
    "`RcolorBrewer()` provides us with beautiful color palettes and graphics for the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rattle)\n",
    "fancyRpartPlot(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?fancyRpartPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the unpruned tree to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(fit, newdata = Boston[-train ,])\n",
    "boston.test = Boston[-train ,\"medv\"]\n",
    "plot(yhat, boston.test)\n",
    "abline (0,1)\n",
    "mean((yhat -boston.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the test set MSE associated with the regression tree is 25.35. \n",
    "The square root of the MSE is therefore around 5.005, \n",
    "indicating that this model leads to test predictions that are within around $5,005 \n",
    "of the true median home value for the suburb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
