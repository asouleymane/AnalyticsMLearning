{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate --> Train, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When constructing a model, data availability may become an issue. \n",
    "In order to avoid overfitting, it is necessary to withhold some portion of the data as a test set. \n",
    "However, overfitting *on the test set* may also occur without a secondary validation step. \n",
    "As such, `scikit` contains a number of methods for cross-validation of data.\n",
    "\n",
    "## References\n",
    "1. [Scikit documentation - GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load dataset (we're working with Iris again)\n",
    "raw = load_iris()\n",
    "X = raw.data[:, :2] # slice off only the first feature (.data is multi-dimensional)\n",
    "y = raw.target # the target data is a single label, so it can all be kept\n",
    "\n",
    "# validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "# we'll use the Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Though a manual CV workflow was described in [the cross-validation lab](./CrossValidation.ipynb), the automated `cross_val_score()` will work well enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automated CV step\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "print(scores) # TODO: visualization of CV process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cross-validation values represent how well (with 1 being a perfect score) the model performed against a small, as-yet-untrained portion of the data for the classification task.\n",
    "\n",
    "## Training the new model\n",
    "Since the CV values are relatively high, we can proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit new model\n",
    "fitted = classifier.fit(X_train, y_train)\n",
    "\n",
    "# GaussianNB.predict() returns class labels (integers)\n",
    "labels = classifier.predict(X_test)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Class %i\" % labels[i])\n",
    "\n",
    "# cut out duplicate labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above - as in the [clustering examples](../resources/clustering.ipynb) seen earlier, \n",
    "groups each model-determined class by its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models through pickling / `joblib`\n",
    "\n",
    "In many cases, the model being trained will be used more than once for the same or different data. \n",
    "For large datasets, training can be computationally expensive as well. \n",
    "For these and other reasons it is often necessary to save a trained model so it can later be loaded.\n",
    "This is relatively easy to do from within `scikit` via `joblib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming some model 'clf' is initialized and trained:\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, 'GaussianIris.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib` makes loading in pickled models similarly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll load to a new variable this time, for clarity\n",
    "loaded_model = joblib.load('GaussianIris.pkl')\n",
    "\n",
    "# and predict with the 'new' model:\n",
    "labels = loaded_model.predict(X_test)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Class %i\" % labels[i])\n",
    "\n",
    "# cut out duplicate labels    \n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results are exactly the same.\n",
    "This is no accident - the model being loaded below is *the same* as the one used above.\n",
    "\n",
    "Please read a little about Pickling (or Serialization) [here](https://docs.python.org/3/library/pickle.html) and [here](https://en.wikipedia.org/wiki/Serialization).\n",
    "\n",
    "## Notes on pickling - safety, efficiency\n",
    "\n",
    "`joblib` (and by extension the default `pickle` module) is by no means your only option for model storage.\n",
    "`cPickle` is a [faster](https://docs.python.org/2.2/lib/module-cPickle.html) \n",
    "C-based implementation of the same pickling algorithm, \n",
    "and for more significant models (as we may cover in later modules) it would be worth looking into.\n",
    "\n",
    "Moreover, consider reading the `scikit` docs on [persistence](http://scikit-learn.org/stable/modules/model_persistence.html)\n",
    "for considerations on the long-term safety of pickling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
