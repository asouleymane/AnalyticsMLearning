{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn: Regression Algorithms\n",
    "\n",
    "*Regression algorithms* are primarily aimed at predicting future values of a (numerical/quantitative) attribute based on some continuous functional model for that attribute.\n",
    " \n",
    "There are a number of algorithms built into the `scikit` suite that are tailor-made to solve regression \n",
    "problems - a few of these are enumerated below, along with their ideal use cases.\n",
    "\n",
    "## Numpy and Scipy\n",
    "\n",
    "`numpy` is the most commonly used general-purpose Python library for mathematics and numerical processing. \n",
    "A sister library, `scipy`, contains functions that are useful for scientific data processing and analysis. \n",
    "Both find use within `scikit` applications in various contexts.\n",
    "This lab will assume basic familiarity with both.\n",
    "\n",
    "## References\n",
    "1. [Scikit Documentation](http://scikit-learn.org/stable/user_guide.html)\n",
    "    * [LinearRegression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)\n",
    "    * [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "    * [Lasso and ElasticNet](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py)\n",
    "    * [Ridge/OLS](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Large datasets: linear regression, SGDRegressor\n",
    "\n",
    "Linear regression algorithms fit a linear model (i.e. a trendline) to the training data.\n",
    "There are a number of these algorithms included in the `scikit` library, \n",
    "so we'll cover two of the simpler implementations: `LinearRegression` and `SGDRegressor`.\n",
    "\n",
    "### Linear Regression\n",
    "`LinearRegression` uses *least-squares regression* to fit its linear model, \n",
    "and is best suited to large data sets (to reduce the effect of random noise and outliers).\n",
    "\n",
    "*This example comes from the `scikit` documentation for `LinearRegression`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and libraries\n",
    "from sklearn import datasets, linear_model\n",
    "diabetes = datasets.load_diabetes()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce the dataset to a single feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2] # index 2 --> mono-feature array\n",
    "\n",
    "# separate out training and test data sets\n",
    "X_train = diabetes_X[:-20]\n",
    "X_test = diabetes_X[-20:]\n",
    "\n",
    "# separate target data into training and test sets\n",
    "y_train = diabetes.target[:-20]\n",
    "y_test = diabetes.target[-20:]\n",
    "\n",
    "# create and fit a model using the training sets\n",
    "model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# plot the model\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, model.predict(X_test), color='blue', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The above line represents the least-squares regression on the dataset, and indicates a positive trend in the data.\n",
    "\n",
    "---\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "*Stochastic gradient descent* or SGD is an efficient and (typically) effective linear regression algorithm with some advantages over `LinearRegression`; \n",
    "a mathematical explanation can be found on here: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "SGD is natively implemented within `scikit` via `SGDRegressor`, \n",
    "and is best suited to large datasets with low dimensionality (i.e. small number of relevant variables).\n",
    "\n",
    "##### Data considerations and a note on other algorithms\n",
    "\n",
    "The native implementation of `SGDRegressor` is designed to work with `numpy` arrays of floating point values for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "X = np.array(dataset.iloc[:,:-1])[:, [1]]\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4) # split into training/test sets\n",
    "\n",
    "clf = SGDRegressor(n_iter=250) # instantiate SGDRegressor, 250 iterations\n",
    "clf.fit(X_train, y_train) # fit a linear model\n",
    "\n",
    "# plot and display \n",
    "plt.scatter(X_test, y_test, color='lightblue')\n",
    "sns.regplot(X_test, clf.predict(X_test), scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word of warning\n",
    "If you were to instantiate the `SGDRegressor` without parameters, you might run into some issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDRegressor() # instantiate SGDRegressor, 5 iterations\n",
    "clf.fit(X_train, y_train) # fit a linear model\n",
    "\n",
    "# plot and display \n",
    "plt.scatter(X_test, y_test, color='lightblue')\n",
    "sns.regplot(X_test, clf.predict(X_test), scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model isn't even fit in the right direction this time (compare to other regressor examples - the slope of the regression line should be negative). \n",
    "Note the following from the `scikit` documentation:\n",
    "\n",
    "![image](./n_iter.png)\n",
    "\n",
    "The default constructor for `SGDRegressor` makes only 5 passes over the training data, \n",
    "and since the data are so striated, this can lead to severe underfitting. \n",
    "Thus, the 'correct' example above uses a much larger value for `n_iter` - try tweaking it yourself and plotting the results to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lasso / ElasticNet\n",
    "\n",
    "*The following example is based on the Lasso and Elastic Net demonstration in the `scikit` documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# generate sparse, multi-featural data\n",
    "np.random.seed(56)\n",
    "n_samples, n_features = 75, 250\n",
    "X = np.random.randn(n_samples, n_features) # Generate a random matrix\n",
    "coef = 3 * np.random.randn(n_features)\n",
    "noise = np.arange(n_features)\n",
    "np.random.shuffle(noise)\n",
    "coef[noise[10:]] = 0 \n",
    "y = np.dot(X, coef) # y is the dot product of sparse coef and X vals\n",
    "\n",
    "# throw in some random noise\n",
    "y += 0.01 * np.random.normal((n_samples,))\n",
    "\n",
    "# split data into training and test set (note that test_size is the fraction of the dataset to use for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# import models\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "# construct lasso model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lassomodel = lasso.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# construct ElasticNet model\n",
    "enet = ElasticNet(alpha = 0.1, l1_ratio = 0.7)\n",
    "enetmodel= enet.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# plot models and compare\n",
    "plt.plot(enet.coef_, color='red', linewidth=2, \n",
    "    label=\"ENet coefficients\")\n",
    "plt.plot(lasso.coef_, color='green', linewidth=2,\n",
    "    label=\"Lasso coefficients\")\n",
    "plt.plot(coef, '--', color='blue', label=\"original coefficients\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ridge regression\n",
    "\n",
    "`Ridge` is another model within `sklearn.linear_model` that produces a linear model using the linear least squares function, \n",
    "similar to `LinearRegression` above, but regularized differently.\n",
    "\n",
    "The specific computational method is a parameter for the model, \n",
    "namely `solver` - see [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) for explanations of each.\n",
    "\n",
    "*This example is adapted from the `scikit` documentation, \n",
    "and compares models produced by ordinary least squares regression to those produced by ridge regression. \n",
    "We will be using the wine quality dataset referenced in the introductory Regression lab from module 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "X = np.array(dataset.iloc[:,:-1])[:, [1]]\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "# Split training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)\n",
    "\n",
    "# Fit models\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, ols.predict(X_test), color='red', linewidth=8,\n",
    "        label='ordinary least squares')\n",
    "plt.plot(X_test, ridge.predict(X_test), color='blue', linewidth=2,\n",
    "        label='ridge regression')\n",
    "plt.ylim(4.1,6.6)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results of ordinary least squares regression are very similar to ridge regression for a static dataset.\n",
    "It's also worth noting that for all of the above regression algorithms, \n",
    "the only real differences in the initial setup come in during the *preparation* phase - selecting and slicing your dataset prior to training. \n",
    "Building machine learning systems in the `scikit` API works more or less the same across different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithms of last resort: SVR w/ RBF kernel, EnsembleRegressors\n",
    "\n",
    "### SVR\n",
    "\n",
    "*Support vector regression* (SVR) uses support vector machines for regression. \n",
    "`SVR` is part of the `sklearn.svm` package and is notable for allowing the use of different \n",
    "functional *kernels* for model production.\n",
    "As these move up in complexity from the linear to the RBF (radial basis function) kernel, computational costs increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# generate some roughly linear data\n",
    "X, y = make_regression(n_samples=200, n_features=1, n_informative = 1,\n",
    "                        n_targets=1, noise=0.8, random_state=2) # RNG seed locked for demonstrative purposes\n",
    "\n",
    "# Split training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Construct models using different kernels\n",
    "rbf = SVR(kernel='rbf', C=100, gamma=0.1)\n",
    "poly = SVR(kernel='poly', C=100, degree=2, epsilon=0.1)\n",
    "lin = SVR(kernel='linear', C=1e3)\n",
    "\n",
    "# Fit models\n",
    "y_rbf = rbf.fit(X, y).predict(X_test)\n",
    "y_poly = poly.fit(X, y).predict(X_test)\n",
    "y_lin = lin.fit(X, y).predict(X_test)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_rbf, color='red', linewidth=3,\n",
    "        label='rbf')\n",
    "plt.plot(X_test, y_poly, color='blue', linewidth=3,\n",
    "        label='polynomial')\n",
    "plt.plot(X_test, y_lin, color='green', linewidth=3,\n",
    "        label='linear')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion\n",
    "Note that in the example, for roughly linear (degree 1) data, a polynomial kernel did an *extremely* poor job of approximating the test set. \n",
    "This is a compelling reason to carefully consider your available kernels when using support vector regression. \n",
    "(Intuitively, it makes very little sense to try and approximate a line with a curve.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnsembleRegressors\n",
    "\n",
    "`Ensemble` regression uses more than one regression - \n",
    "there are a number of ensemble-based regressors in `scikit`, \n",
    "but we'll focus on `GradientBoostingRegressor` as an  example:\n",
    "\n",
    "*This example comes from the `scikit` documentation for `GradientBoostingRegressor`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# load dataset\n",
    "dataset = load_boston()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "X, Y = shuffle(X, Y, random_state=2)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "\n",
    "# construct regressor (500 estimators)\n",
    "gbr = GradientBoostingRegressor(n_estimators=500, loss = 'ls', verbose=0)\n",
    "gbr.fit(X_train, Y_train)\n",
    "\n",
    "# build and fill out an array of loss values\n",
    "test_score = np.zeros((500),dtype=np.float64)\n",
    "for i, Y_pred in enumerate(gbr.staged_predict(X_test)):\n",
    "    test_score[i] = gbr.loss_(Y_test, Y_pred)\n",
    "\n",
    "# plot deviance\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(500) + 1, gbr.train_score_, 'b-', label=\"Training Set Deviance\")\n",
    "plt.plot(np.arange(500) + 1, test_score, 'b-', label=\"Test Set Deviance\", color='red')\n",
    "plt.xlabel('Estimators')\n",
    "plt.ylabel('Deviance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
