{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "## Discovering Classifications\n",
    "*Clustering* aims to uncover **groupings** in data, rather than numeric patterns. \n",
    "It is an unsupervised process, meaning there are no 'valid' or 'correct' response \n",
    "values being used to train the model - it simply finds clusters of similar data *a posteriori*, \n",
    "without inherent target groupings.\n",
    "\n",
    "## References\n",
    "1. Scikit documentation\n",
    "    * [K-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    * [Mini-batch k-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans)\n",
    "    * [Affinity Propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## K-means  <a id='KMC'></a>\n",
    "K-means is a method of grouping data into clusters by randomly assigning the data a set of *centroids* and moving those on each subsequent iteration. \n",
    "An excellent visual explanation can be found [here](http://bigdata-madesimple.com/possibly-the-simplest-way-to-explain-k-means-algorithm/).\n",
    "\n",
    "A very basic example follows, using the classic Iris dataset, \n",
    "which consists of numeric data on three species of *Iris* plants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load data and separate variables\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.6)\n",
    "\n",
    "#print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# construct KMeans model and fit data\n",
    "# you could do this with a larger number of clusters,\n",
    "# but we know the Iris dataset has three prominent groups/species\n",
    "km = KMeans(n_clusters = 3, random_state=2).fit(X_train, y_train)\n",
    "\n",
    "# predict() returns the numeric index of the cluster to which each test point belongs\n",
    "labels = km.predict(X_test)\n",
    "\n",
    "# max_index = np.max(labels)\n",
    "# unique_labels = np.arange(0,max_index+1)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Cluster %i\" % labels[i])\n",
    "\n",
    "# remove duplicates from legend and plot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes\n",
    "If you run this code several times, \n",
    "you'll notice that the specific boundaries of each cluster will change, \n",
    "except for the extreme outliers of each cluster. \n",
    "This is influenced somewhat by the sparsity of the given dataset and the use of random centroid placement \n",
    "in Lloyd's algorithm (the basis of the `scikit` implementation for `KMeans`). \n",
    "The random nature of `train_test_split` explains the changing shape as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Minibatch k-means\n",
    "These work more or less the same as standard `KMeans` but are more scalable to large datasets. \n",
    "The minibatch method splits training data into smaller, easier-to-process chunks, \n",
    "defined by the `batch_size` parameter.\n",
    "\n",
    "The only change that needs to be made to switch from `KMeans` to `MiniBatchKMeans` is as follows:\n",
    "\n",
    "* change the import statements\n",
    "```python\n",
    "from sklearn.cluster import KMeans --> from sklearn.cluster import MiniBatchKMeans\n",
    "```\n",
    "* change the object/model constructor\n",
    "```python\n",
    "model = KMeans(...) --> model = MiniBatchKMeans(batch_size=n, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbkm = MiniBatchKMeans(n_clusters = 3, random_state=2, batch_size=60).fit(X_train, y_train)\n",
    "\n",
    "# predict() returns the numeric index of the cluster to which each test point belongs\n",
    "labels = mbkm.predict(X_test)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Cluster %i\" % labels[i])\n",
    "\n",
    "# remove duplicates from legend and plot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that the results are more or less the same as in the full `KMeans` example. \n",
    "Real-world usage of the mini-batch algorithm should be restricted to larger datasets; \n",
    "this is purely to demonstrate their similarity for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Alternatives to `KMeans`\n",
    "\n",
    "## `AffinityPropagation`\n",
    "\n",
    "`sklearn.cluster.AffinityPropagation` is a clustering model notable for its ability to independently determine the number of clusters within a dataset, unlike `KMeans` which takes in a parameter for the number of clusters (as noted above). \n",
    "\n",
    "An example of how to use `AffinityPropagation` follows on the same Iris dataset used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load data and separate variables\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)\n",
    "\n",
    "#print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# construct KMeans model and fit data\n",
    "# you could do this with a larger number of clusters,\n",
    "# but we know the Iris dataset has three prominent groups/species\n",
    "cls = AffinityPropagation(max_iter=600).fit(X_train, y_train)\n",
    "\n",
    "# predict() returns the numeric index of the cluster to which each test point belongs\n",
    "labels = cls.predict(X_test)\n",
    "\n",
    "# max_index = np.max(labels)\n",
    "# unique_labels = np.arange(0,max_index+1)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \n",
    "          \"black\", \"purple\", \"grey\", \"lightblue\", \"lightgreen\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Cluster %i\" % labels[i])\n",
    "\n",
    "# remove duplicates from legend and plot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is sometimes unruly and can be computationally expensive, \n",
    "but is useful for smaller datasets where the number of local clusters is not well understood.\n",
    "Another demonstration (based on the one in the `scikit` [documentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py)) follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "X, Y = make_blobs(n_samples=300, centers=[[1,1],[-1,-1],[-1,1],[1,-1]], \n",
    "                       cluster_std=0.5, random_state=2)\n",
    "af = AffinityPropagation(preference=-50).fit(X)\n",
    "labels = af.labels_\n",
    "indices = af.cluster_centers_indices_\n",
    "\n",
    "n_clusters_ = len(indices)\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    members = labels == k\n",
    "    center = X[indices[k]]\n",
    "    plt.plot(X[members, 0], X[members, 1], col + '.')\n",
    "    plt.plot(center[0], center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[members]:\n",
    "        plt.plot([center[0], x[0]], [center[1], x[1]], col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
