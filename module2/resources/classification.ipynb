{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms\n",
    "\n",
    "*Classification* problems involve looking at labeled data and separating it into classes.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Scikit-Learn documentation\n",
    "    * [SVC on the Iris dataset](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py)\n",
    "    * [SGD on the Iris dataset](http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html)\n",
    "\n",
    "\n",
    "## Support Vector Classification (SVC)\n",
    "Support vector classification, like support vector regression, uses `scikit`'s SVM components to compare data,\n",
    "in this case to classify. \n",
    "`SVC` is best suited to smaller numeric datasets.\n",
    "\n",
    "In the example below, taken directly from the `scikit-learn` documentation, \n",
    "several different SVM kernels are tested against the same data, \n",
    "and the resultant classifications are plotted for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "\n",
    "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Note that in the above example, the kernel used for the support vector classification had a significant effect\n",
    "on the decision boundaries which delineate each subcategory/class in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SGD Classification with SGDClassifier\n",
    "Much like `SGDRegressor`, `SGDClassifier` is based on stochastic gradient descent. \n",
    "It is best suited to classification tasks involving very large data sets (greater than 100,000 samples).\n",
    "\n",
    "It can, however, be used on smaller datasets, such as the Iris dataset used above. \n",
    "This is the basis of the next example, also taken from the `scikit` documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "colors = \"bry\"\n",
    "\n",
    "# shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(13)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# standardize\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "clf = SGDClassifier(alpha=0.001, n_iter=75).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                cmap=plt.cm.Paired)\n",
    "plt.title(\"Decision surface of multi-class SGD\")\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot the three one-against-all classifiers\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "coef = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "\n",
    "# plot hyperplanes\n",
    "def plot_hyperplane(c, color):\n",
    "    def line(x0):\n",
    "        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "\n",
    "    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "             ls=\"--\", color=color)\n",
    "\n",
    "for i, color in zip(clf.classes_, colors):\n",
    "    plot_hyperplane(i, color)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SGD classification results\n",
    "\n",
    "SGD more easily distinguished *I. setosa* from the other variants than *I. versicolor* from *I. virginica*, \n",
    "due to stronger collocation between versicolor and virginica datapoints. \n",
    "This is consistent with observation; \n",
    "*versicolor* and *virginica* are much more difficult to distinguish than *setosa*, as their growth habits are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a set of supervised learning algorithms based on [Bayes' theorem](https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/). \n",
    "There are a number of variants, \n",
    "but the gist is that Naive Bayes is best for data with multiple predictors that are independent of one another \n",
    "(this is the 'naive' part of the name; in reality, many response variables have interrelated predictors).\n",
    "\n",
    "We'll use the variant that assumes Gaussian distribution, `GaussianNB`, as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load data and separate variables\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.6)\n",
    "\n",
    "#print(X_test.shape, Y_test.shape)\n",
    "\n",
    "# construct Gaussian Naive Bays model and fit data\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# predict() returns the numeric index of the class to which each test point belongs\n",
    "labels = gnb.predict(X_test)\n",
    "\n",
    "# pick your favorite colors!\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# iterate over labels and assign color to each point\n",
    "for i in range(0,len(X_test)):\n",
    "    col = colors[labels[i]]\n",
    "    plt.plot(X_test[:,0][i], X_test[:,1][i], color=col, marker='o', \n",
    "             markersize=5, label=\"Cluster %i\" % labels[i])\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this example functions pretty much exactly the same as the [K-means clustering example](./clustering.ipynb#KMC). This would not be the case for unlabeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
