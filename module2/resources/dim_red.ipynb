{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "### What is dimensionality reduction?\n",
    "In data analysis, *dimensionality* can be roughly understood as the number of features in a dataset.\n",
    "In cases where the dimensionality is very high, \n",
    "certain supervised learning algorithms will underperform or fail altogether unless the supervised process \n",
    "is preceded by a  *reduction* step.\n",
    "Here we'll be outlining a few of the more common dimensionality reduction algorithms in `scikit-learn`.\n",
    "\n",
    "### References\n",
    "\n",
    "1. Scikit-learn documentation\n",
    "    * [Principal component analysis (PCA)](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)\n",
    "    * [LLE/Isomap/Spectral Embedding](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) \n",
    "PCA is based on some fairly complicated maths, but can be simply understood as *transforming* a dataset into a new,\n",
    "less noisy one of reduced dimension. \n",
    "Specifically, the dimensions that are removed are those that don't explain the variance in the remaining variables, leaving behind only those that best explain said variance. This is ideal.\n",
    "\n",
    "Because dimensionality reduction in-itself isn't all that useful, except perhaps for visualizing data, \n",
    "this example will employ *pipelining* - chaining together a PCA and a regression step for more accurate prediction.\n",
    "\n",
    "*This example is adapted from the scikit-learn documentation for PCA.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, decomposition, datasets # decomposition roughly equals reduction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV # CV tools to set dimensionality of the dataset\n",
    "\n",
    "# Construct model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Prepare PCA pipeline\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "# Load dataset (we're using the toy dataset 'digits')\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Plot original data\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(X_digits)\n",
    "\n",
    "# Plot PCA spectrum\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n",
    "\n",
    "# Prepare for prediction step\n",
    "n_components = [20, 40, 64]\n",
    "Cs = np.logspace(-4, 4, 3) #generates 3 numbers that are evenly spaced on a log scale from -4 to 4\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "\n",
    "# Cross-validate\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(X_digits, y_digits)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very large datasets: incremental PCA\n",
    "While standard `PCA` is very useful for smaller datasets, \n",
    "there are some problems with the default implementation.\n",
    "`PCA` as it exists in `scikit-learn` is based on batch processing, \n",
    "which requires the entire dataset being processed to fit within main memory.\n",
    "This becomes problematic as the size of the dataset increases. \n",
    "\n",
    "As such, `IncrementalPCA` was developed.\n",
    "This PCA implementation uses 'minibatch' processing,\n",
    "splitting a dataset into moderately sized chunks and processing them sequentially.\n",
    "\n",
    "Aside from the different use case, the way to use iPCA is the exact same as PCA.\n",
    "Just know that if your dataset is particularly large, \n",
    "you should look at the iPCA documentation and most likely use `IncrementalPCA()` rather \n",
    "than `decomposition.PCA()` for your reduction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When PCA fails...\n",
    "If `PCA` isn't working out, you have a few options. \n",
    "Smaller datasets (less than 10K samples) can safely employ `Isomap` or `SpectralEmbedding` \n",
    "(or the equivalent function `spectral_embedding`) for reduction - failing that, \n",
    "`LocallyLinearEmbedding` or `locally_linear_embedding`.\n",
    "Larger datasets are generally relegated to kernel approximation, which will be covered elsewhere.\n",
    "\n",
    "## Spectral Embedding\n",
    "You will not be required to understand the math behind this.\n",
    "Running the code below (from the `scikit` [documentation](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py)) \n",
    "produces a random S-shaped curve and plots the results of several different reduction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "# Next line to silence pyflakes. This import is needed.\n",
    "Axes3D\n",
    "\n",
    "n_points = 1000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "plt.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
    "             % (1000, n_neighbors), fontsize=14)\n",
    "\n",
    "try:\n",
    "    # compatibility matplotlib < 1.0\n",
    "    ax = fig.add_subplot(251, projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "    ax.view_init(4, -72)\n",
    "except:\n",
    "    ax = fig.add_subplot(251, projection='3d')\n",
    "    plt.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "\n",
    "methods = ['standard', 'ltsa', 'hessian', 'modified']\n",
    "labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    t0 = time()\n",
    "    Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,\n",
    "                                        eigen_solver='auto',\n",
    "                                        method=method).fit_transform(X)\n",
    "    t1 = time()\n",
    "    print(\"%s: %.2g sec\" % (methods[i], t1 - t0))\n",
    "\n",
    "    ax = fig.add_subplot(252 + i)\n",
    "    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "    plt.title(\"%s (%.2g sec)\" % (labels[i], t1 - t0))\n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    plt.axis('tight')\n",
    "\n",
    "t0 = time()\n",
    "Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Isomap: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(257)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Isomap (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "mds = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
    "Y = mds.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"MDS: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(258)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"MDS (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "se = manifold.SpectralEmbedding(n_components=n_components,\n",
    "                                n_neighbors=n_neighbors)\n",
    "Y = se.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"SpectralEmbedding: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(259)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "t0 = time()\n",
    "tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "Y = tsne.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"t-SNE: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "The images above represent two-dimensional projections or mappings of the three-dimensional S curve to the left. \n",
    "Each of these is designed to preserve local distances and variance while eliminating dimensional complexity. `SpectralEmbedding`, `Isomap`, and the `LLE` variants are the fastest, \n",
    "so these tend to be preferred when applicable. \n",
    "Note that each method (aside from the LLE variants) produces a different reduced image, \n",
    "but every product has lost at least one dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
