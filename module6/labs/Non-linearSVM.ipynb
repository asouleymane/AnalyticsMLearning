{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear SVM\n",
    "## Classification of linearly inseparable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we'll be revisiting the Iris dataset.  \n",
    "We will build and train a non-linear SVM classifier to detect whether data points\n",
    "represent *I. setosa* or one of the other *Iris* varieties.  \n",
    "Then, we will plot the decision boundaries resulting from different parameters\n",
    "for training the support vector machine.\n",
    "\n",
    "## Initial imports\n",
    "First step is to load the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow initialization\n",
    "Next, we need to import TensorFlow and clear the default computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset using sklearn.datasets.load_iris()\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "# We select a pair of features rather than using the whole set\n",
    "X = np.array([[X[0], X[3]] for X in dataset.data])\n",
    "\n",
    "# We binarize class labels - 1 if Setosa, -1 otherwise\n",
    "y = np.array([1 if y==0 else 0 for y in dataset.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting up model parameters, placeholder grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deciding ahead of time what batch size should be used\n",
    "batch_size = 200\n",
    "\n",
    "# Init X, y placeholder grids\n",
    "X_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "y_grid = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Grid for predictions\n",
    "prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "# Creating b-value for the SVM kernel\n",
    "b = tf.Variable(tf.random_normal(shape=[1, batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the RBF kernel\n",
    "\n",
    "The Gaussian / Radial Basis Function (RBF) kernel may be defined as follows:  \n",
    "\n",
    "$$\n",
    "K(x_{1}, x_{2})=exp\\left(-\\gamma*(x_{1}-x_{2})^{2}\\right)\n",
    "$$  \n",
    "which, where `X` is some vector of points, is roughly equivalent to  \n",
    "\n",
    "$$K(\\textbf{x})=exp\\left( -\\gamma * |\\textbf{x} \\cdot \\textbf{x}^{T}| \\right)$$  \n",
    "\n",
    "which is the relation we will use for our kernel calculation.\n",
    "\n",
    "  * [Read more about the RBF Kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n",
    "\n",
    "## What's with the gamma?\n",
    "Gamma is a constant for use in the Radial Basis Function (RBF) kernel that effectively determines the range of influence for a single subsample, i.e., the radius.\n",
    "\n",
    "  * Smaller values for gamma *increase* that relative influence, producing a wider kernel.  \n",
    "  * Larger values *decrease* the influence of a subsample, producing 'tighter'-looking decision boundaries.\n",
    "\n",
    "The code below is a TensorFlow representation of the above RBF kernel (remember, we defined gamma as negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = tf.constant(-45.0) # Gamma is some constant, which we make negative\n",
    "sq_vec = tf.multiply(2., tf.matmul(X_grid, tf.transpose(X_grid)))\n",
    "rbf_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computational step\n",
    "The non-linear SVM actually aims at *maximizing* the loss function, specifically by minimizing its negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = tf.reduce_sum(b)\n",
    "b_cross = tf.matmul(tf.transpose(b), b)\n",
    "y_grid_cross = tf.matmul(y_grid, tf.transpose(y_grid))\n",
    "second = tf.reduce_sum(tf.multiply(rbf_kernel, tf.multiply(b_cross, y_grid_cross)))\n",
    "\n",
    "# Loss is negative here because this value needs to be maximized.\n",
    "# Minimizing a negative maximizes the positive equivalent.\n",
    "loss = tf.negative(tf.subtract(first, second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building and applying a prediction kernel\n",
    "Next, we need to produce a predictor kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RBF prediction kernel\n",
    "rA = tf.reshape(tf.reduce_sum(tf.square(X_grid), 1),[-1,1])\n",
    "rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
    "pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(X_grid, tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n",
    "\n",
    "# Applying said kernel\n",
    "pred_output = tf.matmul(tf.multiply(tf.transpose(y_grid),b), pred_kernel)\n",
    "prediction = tf.sign(pred_output-tf.reduce_mean(pred_output))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_grid)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Declare optimizer function, train step\n",
    "Next, we need to declare an optimizer function to train the classifier.  \n",
    "We'll use a `GradientDescentOptimizer` from `tensorflow.train`.\n",
    "\n",
    "Furthermore, we aim to train the model by minimizing the loss function.  \n",
    "This informs our definition for `train_step` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize gradient descent optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "# Note: the parameter (0.01) is the learning rate - toy with this and see what happens.\n",
    "\n",
    "# Define training step\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training preparation: global variables and loop parameters\n",
    "Having cleared the graph state earlier, we should re-initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# Loop parameters/variables\n",
    "num_iter = 300\n",
    "# Note: here's where you'd want to track temp_loss\n",
    "# and a temp_accuracy if you're interested in the\n",
    "# additional exercises at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model (finally)\n",
    "Finally, we construct a training loop that runs the `tf` session with our optimizer via `train_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for i in range(num_iter):\n",
    "    rand_index = np.random.choice(len(X), size=batch_size)\n",
    "    rand_X = X[rand_index]\n",
    "    rand_y = np.transpose([y[rand_index]])\n",
    "    session.run(train_step, feed_dict={X_grid: rand_X, y_grid: rand_y})\n",
    "    \n",
    "    # It's a good idea to confirm that our loss values are decreasing:\n",
    "    temp_loss = session.run(loss, feed_dict={X_grid: rand_X, y_grid: rand_y})\n",
    "\n",
    "    if (i+1)%50==0:\n",
    "        print('Loss @ step ' + str(i+1) + '= ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We've confirmed that the loss minimizer is working, so the model is (probably) learning.\n",
    "\n",
    "## Visualizing the classifier: grid construction\n",
    "Having trained up our classifier, we can visually confirm its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we're ready to plot, we should import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct numpy mesh for plotting\n",
    "X_min, X_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(X_min, X_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "[grid_predictions] = session.run(prediction, feed_dict={X_grid: rand_X,\n",
    "                                                       y_grid: rand_y,\n",
    "                                                       prediction_grid: grid_points})\n",
    "grid_predictions = grid_predictions.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pulling per-class data from feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pulling out sepal width and length for each class for plotting\n",
    "X1 = [x[0] for i,x in enumerate(X) if y[i]==1]\n",
    "y1 = [x[1] for i,x in enumerate(X) if y[i]==1]\n",
    "X2 = [x[0] for i,x in enumerate(X) if y[i]==0]\n",
    "y2 = [x[1] for i,x in enumerate(X) if y[i]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot points and grid\n",
    "plt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.plot(X1, y1, 'ro', label='I. setosa')\n",
    "plt.plot(X2, y2, 'kx', label='Non setosa')\n",
    "plt.title('RBF Kernel Results on Iris Data')\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([-0.5, 3.0])\n",
    "plt.xlim([3.5, 8.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Sweet victory!_\n",
    "The classifier can clearly differentiate between *I. setosa* and the other varieties.\n",
    "\n",
    "Notice the decisoin boundary is a curve.\n",
    "You can imagine that for each support vector selected, an ellipsoid is projected out with it at the center.\n",
    "The overlap of the hyper-ellipsoids forms the decision boundaries.\n",
    "\n",
    "## Going further: additional exercises\n",
    "* Plot the boundaries produced when using different values for `gamma`.\n",
    "* Plot per-batch accuracy over time\n",
    "* Plot the loss function over time (intuitively, how should this graph look?)\n",
    "* Describe the tradeoffs between boundary smoothness and classification accuracy based on your understanding of `gamma` and the first question above.\n",
    "\n",
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
