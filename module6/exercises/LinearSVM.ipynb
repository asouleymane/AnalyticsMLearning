{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: Exercise\n",
    "\n",
    "In this session, we fit a linear SVM on **Medical Appointment No Shows** dataset\n",
    "with the typical train/validate workflow.\n",
    "\n",
    "In addition, you are supposed to perform outlier removal and feature selection before\n",
    "training a linear SVM on this dataset as well.\n",
    "\n",
    "Please follow **LinearSVM** and **Processing** labs in this module to get familliarized with\n",
    "linear SVM model and prepare for this exercise.\n",
    "\n",
    "The **Processing** provides an example of how you could incorporate feature selections and\n",
    "outlier detection into a more complete data analysis workflow.\n",
    "Please refer back to labs in **Module 3** and **Module 4** respectively for more details.\n",
    "\n",
    "Dataset: https://www.kaggle.com/joniarroba/noshowappointments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "This dataset is used to potentially predict **No-show** from various factors recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/AppliedML_M6/appointment_noshow.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "<span style=\"background: yellow;\">For this section, every time when you are debugging your code to modify the dataset, you probably need to re-run cells from **Load dataset**. (above cell)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List top 5 records to have a preview of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6001)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we won't be using **PatientId** and **AppointmentID** columns.\n",
    "\n",
    "Delete those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6002)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert **Gender** and **No-show** to binary (0s and 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6003)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert **ScheduledDay** and **AppointmentDay** into np.datetime data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6004)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column **AwaitingTime** filled with the time difference between **AppointmentDay** and **ScheduledDay**,\n",
    "in number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['AwaitingTime'] = (dataset['AppointmentDay'] - dataset['ScheduledDay']).apply(lambda dt: dt.days)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check unique values of all columns, except **ScheduledDay** and **AppointmentDay**\n",
    "because they would have too many unique values.\n",
    "\n",
    "The goal is to understand whether there's missing values or \"bad\" values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6005)\n",
    "# ----------------------------------\n",
    "for column_name in set(dataset.columns)-{'ScheduledDay', 'AppointmentDay'}:\n",
    "    print(column_name, sorted(np.unique(dataset[<placeholder>])))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Import some outlier detection utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we noticed is that age can't be less than or equal to 0 unless there's probably some discrepancy in the dataset. We remove those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6006)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "print('Number of records', len(dataset))\n",
    "print('Age', np.unique(dataset['Age']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers in **AwaitingTime** with **Elliptic Envelope**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "awaiting_time = np.array(dataset['AwaitingTime']).reshape((-1, 1))\n",
    "# print('awaiting_time.shape', awaiting_time.shape)\n",
    "\n",
    "# Complete code below this comment  (Question #E6007)\n",
    "# ----------------------------------\n",
    "envelope = EllipticEnvelope(contamination = 0.003)\n",
    "envelope.fit(<placeholder>)\n",
    "outliers = <placeholder>\n",
    "dataset.drop(np.flatnonzero(outliers), inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "# ----------------------------------\n",
    "\n",
    "print({'inliers': np.sum(~outliers), 'outliers': np.sum(outliers)})\n",
    "print('Number of records', len(dataset))\n",
    "print('AwaitingTime', np.unique(dataset['AwaitingTime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates and times are usually difficult to deal with for predictive models as input data.\n",
    "\n",
    "Therefore, we create take day and month out of **AppointmentDay** and **create two new columns** respectively.\n",
    "\n",
    "Also remove **ScheduledDay** because it can be derived from these two columns and **AwaitingTime**,\n",
    "so this column would become redundant.\n",
    "\n",
    "Remove column **AppointmentDay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6008)\n",
    "# ----------------------------------\n",
    "dataset['AppointmentDate_day'] = dataset['AppointmentDay'].apply(<placeholder>)\n",
    "dataset['AppointmentDate_month'] = dataset['AppointmentDay'].apply(<placeholder>)\n",
    "\n",
    "# Add code below this comment to delete columns ScheduledDay and AppointmentDay (Question #E6009)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings are also undesirable data types here. We use **LabelBinarizer** to create a one-hot encoding for **Neighbourhood** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Complete code below this comment  (Question #E6010)\n",
    "# ----------------------------------\n",
    "encoder = LabelBinarizer()\n",
    "Neighbourhood_onehot = encoder.fit_transform(dataset[<placeholder>])\n",
    "# ----------------------------------\n",
    "\n",
    "for j, neighborhood in enumerate(encoder.classes_):\n",
    "    dataset['Neighbourhood ({})'.format(neighborhood)] = Neighbourhood_onehot[:, j]\n",
    "\n",
    "del dataset['Neighbourhood']\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all columns are integer type.\n",
    "\n",
    "Check statictics for the rest of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.loc[:, [column_name for column_name in dataset.columns\n",
    "    if not column_name.startswith('Neighbourhood')]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check class balance.\n",
    "\n",
    "This dataset is used to potentially predict **No-show** from various factors recorded.\n",
    "Since no-shows should usually be the minority cases, it's very likely that this dataset  \n",
    "is very imbalanced.\n",
    "\n",
    "We want to understand how balanced it is between number of positive and negative samples quantitatively.  \n",
    "So we find out the ratio of no-shows among the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6011)\n",
    "# ----------------------------------\n",
    "num_noshow = np.sum(<placeholder>) # find out total number of no-show cases\n",
    "print('noshow ratio:', num_noshow, '/', len(dataset), '=', num_noshow / len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of fairness, we will resample no-show cases to rebalance the dataset.\n",
    "\n",
    "First, we calculate this upsample rate that would make positive and negative samples appear 50/50,\n",
    "when multiplied to number of no-show cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upsample_rate = (len(dataset) - num_noshow) / num_noshow\n",
    "print('upsample_rate:', upsample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify this upsample rate by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(int(num_noshow * upsample_rate), len(dataset) - num_noshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we resample dataset. Please upsample these no-show cases then concatenate with original \"show-up\" cases and\n",
    "and create a new dataset **dataset_resampled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6012)\n",
    "# ----------------------------------\n",
    "dataset_resampled = pd.concat([\n",
    "    dataset[dataset['No-show'] == 1].sample(<placeholder>).reset_index(drop=True),\n",
    "    dataset[dataset['No-show'] == 0]\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle **dataset_resampled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6013)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify no-show ratio again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('noshow ratio:', np.sum(dataset_resampled['No-show'] == 1) / len(dataset_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid mixing up **dataset** and **dataset_resampled**,\n",
    "we replace **dataset** and delete **dataset_resampled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset_resampled\n",
    "del dataset_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing, you may realize that we got a lot of columns just for neighborhood.\n",
    "Number of these columns are way higher than other features combined.\n",
    "Since we have already made an decision to encode them with one-hot encoding arbitrary,\n",
    "why not use PCA on these columns and see if we could compress them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import scale, LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all Neighbourhood columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6014)\n",
    "# ----------------------------------\n",
    "X_neighbors = np.array(dataset.loc[:, [column_name for column_name in <placeholder>\n",
    "    if column_name.startswith('Neighbourhood')]])\n",
    "X_neighbors.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA to **X_neighbors** with 60 pincipal components to create **X_neighbors_PCA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6015)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check combined variance explained ratio to make sure it doesn't drop too significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all Neighbourhood columns because we are going to replace them all with their principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6016)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attatch principal components of the original Neighbourhood columns onto the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(X_neighbors_PCA.shape[1]):\n",
    "    dataset['N{}'.format(j)] = X_neighbors_PCA[:, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create following arrays for easier access to column names and features.\n",
    "\n",
    "Study what they are and what do they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = np.array(dataset.columns)\n",
    "original_features = column_names!='No-show'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train/test split **X_train, X_test, y_train, y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Complete code below this comment  (Question #E6017)\n",
    "# ----------------------------------\n",
    "# Pull features and labels\n",
    "X = scale(np.array(dataset.loc[:, <placeholder>]))  # use what you learned from the cell above this\n",
    "y = np.array(dataset['No-show'])\n",
    "\n",
    "# Add code below this comment  (Question #E6018)\n",
    "# ----------------------------------\n",
    "# Create training/validation split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a feature selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6019)\n",
    "# ----------------------------------\n",
    "selector = SelectKBest(f_classif, k=20)\n",
    "selector.fit(<placeholder>)\n",
    "selected_features = column_names[original_features][selector.get_support()]\n",
    "print(selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a linear SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Please ignore the configurations below\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import tf_threads\n",
    "estimator_config = tf.contrib.learn.RunConfig(session_config=\n",
    "    tf_threads.limit(tf, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature columns as TensorFlow placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6020)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E6021)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_fn() to supply training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #E6022)\n",
    "# ----------------------------------\n",
    "def input_fn():\n",
    "    X_selected = selector.transform(X_train)\n",
    "    columns = {\n",
    "        feature_name: tf.constant(np.expand_dims(<placeholder>, 1))\n",
    "            for i,feature_name in enumerate(selected_features)\n",
    "    }\n",
    "    columns['example_id'] = tf.constant([str(i+1) for i in range(len(X_selected))])\n",
    "    labels = tf.constant(y_train)\n",
    "    return columns, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM\n",
    "\n",
    "This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add code below this comment  (Question #E6023)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "First create a predict_fn() to supply data from test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_fn():\n",
    "    X_selected = selector.transform(X_test)\n",
    "    columns = {\n",
    "        feature_name: tf.constant(np.expand_dims(X_selected[:, i], 1))\n",
    "            for i,feature_name in enumerate(selected_features)\n",
    "    }\n",
    "    columns['example_id'] = tf.constant([str(i+1) for i in range(len(X_selected))])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(input_fn=predict_fn)\n",
    "y_pred = list(map(lambda i: i['classes'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy and create confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Add code below this comment  (Question #E6024)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
