{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network Model\n",
    "## Keras\n",
    "\n",
    "In this lab, we are going to drill down into some Neural Network basics using the Keras package with the TensorFlow backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neuron\n",
    "\n",
    "Recall the concept of a [neuron](https://en.wikipedia.org/wiki/Artificial_neuron) based on its mathematical formula.\n",
    "\n",
    "$$ y_k = \\varphi \\left( \\sum_{j=0}^{m}{w_{kj}x_j} +b_k \\right) $$\n",
    "\n",
    "This is a simple linear neuron.\n",
    "\n",
    "Keras, as well as other NN packages, support numerous types of neurons.\n",
    "Typically, neurons are composed into layers, and a single layer has only a single type of neuron.\n",
    "\n",
    "In this lab, we are going to look at some data that is, first, easily separable; then later less separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider data that is easy to divide\n",
    "\n",
    "First, we will generate some data that is easily separated.\n",
    "This data is easily separated by a decision along the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=300, centers=2, n_features=2)\n",
    "X = scale(X, with_mean=False, with_std = False) # Center X\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a neural network\n",
    "\n",
    "Now we will construct a basic Neural Network with\n",
    " * One hidde layer fed by 2 input values\n",
    " * One output later \n",
    " \n",
    "##### Note: The summary will show that we have 5 total learnable parameters:\n",
    "  * 3 for the hidden layer ($X_0$, $X_1$, and bias) \n",
    "  * 2 for the output layer (Hidden ($H_0$) and bias) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Build a mode that is composed of this list of layers\n",
    "model = Sequential(\n",
    "    [\n",
    "          # This specifies a single neuron, and the input is 2 numbers.\n",
    "    Dense(1, input_dim=2),  # a dense layer, every neuron is connected to all points from the lower layer (input)\n",
    "    Activation('linear'),   # Specify the type of decision surface, i.e., simple linear regression\n",
    "    Dense(1),               # another dense layer, input_dim is inferred from the previous layer's output\n",
    "    Activation('sigmoid')   # Specify the type of decision surface, i.e., simple logistic regression\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number of trainable parameters highlights is the power and cost of the NN models.\n",
    "We can see that clearly, a two parameter mode should be sufficient (think decision tree with feature specification and decision point on feature).\n",
    "However, we are trainng 5 or 150% increace over the parameters of a decision tree that would achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a binary classification problem we are defining our loss as \"binary\" and the measurement as cross-entropy\n",
    "model.compile(optimizer='rmsprop',  # this is an optimizer name, we will revisit this part later!\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, iterating on the data in batches of 4 samples\n",
    "model.fit(X, y, epochs=10, batch_size=4)\n",
    "\n",
    "#  recall, an epoch is a round of training in which the model sees all the training data one time\n",
    "#  Epoch = all 300 training sample here\n",
    "#  Batch is the number of feed forward training samples pushed through the network before the \n",
    "#          accumulated error is pushed back\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jiggle the data\n",
    "\n",
    "Below we jiggle the data a little bit to create a test set.\n",
    "This is done by generating some random noise and adding it to the existing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X + np.random.normal(0.0, 0.5, X.size).reshape(300,2)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand what we get from the model evaluation, let's look at the function through help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our loss was very small and the accuracy was 1.0.\n",
    "We should have expected this!\n",
    "The data was easy!!!\n",
    "\n",
    "---\n",
    "## Consider data that is less easy to divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state = 76533)\n",
    "X = scale(X, with_std = False) # Center X\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "X_test = X + np.random.normal(0.0, 0.5, X.size).reshape(int(X.size/2),2)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "modelV = Sequential(\n",
    "    [\n",
    "    Dense(1, input_dim=2),\n",
    "    Activation('linear'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "    ]\n",
    ")\n",
    "# For a binary classification problem\n",
    "modelV.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, iterating on the data in batches of 4 samples\n",
    "modelV.fit(X, y, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the accuracy takes longer to get above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelV.evaluate(X_test, y, batch_size=4)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets adjust the hidden layer from 1 to 2 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "modelW = Sequential(\n",
    "    [\n",
    "    Dense(2, input_dim=2),\n",
    "    Activation('linear'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "    ]\n",
    ")\n",
    "# For a binary classification problem\n",
    "modelW.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 4 samples\n",
    "modelW.fit(X, y, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelW.evaluate(X_test, y, batch_size=4)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets adjust the hidden layer from 1 layer with 2 neurons\n",
    "\n",
    "### To 2 layers with 2 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "modelY = Sequential(\n",
    "    [\n",
    "    Dense(2, input_dim=2),\n",
    "    Activation('linear'),\n",
    "        # Notice we are adding a new hidden layer\n",
    "    Dense(2, input_dim=2),\n",
    "    Activation('linear'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "    ]\n",
    ")\n",
    "# For a binary classification problem\n",
    "modelY.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 4 samples\n",
    "modelY.fit(X, y, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelY.evaluate(X_test, y, batch_size=4)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Keras API and helpful links\n",
    "\n",
    " * Layers: https://keras.io/layers/core/\n",
    " * Loss / Loss Functions : https://keras.io/losses/\n",
    " * Optimizers (learning algorithm) : https://keras.io/optimizers/\n",
    " * Neuron Activation Functions : https://keras.io/activations/\n",
    " \n",
    "#### Now, look at using a customized optimizer:\n",
    "\n",
    "We will specify the Stochastic Gradient Descent optimizer (vector calculus fun)\n",
    "```\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "modelZ = Sequential(\n",
    "    [\n",
    "    Dense(2, input_dim=2),\n",
    "    Activation('linear'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Changing learning_rate and moments from default\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.1)\n",
    "\n",
    "# For a binary classification problem\n",
    "modelZ.compile(optimizer=sgd,  # previous we used a string, rmsprop that got us that optimizer with default values!\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 4 samples\n",
    "modelZ.fit(X, y, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelZ.evaluate(X_test, y, batch_size=4)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please restart the kernel and clear all output, then play around with parameters or add cells and create additional notebooks\n",
    "\n",
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
