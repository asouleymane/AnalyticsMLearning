{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Multilayer Perceptron\n",
    "\n",
    "As you are encountering complex models, the lirbaries to use them get more and more complex.\n",
    "Please do not feel like you have to master everything within these notebooks.\n",
    "Consider them as introductory and _tip of the iceberg_. \n",
    "Neural networks are complex and take many months to master.\n",
    "In subsequent labs you will see higher-level assemblies of networks using alternative API.\n",
    "\n",
    "In this lab you will learn about ...\n",
    "\n",
    "TensorFlow API reference\n",
    "+ [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)\n",
    "+ [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable)\n",
    "+ [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    "+ [tf.train.GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "+ [tf.Session](https://www.tensorflow.org/api_docs/python/tf/Session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neuron\n",
    "\n",
    "We can define parameters inside the model using `tf.Variable()`.\n",
    "They will be initialized using a [\"truncated\" normal distribution](https://www.tensorflow.org/api_docs/python/tf/truncated_normal).\n",
    "Then define the concept of a [neuron](https://en.wikipedia.org/wiki/Artificial_neuron) based on its mathematical formula.\n",
    "\n",
    "$$ y_k = \\varphi \\left( \\sum_{j=0}^{m}{w_{kj}x_j} +b_k \\right) $$\n",
    "\n",
    "The most basic units within tensorflow are [tf.constant](https://www.tensorflow.org/api_docs/python/tf/constant), [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) and [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "They are the data structures that hold multidimensional arrays in the TensorFlow framdwork.\n",
    "\n",
    "The difference between a `tf.Variable` and a `tf.placeholder` is that `tf.placeholders` are \n",
    "variables that can be assigned by a \"driver\" program, e.g. a script that invokes TensorFlow APIs and runs the training.\n",
    "Placeholders will always need to be fed some external information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weights = lambda shape: tf.Variable(tf.truncated_normal(shape, seed = 0x3fe69))\n",
    "Biases = lambda shape: tf.Variable(tf.truncated_normal(shape, seed = 0xac5b0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate two-blob data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=600, centers=2, n_features=2, random_state = 76533)\n",
    "X = scale(X, with_std = False) # Center X\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fit one neuron\n",
    "\n",
    "Training one neuron to classify the two-blobs data. \n",
    "\n",
    "This section aims to demonstrate the workflow of building and training a model using TensorFlow. \n",
    "Click [here](http://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=1&seed=0.99526&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) to view the problem scenario on TensorFlow Playground.\n",
    "This is the same site that was used for some of the lecture videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Input/output\n",
    "\n",
    "During training, we should use [tf.placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) to \"feed\" information into the neural network model.\n",
    "In this example, the model will be fed with features and labels for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.placeholder(\"float\", (None, X.shape[1]))\n",
    "labels = tf.placeholder(\"float\", (None, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Computation graph\n",
    "\n",
    "This is the part where you describe the dataflow.\n",
    "A [Graph](https://www.tensorflow.org/api_docs/python/tf/Graph) contains a set of [tf.Operation](https://www.tensorflow.org/api_docs/python/tf/Operation) objects, \n",
    "which represent units of computation;\n",
    "and [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects, \n",
    "which represent the units of data that flow between operations.\n",
    "\n",
    "TensorFlow Operations are callable objects that take Tensors and return Tensors.\n",
    "TensorFlow Tensors are in fact lazy datastructures representing multidimensional arrays that won't hold values until evaluation.\n",
    "\n",
    "The following cell defines a graph containing one neuron as well as the loss function.\n",
    "In addition, the backpropagation is also defined by **optimizer** and **training** as part of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_neuron_graph():\n",
    "    # Create a single neuron for this model.\n",
    "    neuron = lambda x: tf.sigmoid(tf.add(tf.matmul(x, Weights((X.shape[1], 1))), Biases((1, ))))\n",
    "\n",
    "    # Predictions are made by this neuron.\n",
    "    predictions = neuron(features)\n",
    "\n",
    "    # Loss function to be optimized. We use mean squared error.\n",
    "    loss = tf.losses.mean_squared_error(labels, tf.squeeze(predictions))\n",
    "\n",
    "    # An optimizer defines the operation for updating parameters within the model.\n",
    "    # Learning rate = 0.5\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "    # Training is defined as minimizing the loss function using gradient descent.\n",
    "    training = optimizer.minimize(loss)\n",
    "    \n",
    "    return [training, loss, predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run computation graph with TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneNeuron(object):\n",
    "    def __init__(self, session):\n",
    "        self.context = [session] + one_neuron_graph()\n",
    "\n",
    "    def fit(self, X, y, N_BATCH = 32):\n",
    "        sess, training, loss, _  = self.context\n",
    "        \n",
    "        # An array recording training loss\n",
    "        training_loss = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(10):\n",
    "            epoch_loss = []\n",
    "            for i in range(0, X.shape[0], N_BATCH):\n",
    "                _, batch_loss = sess.run([training, loss], feed_dict={\n",
    "                    features: X[i:i+N_BATCH],\n",
    "                    labels: y[i:i+N_BATCH]\n",
    "                })\n",
    "                epoch_loss.append(batch_loss)\n",
    "            training_loss.append(np.mean(epoch_loss))\n",
    "        \n",
    "        self.training_loss = training_loss\n",
    "    \n",
    "    def predict(self, X, N_BATCH = 32):\n",
    "        sess, _, _, predictions  = self.context\n",
    "        \n",
    "        y_pred = []\n",
    "        for i in range(0, X.shape[0], N_BATCH):\n",
    "            batch_prediction = sess.run(predictions, feed_dict={\n",
    "                features: X[i:i+N_BATCH]\n",
    "            })\n",
    "            y_pred.extend(batch_prediction.squeeze())\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Create OneNeuron model\n",
    "    one_neuron = OneNeuron(sess)\n",
    "    \n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training\n",
    "    one_neuron.fit(X, y)\n",
    "        \n",
    "    # Evaluation       \n",
    "    print('accuracy', accuracy_score(y, one_neuron.predict(X)>=0.5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('loss')\n",
    "    plt.xticks(range(len(one_neuron.training_loss)))\n",
    "    plt.plot(range(len(one_neuron.training_loss)), one_neuron.training_loss)\n",
    "        \n",
    "    # Plot decision plane\n",
    "    x1, x2 = np.meshgrid(np.linspace(-6, 6, 120), np.linspace(-6, 6, 120))\n",
    "    Z = np.array(one_neuron.predict(np.column_stack([x1.ravel(), x2.ravel()]))).reshape(x1.shape)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "        extent=(x1.min(), x1.max(), x2.min(), x2.max()), vmin=0.0, vmax=1.0,\n",
    "        aspect='equal', origin='lower', cmap='binary'\n",
    "    )\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layer feedforward network\n",
    "\n",
    "In this section, we will fit a two-layer feedforward neural network on the red wine dataset and deal with some \n",
    "practical problems in applying neural network to a real-world dataset before we move on to more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Load dataset from files into multi-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "# Pull features and labels\n",
    "X = scale(np.array(dataset.iloc[:, :-1]))\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "# Create training/validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print('Class distribution:', {i: np.sum(y==i) for i in np.unique(dataset.quality)})\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layers\n",
    "\n",
    "Any layers that are not the output layer are referred to as hidden layers.\n",
    "So in this model we will have a hidden layer and an output layer, each containing a few artificial neurons.\n",
    "In this model, the neurons are fully connected between layers. \n",
    "Therefore, these layers are called dense layers.\n",
    "\n",
    "First we define concept of a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dense(n, activation):\n",
    "    return lambda x: activation(\n",
    "        tf.matmul(x, Weights((x.get_shape().as_list()[1], n))) + Biases((n, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNN(object):\n",
    "    def __init__(self, session):\n",
    "        # Two-layer FeedForwardNN\n",
    "        hidden_layer = Dense(10, tf.sigmoid)\n",
    "        output_layer = Dense(6, tf.identity)\n",
    "        predictions = output_layer(hidden_layer(features))\n",
    "\n",
    "        # Loss function\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=predictions))\n",
    "\n",
    "        # An optimizer defines the operation for updating parameters within the model.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.03)\n",
    "\n",
    "        # Training is defined as minimizing the loss function using gradient descent.\n",
    "        training = optimizer.minimize(loss)\n",
    "        \n",
    "        self.context = [session, training, loss, predictions]\n",
    "        \n",
    "    def fit(self, X_train, y_train, N_BATCH=32):\n",
    "        sess, training, loss, _  = self.context\n",
    "        label_encoding=LabelBinarizer()\n",
    "        label_encoding.fit(y)\n",
    "        \n",
    "        training_loss = []\n",
    "        for epoch in range(200):\n",
    "            epoch_loss = []\n",
    "            for i in range(0, X_train.shape[0], N_BATCH):\n",
    "                _, batch_loss = sess.run([training, loss], feed_dict={\n",
    "                    features: X_train[i:i+N_BATCH],\n",
    "                    labels: label_encoding.transform(y_train[i:i+N_BATCH])\n",
    "                })\n",
    "                epoch_loss.append(batch_loss)\n",
    "            training_loss.append(np.mean(epoch_loss))\n",
    "        self.training_loss = training_loss\n",
    "        self.label_encoding = label_encoding\n",
    "        \n",
    "    def predict(self, X_test, N_BATCH=32):\n",
    "        sess, _, _, predictions  = self.context\n",
    "        \n",
    "        y_pred = []\n",
    "        for i in range(0, X_test.shape[0], N_BATCH):\n",
    "            batch_prediction = sess.run(predictions, feed_dict={\n",
    "                features: X_test[i:i+N_BATCH]\n",
    "            })\n",
    "            class_probablity = self.label_encoding.inverse_transform(np.exp(batch_prediction))\n",
    "            y_pred.extend(class_probablity)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    features = tf.placeholder(\"float\", (None, 11))\n",
    "    labels = tf.placeholder(\"float\", (None, 6))\n",
    "    feedforward = FeedForwardNN(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feedforward.fit(X_train, y_train)\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('loss')\n",
    "    plt.plot(range(len(feedforward.training_loss)), feedforward.training_loss)\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    y_pred = feedforward.predict(X_test)\n",
    "    print('accuracy', accuracy_score(y_test, y_pred))\n",
    "    plt.imshow(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(FeedForwardNN):\n",
    "    def __init__(self, session, features, labels):\n",
    "        # Two-layer FeedForwardNN\n",
    "        hidden_layer = tf.layers.dense(features, 10, tf.tanh)\n",
    "        hidden_layer2 = tf.layers.dense(hidden_layer, 8, tf.tanh)\n",
    "        hidden_layer3 = tf.layers.dense(hidden_layer2, 8, tf.sigmoid)\n",
    "        predictions = tf.layers.dense(hidden_layer3, 6)\n",
    "\n",
    "        # Loss function\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=predictions))\n",
    "\n",
    "        # An optimizer defines the operation for updating parameters within the model.\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Training is defined as minimizing the loss function using gradient descent.\n",
    "        training = optimizer.minimize(loss)\n",
    "        \n",
    "        self.context = [session, training, loss, predictions]\n",
    "        \n",
    "    def fit(self, X_train, y_train, N_BATCH=32):\n",
    "        sess, training, loss, _  = self.context\n",
    "        label_encoding=LabelBinarizer()\n",
    "        label_encoding.fit(y)\n",
    "        \n",
    "        training_loss = []\n",
    "        for epoch in range(500):\n",
    "            epoch_loss = []\n",
    "            for i in range(0, X_train.shape[0], N_BATCH):\n",
    "                _, batch_loss = sess.run([training, loss], feed_dict={\n",
    "                    features: X_train[i:i+N_BATCH],\n",
    "                    labels: label_encoding.transform(y_train[i:i+N_BATCH])\n",
    "                })\n",
    "                epoch_loss.append(batch_loss)\n",
    "            training_loss.append(np.mean(epoch_loss))\n",
    "        self.training_loss = training_loss\n",
    "        self.label_encoding = label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    features = tf.placeholder(\"float\", (None, 11))\n",
    "    labels = tf.placeholder(\"float\", (None, 6))\n",
    "    mlp = MultilayerPerceptron(sess, features, labels)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('loss')\n",
    "    plt.plot(range(len(mlp.training_loss)), mlp.training_loss)\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    print('accuracy', accuracy_score(y_test, y_pred))\n",
    "    plt.imshow(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This code may seem overwhelming to take in the first time through.\n",
    "Please take a break and revisit later to give it another read.\n",
    "\n",
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
