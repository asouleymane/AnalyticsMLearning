{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for \n",
    "classification, regression and other tasks, that operate by constructing a \n",
    "multitude of decision trees at training time and outputting the class that \n",
    "is the mode of the classes (classification) or mean prediction (regression) of \n",
    "the individual trees. \n",
    "Random decision forests correct for decision trees' habit of overfitting to \n",
    "their training set.\n",
    "    - From Wikipedia\n",
    "\n",
    "## For Classification \n",
    "\n",
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Classify the Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "# Slice data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# Create and fit the model with full data\n",
    "model = DecisionTreeClassifier(max_depth=None)\n",
    "classifier = model.fit(X, y) # Fit the model to this data\n",
    "scores = classifier.score(X, y)\n",
    "\n",
    "\n",
    "print(\"Decision Tree Performance: {}\".format(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "# Do the cross-validation\n",
    "sklearn.model_selection.cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the full training versus the cross-validation, we expect that the full training data model is overfitting the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the Random Forest is from the ensemble set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "# Slice data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Shuffle\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(RANDOM_SEED)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# Create and fit the model with full data\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "classifier = model.fit(X, y) # Fit the model to this data\n",
    "scores = classifier.score(X, y)\n",
    "\n",
    "\n",
    "print(\"Random Forest Performance: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice we have generalized a little better!  \n",
    "\n",
    "Let's look at the cross-validation for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "# Do the cross-validation\n",
    "sklearn.model_selection.cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Look at some visuals of Decision Tree versus Random Forest decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig=plt.figure(figsize=(10, 30))\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Notice the Random Forest is from the ensemble set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "n_estimators = 30\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators)]\n",
    "\n",
    "# Iterate through a pair of features (recall data is actually 4-D)\n",
    "# This is strictly for visualization later\n",
    "for pair in ([0,1], [0,2], [0,3], [1,2],[1,3],[2,3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = iris.data[:, pair]\n",
    "        y = iris.target\n",
    "\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Standardize the data to mean = 0 and standard deviation of 1\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std   # look, we are turning the data into Z-scores\n",
    "\n",
    "        # Train\n",
    "        clf = clone(model)   # snag a copy of the model from the list created above\n",
    "        clf = model.fit(X, y) # Fit the model to this data\n",
    "\n",
    "        scores = clf.score(X, y)\n",
    "        \n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "            \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "              \"has a score of\", scores)\n",
    "        print()\n",
    "\n",
    "        # 6 feature pairs (rows) and 2 models (columns) \n",
    "        plt.subplot(6, 2, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title)\n",
    "\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifier\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            # that are in use (noting that AdaBoost can use fewer estimators\n",
    "            # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                         yy_coarser.ravel()]\n",
    "                                         ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# This is another type of Random Forest\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "help(ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a classification task using 3 informative features\n",
    "#   Consult the API for more details\n",
    "X, y = make_classification(n_samples=1000,    # 1K data point\n",
    "                           n_features=10,     # 10-D data\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,       # 2 classes\n",
    "                           random_state=0,\n",
    "                           shuffle=False)\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "# Fit the model to the data\n",
    "forest.fit(X, y)\n",
    "\n",
    "# Pull the feature importances from the model\n",
    "importances = forest.feature_importances_\n",
    "# Compute the standard deviation of the feature importance for each feature of all trees\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# Sort the data, but get back the list of indices that were sorted with the data\n",
    "#   not the data itself\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save your Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
